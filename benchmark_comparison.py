#!/usr/bin/env python3
"""
Hull Tacticalå¸‚åœºé¢„æµ‹é¡¹ç›® - åŸºå‡†å¯¹æ¯”åˆ†æå·¥å…·

è¯¥å·¥å…·ç”¨äºå¯¹æ¯”ä¸åŒç‰ˆæœ¬/ç­–ç•¥çš„æ€§èƒ½è¡¨ç°ï¼ŒåŒ…æ‹¬ï¼š
1. ä¼˜åŒ–å‰åçš„æ€§èƒ½å¯¹æ¯”
2. ä¸åŒé›†æˆç­–ç•¥çš„æ•ˆæœåˆ†æ
3. ç‰¹å¾å·¥ç¨‹æ”¹è¿›æ•ˆæœè¯„ä¼°
4. æ€§èƒ½è¶‹åŠ¿åˆ†æ
5. ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ

ä½œè€…: iFlow AIç³»ç»Ÿ
æ—¥æœŸ: 2025-11-11
"""

import sys
import os
import json
import warnings
import traceback
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
import numpy as np
import pandas as pd
from dataclasses import dataclass
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import statsmodels.stats.api as sms

# æŠ‘åˆ¶è­¦å‘Š
warnings.filterwarnings('ignore')

# è®¾ç½®matplotlibä¸­æ–‡æ”¯æŒ
plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False


@dataclass
class BenchmarkResult:
    """åŸºå‡†æµ‹è¯•ç»“æœæ•°æ®ç±»"""
    strategy: str
    version: str  # 'baseline', 'optimized', 'advanced'
    performance_metrics: Dict[str, float]
    timing_metrics: Dict[str, float]
    memory_metrics: Dict[str, float]
    stability_metrics: Dict[str, float]
    test_metadata: Dict[str, Any]
    timestamp: str
    data_size: int
    features_used: int


class BenchmarkComparator:
    """åŸºå‡†å¯¹æ¯”åˆ†æå™¨"""
    
    def __init__(self):
        self.baseline_results: List[BenchmarkResult] = []
        self.comparison_results: List[BenchmarkResult] = []
        self.comparison_data: Dict[str, Any] = {}
        
    def load_baseline_results(self, baseline_file: str) -> bool:
        """åŠ è½½åŸºçº¿ç»“æœ"""
        try:
            if os.path.exists(benchmark_file):
                with open(benchmark_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # è½¬æ¢ä¸ºBenchmarkResultå¯¹è±¡
                for item in data:
                    result = BenchmarkResult(
                        strategy=item.get('strategy', 'unknown'),
                        version=item.get('version', 'baseline'),
                        performance_metrics=item.get('performance', {}),
                        timing_metrics=item.get('timing', {}),
                        memory_metrics=item.get('memory', {}),
                        stability_metrics=item.get('stability', {}),
                        test_metadata=item.get('metadata', {}),
                        timestamp=item.get('timestamp', datetime.now().isoformat()),
                        data_size=item.get('data_size', 1000),
                        features_used=item.get('features_used', 20)
                    )
                    self.baseline_results.append(result)
                
                print(f"âœ… æˆåŠŸåŠ è½½ {len(self.baseline_results)} ä¸ªåŸºçº¿ç»“æœ")
                return True
            else:
                print(f"âš ï¸ åŸºçº¿æ–‡ä»¶ä¸å­˜åœ¨: {benchmark_file}")
                return False
                
        except Exception as e:
            print(f"âŒ åŠ è½½åŸºçº¿ç»“æœå¤±è´¥: {e}")
            return False
    
    def load_comparison_results(self, comparison_file: str) -> bool:
        """åŠ è½½å¯¹æ¯”ç»“æœ"""
        try:
            if os.path.exists(comparison_file):
                with open(comparison_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # è½¬æ¢ä¸ºBenchmarkResultå¯¹è±¡
                for item in data:
                    result = BenchmarkResult(
                        strategy=item.get('strategy', 'unknown'),
                        version=item.get('version', 'optimized'),
                        performance_metrics=item.get('performance', {}),
                        timing_metrics=item.get('timing', {}),
                        memory_metrics=item.get('memory', {}),
                        stability_metrics=item.get('stability', {}),
                        test_metadata=item.get('metadata', {}),
                        timestamp=item.get('timestamp', datetime.now().isoformat()),
                        data_size=item.get('data_size', 1000),
                        features_used=item.get('features_used', 20)
                    )
                    self.comparison_results.append(result)
                
                print(f"âœ… æˆåŠŸåŠ è½½ {len(self.comparison_results)} ä¸ªå¯¹æ¯”ç»“æœ")
                return True
            else:
                print(f"âš ï¸ å¯¹æ¯”æ–‡ä»¶ä¸å­˜åœ¨: {comparison_file}")
                return False
                
        except Exception as e:
            print(f"âŒ åŠ è½½å¯¹æ¯”ç»“æœå¤±è´¥: {e}")
            return False
    
    def generate_synthetic_baseline(self) -> List[BenchmarkResult]:
        """ç”ŸæˆåˆæˆåŸºçº¿æ•°æ®ï¼ˆç”¨äºæ¼”ç¤ºï¼‰"""
        print("ğŸ”§ ç”ŸæˆåˆæˆåŸºçº¿æ•°æ®...")
        
        # æ¨¡æ‹Ÿä¸åŒç­–ç•¥çš„åŸºçº¿æ€§èƒ½
        baseline_strategies = [
            ('å•ä¸ªåŸºçº¿æ¨¡å‹', {'mse': 0.257, 'mae': 0.410, 'rmse': 0.507}),
            ('ç®€å•å¹³å‡é›†æˆ', {'mse': 0.257, 'mae': 0.410, 'rmse': 0.507}),
            ('åŠ æƒå¹³å‡é›†æˆ', {'mse': 0.257, 'mae': 0.410, 'rmse': 0.507}),
        ]
        
        baseline_data = []
        for strategy, metrics in baseline_strategies:
            result = BenchmarkResult(
                strategy=strategy,
                version='baseline',
                performance_metrics=metrics,
                timing_metrics={'total_time': 0.5, 'training_time': 0.3, 'prediction_time': 0.2},
                memory_metrics={'usage': 50.0, 'peak': 80.0},
                stability_metrics={'consistency': 0.80, 'variance': 0.02},
                test_metadata={'model_type': 'random_forest', 'n_estimators': 100},
                timestamp='2025-01-01T00:00:00',
                data_size=1000,
                features_used=20
            )
            baseline_data.append(result)
        
        self.baseline_results = baseline_data
        print(f"âœ… ç”Ÿæˆ {len(baseline_data)} ä¸ªåŸºçº¿ç»“æœ")
        return baseline_data
    
    def generate_synthetic_optimized(self) -> List[BenchmarkResult]:
        """ç”Ÿæˆåˆæˆä¼˜åŒ–æ•°æ®ï¼ˆç”¨äºæ¼”ç¤ºï¼‰"""
        print("ğŸ”§ ç”Ÿæˆåˆæˆä¼˜åŒ–æ•°æ®...")
        
        # æ¨¡æ‹Ÿä¼˜åŒ–åçš„æ€§èƒ½ï¼ˆæ”¹è¿›15-25%ï¼‰
        optimized_strategies = [
            ('åŠ¨æ€æƒé‡é›†æˆ', {'mse': 0.248, 'mae': 0.403, 'rmse': 0.498}),
            ('æ™ºèƒ½ç‰¹å¾å·¥ç¨‹', {'mse': 0.230, 'mae': 0.385, 'rmse': 0.480}),
            ('è‡ªé€‚åº”æ—¶é—´çª—å£', {'mse': 0.242, 'mae': 0.395, 'rmse': 0.492}),
            ('è¶…å‚æ•°ä¼˜åŒ–', {'mse': 0.235, 'mae': 0.390, 'rmse': 0.485}),
        ]
        
        optimized_data = []
        for strategy, metrics in optimized_strategies:
            result = BenchmarkResult(
                strategy=strategy,
                version='optimized',
                performance_metrics=metrics,
                timing_metrics={'total_time': 0.6, 'training_time': 0.35, 'prediction_time': 0.25},
                memory_metrics={'usage': 60.0, 'peak': 95.0},
                stability_metrics={'consistency': 0.92, 'variance': 0.015},
                test_metadata={'model_type': 'ensemble', 'optimization': True},
                timestamp=datetime.now().isoformat(),
                data_size=1000,
                features_used=45
            )
            optimized_data.append(result)
        
        self.comparison_results = optimized_data
        print(f"âœ… ç”Ÿæˆ {len(optimized_data)} ä¸ªä¼˜åŒ–ç»“æœ")
        return optimized_data
    
    def compare_performance(self) -> Dict[str, Any]:
        """å¯¹æ¯”æ€§èƒ½è¡¨ç°"""
        print("\nğŸ“Š æ‰§è¡Œæ€§èƒ½å¯¹æ¯”åˆ†æ...")
        
        comparison = {
            'summary': {},
            'by_strategy': {},
            'statistical_tests': {},
            'improvements': []
        }
        
        # ç­–ç•¥å¯¹æ¯”
        strategies = set([r.strategy for r in self.baseline_results + self.comparison_results])
        
        for strategy in strategies:
            baseline_result = next((r for r in self.baseline_results if r.strategy == strategy), None)
            optimized_result = next((r for r in self.comparison_results if r.strategy == strategy), None)
            
            if baseline_result and optimized_result:
                strategy_comparison = self._compare_strategy(baseline_result, optimized_result)
                comparison['by_strategy'][strategy] = strategy_comparison
            elif optimized_result:
                # åªæœ‰ä¼˜åŒ–ç‰ˆæœ¬çš„ç»“æœ
                comparison['by_strategy'][strategy] = self._analyze_single_result(optimized_result)
        
        # æ•´ä½“æ‘˜è¦
        comparison['summary'] = self._generate_summary_stats(comparison['by_strategy'])
        
        # ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ
        if len(self.baseline_results) > 0 and len(self.comparison_results) > 0:
            comparison['statistical_tests'] = self._perform_statistical_tests()
        
        # æ”¹è¿›æ•ˆæœåˆ†æ
        comparison['improvements'] = self._analyze_improvements(comparison['by_strategy'])
        
        self.comparison_data = comparison
        return comparison
    
    def _compare_strategy(self, baseline: BenchmarkResult, optimized: BenchmarkResult) -> Dict[str, Any]:
        """å¯¹æ¯”å•ä¸ªç­–ç•¥çš„æ€§èƒ½"""
        comparison = {
            'baseline_metrics': baseline.performance_metrics,
            'optimized_metrics': optimized.performance_metrics,
            'improvements': {},
            'relative_improvements': {},
            'timing_comparison': {},
            'stability_comparison': {},
            'effect_size': {}
        }
        
        # æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”
        for metric in baseline.performance_metrics.keys():
            if metric in optimized.performance_metrics:
                baseline_val = baseline.performance_metrics[metric]
                optimized_val = optimized.performance_metrics[metric]
                
                if baseline_val != 0:
                    improvement = baseline_val - optimized_val
                    relative_improvement = improvement / baseline_val * 100
                else:
                    improvement = 0
                    relative_improvement = 0
                
                comparison['improvements'][metric] = improvement
                comparison['relative_improvements'][metric] = relative_improvement
        
        # æ—¶é—´å¯¹æ¯”
        comparison['timing_comparison'] = {
            'baseline_time': baseline.timing_metrics,
            'optimized_time': optimized.timing_metrics,
            'time_ratio': optimized.timing_metrics.get('total_time', 1) / baseline.timing_metrics.get('total_time', 1)
        }
        
        # ç¨³å®šæ€§å¯¹æ¯”
        comparison['stability_comparison'] = {
            'baseline_stability': baseline.stability_metrics,
            'optimized_stability': optimized.stability_metrics,
            'stability_improvement': (
                optimized.stability_metrics.get('consistency', 0) - 
                baseline.stability_metrics.get('consistency', 0)
            )
        }
        
        # æ•ˆåº”å¤§å°è®¡ç®—ï¼ˆCohen's dï¼‰
        if 'mse' in baseline.performance_metrics and 'mse' in optimized.performance_metrics:
            baseline_mse = baseline.performance_metrics['mse']
            optimized_mse = optimized.performance_metrics['mse']
            
            # æ¨¡æ‹Ÿæ ‡å‡†å·®
            baseline_std = 0.01
            optimized_std = 0.008
            
            pooled_std = np.sqrt((baseline_std**2 + optimized_std**2) / 2)
            cohens_d = (baseline_mse - optimized_mse) / pooled_std
            
            comparison['effect_size'] = {
                'cohens_d': cohens_d,
                'interpretation': self._interpret_effect_size(cohens_d)
            }
        
        return comparison
    
    def _analyze_single_result(self, result: BenchmarkResult) -> Dict[str, Any]:
        """åˆ†æå•ä¸ªç»“æœï¼ˆæ²¡æœ‰å¯¹æ¯”ï¼‰"""
        return {
            'baseline_metrics': {},
            'optimized_metrics': result.performance_metrics,
            'improvements': {},
            'relative_improvements': {},
            'timing_comparison': {'baseline_time': {}, 'optimized_time': result.timing_metrics},
            'stability_comparison': {'baseline_stability': {}, 'optimized_stability': result.stability_metrics},
            'effect_size': {}
        }
    
    def _generate_summary_stats(self, strategy_comparisons: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆæ‘˜è¦ç»Ÿè®¡"""
        summary = {
            'total_strategies': len(strategy_comparisons),
            'avg_improvements': {},
            'best_improvements': {},
            'consistency_score': 0,
            'overall_effectiveness': 0
        }
        
        # è®¡ç®—å¹³å‡æ”¹è¿›
        all_improvements = {metric: [] for metric in ['mse', 'mae', 'rmse']}
        
        for strategy, comparison in strategy_comparisons.items():
            for metric in ['mse', 'mae', 'rmse']:
                if metric in comparison['relative_improvements']:
                    all_improvements[metric].append(comparison['relative_improvements'][metric])
        
        for metric, improvements in all_improvements.items():
            if improvements:
                summary['avg_improvements'][metric] = {
                    'mean': np.mean(improvements),
                    'std': np.std(improvements),
                    'min': np.min(improvements),
                    'max': np.max(improvements)
                }
        
        # æœ€ä½³æ”¹è¿›
        for metric in ['mse', 'mae', 'rmse']:
            if metric in all_improvements and all_improvements[metric]:
                best_idx = np.argmax(all_improvements[metric])
                best_strategy = list(strategy_comparisons.keys())[best_idx]
                summary['best_improvements'][metric] = {
                    'strategy': best_strategy,
                    'improvement': all_improvements[metric][best_idx]
                }
        
        # ä¸€è‡´æ€§è¯„åˆ†ï¼ˆæ‰€æœ‰æŒ‡æ ‡æ”¹è¿›çš„ç­–ç•¥æ¯”ä¾‹ï¼‰
        consistent_strategies = 0
        total_strategies = len(strategy_comparisons)
        
        for strategy, comparison in strategy_comparisons.items():
            positive_improvements = 0
            total_metrics = 0
            
            for metric in ['mse', 'mae', 'rmse']:
                if metric in comparison['relative_improvements']:
                    total_metrics += 1
                    if comparison['relative_improvements'][metric] > 0:
                        positive_improvements += 1
            
            if total_metrics > 0 and positive_improvements / total_metrics > 0.5:
                consistent_strategies += 1
        
        summary['consistency_score'] = consistent_strategies / total_strategies if total_strategies > 0 else 0
        
        # æ•´ä½“æœ‰æ•ˆæ€§è¯„åˆ†
        if 'mse' in summary['avg_improvements']:
            mse_improvement = summary['avg_improvements']['mse']['mean']
            summary['overall_effectiveness'] = min(100, max(0, mse_improvement * 4))  # 0-100åˆ†åˆ¶
        
        return summary
    
    def _perform_statistical_tests(self) -> Dict[str, Any]:
        """æ‰§è¡Œç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ"""
        tests = {}
        
        # å‡†å¤‡æ•°æ®
        baseline_mse = [r.performance_metrics.get('mse', 0) for r in self.baseline_results if 'mse' in r.performance_metrics]
        optimized_mse = [r.performance_metrics.get('mse', 0) for r in self.comparison_results if 'mse' in r.performance_metrics]
        
        if len(baseline_mse) > 1 and len(optimized_mse) > 1:
            # tæ£€éªŒ
            t_stat, p_value = stats.ttest_ind(baseline_mse, optimized_mse)
            tests['t_test'] = {
                't_statistic': t_stat,
                'p_value': p_value,
                'significant': p_value < 0.05,
                'interpretation': 'æ˜¾è‘—å·®å¼‚' if p_value < 0.05 else 'æ— æ˜¾è‘—å·®å¼‚'
            }
            
            # æ•ˆåº”å¤§å° (Cohen's d)
            pooled_std = np.sqrt(((len(baseline_mse) - 1) * np.var(baseline_mse, ddof=1) + 
                                 (len(optimized_mse) - 1) * np.var(optimized_mse, ddof=1)) / 
                                (len(baseline_mse) + len(optimized_mse) - 2))
            
            if pooled_std > 0:
                cohens_d = (np.mean(baseline_mse) - np.mean(optimized_mse)) / pooled_std
                tests['effect_size'] = {
                    'cohens_d': cohens_d,
                    'magnitude': self._interpret_effect_size(cohens_d)
                }
        
        return tests
    
    def _interpret_effect_size(self, cohens_d: float) -> str:
        """è§£é‡Šæ•ˆåº”å¤§å°"""
        abs_d = abs(cohens_d)
        if abs_d < 0.2:
            return "å°æ•ˆåº”"
        elif abs_d < 0.5:
            return "ä¸­ç­‰æ•ˆåº”"
        elif abs_d < 0.8:
            return "å¤§æ•ˆåº”"
        else:
            return "éå¸¸å¤§æ•ˆåº”"
    
    def _analyze_improvements(self, strategy_comparisons: Dict[str, Any]) -> List[Dict[str, Any]]:
        """åˆ†ææ”¹è¿›æ•ˆæœ"""
        improvements = []
        
        for strategy, comparison in strategy_comparisons.items():
            for metric, improvement in comparison['relative_improvements'].items():
                improvements.append({
                    'strategy': strategy,
                    'metric': metric,
                    'improvement_percent': improvement,
                    'magnitude': 'å¤§' if improvement > 10 else 'ä¸­' if improvement > 5 else 'å°'
                })
        
        # æŒ‰æ”¹è¿›å¹…åº¦æ’åº
        improvements.sort(key=lambda x: x['improvement_percent'], reverse=True)
        
        return improvements
    
    def generate_visualization_plots(self, output_dir: str = "benchmark_analysis"):
        """ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨"""
        print(f"\nğŸ“Š ç”Ÿæˆæ€§èƒ½å¯¹æ¯”å¯è§†åŒ–å›¾è¡¨...")
        
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        try:
            # 1. æ€§èƒ½å¯¹æ¯”æ¡å½¢å›¾
            self._plot_performance_comparison(output_path)
            
            # 2. æ”¹è¿›æ•ˆæœåˆ†å¸ƒå›¾
            self._plot_improvement_distribution(output_path)
            
            # 3. æ—¶é—´æ•ˆç‡å¯¹æ¯”å›¾
            self._plot_timing_comparison(output_path)
            
            # 4. ç¨³å®šæ€§åˆ†æå›¾
            self._plot_stability_analysis(output_path)
            
            # 5. ç»¼åˆé›·è¾¾å›¾
            self._plot_comprehensive_radar(output_path)
            
            print(f"âœ… å›¾è¡¨å·²ä¿å­˜åˆ°: {output_path}")
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå›¾è¡¨å¤±è´¥: {e}")
    
    def _plot_performance_comparison(self, output_path: Path):
        """ç»˜åˆ¶æ€§èƒ½å¯¹æ¯”æ¡å½¢å›¾"""
        if not self.comparison_data or 'by_strategy' not in self.comparison_data:
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('Hull Tacticalé¡¹ç›®æ€§èƒ½å¯¹æ¯”åˆ†æ', fontsize=16, fontweight='bold')
        
        metrics = ['mse', 'mae', 'rmse']
        colors = ['#2E86AB', '#A23B72', '#F18F01']
        
        for idx, metric in enumerate(metrics):
            ax = axes[idx // 2, idx % 2]
            
            strategies = []
            baseline_values = []
            optimized_values = []
            
            for strategy, comparison in self.comparison_data['by_strategy'].items():
                strategies.append(strategy)
                baseline_values.append(comparison['baseline_metrics'].get(metric, 0))
                optimized_values.append(comparison['optimized_metrics'].get(metric, 0))
            
            x = np.arange(len(strategies))
            width = 0.35
            
            bars1 = ax.bar(x - width/2, baseline_values, width, label='åŸºçº¿', alpha=0.7, color='lightcoral')
            bars2 = ax.bar(x + width/2, optimized_values, width, label='ä¼˜åŒ–', alpha=0.7, color='skyblue')
            
            ax.set_title(f'{metric.upper()} æ€§èƒ½å¯¹æ¯”')
            ax.set_xlabel('ç­–ç•¥')
            ax.set_ylabel(f'{metric.upper()} å€¼')
            ax.set_xticks(x)
            ax.set_xticklabels(strategies, rotation=45, ha='right')
            ax.legend()
            ax.grid(True, alpha=0.3)
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bars in [bars1, bars2]:
                for bar in bars:
                    height = bar.get_height()
                    ax.text(bar.get_x() + bar.get_width()/2., height,
                           f'{height:.3f}', ha='center', va='bottom', fontsize=8)
        
        # ç©ºç™½å­å›¾
        axes[1, 1].axis('off')
        
        plt.tight_layout()
        plt.savefig(output_path / 'performance_comparison.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_improvement_distribution(self, output_path: Path):
        """ç»˜åˆ¶æ”¹è¿›æ•ˆæœåˆ†å¸ƒå›¾"""
        if not self.comparison_data or 'improvements' not in self.comparison_data:
            return
        
        improvements = self.comparison_data['improvements']
        if not improvements:
            return
        
        # å‡†å¤‡æ•°æ®
        improvement_data = []
        for imp in improvements:
            improvement_data.append({
                'Strategy': imp['strategy'],
                'Metric': imp['metric'].upper(),
                'Improvement': imp['improvement_percent']
            })
        
        df = pd.DataFrame(improvement_data)
        
        # ç®±çº¿å›¾
        plt.figure(figsize=(12, 8))
        
        plt.subplot(2, 1, 1)
        sns.boxplot(data=df, x='Metric', y='Improvement', palette='Set2')
        plt.title('æ”¹è¿›å¹…åº¦åˆ†å¸ƒ (æŒ‰æŒ‡æ ‡)')
        plt.ylabel('æ”¹è¿›ç™¾åˆ†æ¯” (%)')
        plt.grid(True, alpha=0.3)
        
        # æ¡å½¢å›¾
        plt.subplot(2, 1, 2)
        sns.barplot(data=df, x='Improvement', y='Strategy', hue='Metric', palette='Set1')
        plt.title('å„ç­–ç•¥æ”¹è¿›æ•ˆæœ')
        plt.xlabel('æ”¹è¿›ç™¾åˆ†æ¯” (%)')
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(output_path / 'improvement_distribution.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_timing_comparison(self, output_path: Path):
        """ç»˜åˆ¶æ—¶é—´æ•ˆç‡å¯¹æ¯”å›¾"""
        if not self.comparison_data or 'by_strategy' not in self.comparison_data:
            return
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        fig.suptitle('æ—¶é—´æ•ˆç‡åˆ†æ', fontsize=16, fontweight='bold')
        
        strategies = list(self.comparison_data['by_strategy'].keys())
        
        # è®­ç»ƒæ—¶é—´å¯¹æ¯”
        baseline_times = []
        optimized_times = []
        
        for strategy in strategies:
            comparison = self.comparison_data['by_strategy'][strategy]
            baseline_times.append(comparison['timing_comparison']['baseline_time'].get('total_time', 0))
            optimized_times.append(comparison['timing_comparison']['optimized_time'].get('total_time', 0))
        
        x = np.arange(len(strategies))
        width = 0.35
        
        bars1 = ax1.bar(x - width/2, baseline_times, width, label='åŸºçº¿', alpha=0.7, color='lightcoral')
        bars2 = ax1.bar(x + width/2, optimized_times, width, label='ä¼˜åŒ–', alpha=0.7, color='skyblue')
        
        ax1.set_title('æ‰§è¡Œæ—¶é—´å¯¹æ¯”')
        ax1.set_xlabel('ç­–ç•¥')
        ax1.set_ylabel('æ—¶é—´ (ç§’)')
        ax1.set_xticks(x)
        ax1.set_xticklabels(strategies, rotation=45, ha='right')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # æ—¶é—´æ¯”ç‡
        time_ratios = [opt/base if base > 0 else 1 for opt, base in zip(optimized_times, baseline_times)]
        
        bars = ax2.bar(strategies, time_ratios, alpha=0.7, color='gold')
        ax2.set_title('æ—¶é—´æ¯”ç‡ (ä¼˜åŒ–/åŸºçº¿)')
        ax2.set_ylabel('æ¯”ç‡')
        ax2.set_xticklabels(strategies, rotation=45, ha='right')
        ax2.grid(True, alpha=0.3)
        ax2.axhline(y=1, color='red', linestyle='--', alpha=0.7, label='åŸºå‡†çº¿')
        ax2.legend()
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, ratio in zip(bars, time_ratios):
            height = bar.get_height()
            ax2.text(bar.get_x() + bar.get_width()/2., height,
                    f'{ratio:.2f}', ha='center', va='bottom')
        
        plt.tight_layout()
        plt.savefig(output_path / 'timing_comparison.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_stability_analysis(self, output_path: Path):
        """ç»˜åˆ¶ç¨³å®šæ€§åˆ†æå›¾"""
        if not self.comparison_data or 'by_strategy' not in self.comparison_data:
            return
        
        strategies = list(self.comparison_data['by_strategy'].keys())
        
        baseline_consistency = []
        optimized_consistency = []
        
        for strategy in strategies:
            comparison = self.comparison_data['by_strategy'][strategy]
            baseline_consistency.append(comparison['stability_comparison']['baseline_stability'].get('consistency', 0))
            optimized_consistency.append(comparison['stability_comparison']['optimized_stability'].get('consistency', 0))
        
        # æ•£ç‚¹å›¾
        plt.figure(figsize=(10, 8))
        
        plt.scatter(baseline_consistency, optimized_consistency, s=100, alpha=0.7, c=range(len(strategies)), cmap='viridis')
        
        for i, strategy in enumerate(strategies):
            plt.annotate(strategy, (baseline_consistency[i], optimized_consistency[i]), 
                        xytext=(5, 5), textcoords='offset points', fontsize=9)
        
        # æ·»åŠ å¯¹è§’çº¿
        min_val = min(min(baseline_consistency), min(optimized_consistency))
        max_val = max(max(baseline_consistency), max(optimized_consistency))
        plt.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.7, label='y=x (æ— æ”¹è¿›)')
        
        plt.xlabel('åŸºçº¿ç¨³å®šæ€§')
        plt.ylabel('ä¼˜åŒ–åç¨³å®šæ€§')
        plt.title('ç¨³å®šæ€§æ”¹è¿›åˆ†æ')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # é¢œè‰²æ¡
        cbar = plt.colorbar()
        cbar.set_label('ç­–ç•¥ç´¢å¼•')
        
        plt.tight_layout()
        plt.savefig(output_path / 'stability_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_comprehensive_radar(self, output_path: Path):
        """ç»˜åˆ¶ç»¼åˆé›·è¾¾å›¾"""
        if not self.comparison_data or 'summary' not in self.comparison_data:
            return
        
        summary = self.comparison_data['summary']
        
        # å‡†å¤‡é›·è¾¾å›¾æ•°æ®
        categories = ['MSEæ”¹è¿›', 'MAEæ”¹è¿›', 'RMSEæ”¹è¿›', 'ç¨³å®šæ€§', 'æ—¶é—´æ•ˆç‡', 'æ•´ä½“æ•ˆæœ']
        
        # è®¡ç®—å„ç»´åº¦è¯„åˆ† (0-100)
        scores = []
        
        if 'avg_improvements' in summary:
            # æ€§èƒ½æ”¹è¿›è¯„åˆ† (è½¬æ¢ä¸º0-100)
            mse_score = min(100, max(0, summary['avg_improvements'].get('mse', {}).get('mean', 0) * 10))
            mae_score = min(100, max(0, summary['avg_improvements'].get('mae', {}).get('mean', 0) * 10))
            rmse_score = min(100, max(0, summary['avg_improvements'].get('rmse', {}).get('mean', 0) * 10))
        else:
            mse_score = mae_score = rmse_score = 0
        
        # ç¨³å®šæ€§è¯„åˆ†
        stability_score = summary.get('consistency_score', 0) * 100
        
        # æ—¶é—´æ•ˆç‡è¯„åˆ† (æ—¶é—´æ¯”ç‡çš„å€’æ•°)
        if 'by_strategy' in self.comparison_data:
            time_ratios = []
            for comparison in self.comparison_data['by_strategy'].values():
                baseline_time = comparison['timing_comparison']['baseline_time'].get('total_time', 1)
                optimized_time = comparison['timing_comparison']['optimized_time'].get('total_time', 1)
                if baseline_time > 0:
                    time_ratios.append(optimized_time / baseline_time)
            
            avg_time_ratio = np.mean(time_ratios) if time_ratios else 1
            time_score = max(0, min(100, (2 - avg_time_ratio) * 50))  # æ¯”ç‡è¶Šä½åˆ†æ•°è¶Šé«˜
        else:
            time_score = 50
        
        # æ•´ä½“æ•ˆæœè¯„åˆ†
        effectiveness_score = summary.get('overall_effectiveness', 0)
        
        scores = [mse_score, mae_score, rmse_score, stability_score, time_score, effectiveness_score]
        
        # é›·è¾¾å›¾
        angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()
        scores += scores[:1]  # é—­åˆå›¾å½¢
        angles += angles[:1]
        
        fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))
        
        ax.plot(angles, scores, 'o-', linewidth=2, label='ä¼˜åŒ–åè¡¨ç°', color='#2E86AB')
        ax.fill(angles, scores, alpha=0.25, color='#2E86AB')
        
        # æ·»åŠ åŸºå‡†çº¿ (50åˆ†)
        baseline_scores = [50] * len(angles)
        ax.plot(angles, baseline_scores, '--', linewidth=1, label='åŸºå‡†æ°´å¹³', color='gray', alpha=0.7)
        
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(categories)
        ax.set_ylim(0, 100)
        ax.set_yticks([20, 40, 60, 80, 100])
        ax.set_yticklabels(['20', '40', '60', '80', '100'])
        ax.grid(True)
        
        ax.set_title('Hull Tacticalé¡¹ç›®ç»¼åˆæ€§èƒ½é›·è¾¾å›¾', size=16, fontweight='bold', pad=20)
        ax.legend(loc='upper right', bbox_to_anchor=(1.2, 1.0))
        
        plt.tight_layout()
        plt.savefig(output_path / 'comprehensive_radar.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def save_comparison_report(self, output_file: str = "benchmark_comparison_report.json"):
        """ä¿å­˜å¯¹æ¯”åˆ†ææŠ¥å‘Š"""
        if not self.comparison_data:
            print("âš ï¸ æ²¡æœ‰å¯¹æ¯”æ•°æ®å¯ä¿å­˜")
            return None
        
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(self.comparison_data, f, indent=2, ensure_ascii=False, default=str)
            
            print(f"âœ… å¯¹æ¯”æŠ¥å‘Šå·²ä¿å­˜: {output_file}")
            return output_file
            
        except Exception as e:
            print(f"âŒ ä¿å­˜å¯¹æ¯”æŠ¥å‘Šå¤±è´¥: {e}")
            return None


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ Hull Tacticalé¡¹ç›®åŸºå‡†å¯¹æ¯”åˆ†æå·¥å…·")
    print("=" * 60)
    
    try:
        comparator = BenchmarkComparator()
        
        # ç”Ÿæˆç¤ºä¾‹æ•°æ®ï¼ˆå®é™…ä½¿ç”¨æ—¶åº”åŠ è½½çœŸå®æ•°æ®ï¼‰
        print("ğŸ”§ ç”Ÿæˆç¤ºä¾‹æ•°æ®è¿›è¡Œæ¼”ç¤º...")
        baseline_results = comparator.generate_synthetic_baseline()
        comparison_results = comparator.generate_synthetic_optimized()
        
        # æ‰§è¡Œå¯¹æ¯”åˆ†æ
        print("\nğŸ“Š æ‰§è¡Œæ€§èƒ½å¯¹æ¯”åˆ†æ...")
        comparison = comparator.compare_performance()
        
        # ç”Ÿæˆå¯è§†åŒ–
        print("\nğŸ“ˆ ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
        comparator.generate_visualization_plots()
        
        # ä¿å­˜æŠ¥å‘Š
        print("\nğŸ’¾ ä¿å­˜åˆ†ææŠ¥å‘Š...")
        report_file = comparator.save_comparison_report()
        
        # æ‰“å°æ‘˜è¦
        print("\n" + "="*60)
        print("ğŸ“‹ åŸºå‡†å¯¹æ¯”åˆ†ææ‘˜è¦")
        print("="*60)
        
        if 'summary' in comparison:
            summary = comparison['summary']
            print(f"å¯¹æ¯”ç­–ç•¥æ•°: {summary.get('total_strategies', 0)}")
            print(f"ä¸€è‡´æ€§è¯„åˆ†: {summary.get('consistency_score', 0):.1%}")
            print(f"æ•´ä½“æ•ˆæœè¯„åˆ†: {summary.get('overall_effectiveness', 0):.1f}/100")
            
            if 'avg_improvements' in summary:
                for metric, stats in summary['avg_improvements'].items():
                    print(f"{metric.upper()}å¹³å‡æ”¹è¿›: {stats.get('mean', 0):.1f}%")
        
        if 'statistical_tests' in comparison and comparison['statistical_tests']:
            tests = comparison['statistical_tests']
            if 't_test' in tests:
                t_test = tests['t_test']
                print(f"ç»Ÿè®¡æ˜¾è‘—æ€§: {t_test['interpretation']} (p={t_test['p_value']:.4f})")
        
        print(f"\nğŸ“Š è¯¦ç»†æŠ¥å‘Š: {report_file}")
        print(f"ğŸ“ˆ å¯è§†åŒ–å›¾è¡¨: benchmark_analysis/ ç›®å½•")
        
        return comparison
        
    except Exception as e:
        print(f"\nâŒ åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        traceback.print_exc()
        return None


if __name__ == "__main__":
    # è®¾ç½®åŸºå‡†æ–‡ä»¶è·¯å¾„
    benchmark_file = "working/simple_ensemble_benchmark.json"  # åŸºçº¿ç»“æœæ–‡ä»¶
    main()
