# Hull Tactical - è¶…å‚æ•°è°ƒä¼˜ç³»ç»Ÿä½¿ç”¨æŒ‡å—

## ğŸ“‹ æ¦‚è¿°

Hull Tacticalé¡¹ç›®ç°å·²é›†æˆå®Œæ•´çš„æ™ºèƒ½è¶…å‚æ•°è°ƒä¼˜ç³»ç»Ÿï¼Œæ”¯æŒå¤šæ¨¡å‹ã€å¤šç›®æ ‡ä¼˜åŒ–ã€æ—¶é—´åºåˆ—éªŒè¯å’Œè‡ªåŠ¨åŒ–æŠ¥å‘Šç”Ÿæˆã€‚æœ¬ç³»ç»ŸåŸºäºOptunaå®ç°è´å¶æ–¯ä¼˜åŒ–ï¼Œé‡‡ç”¨æ—¶é—´åºåˆ—å‹å¥½çš„éªŒè¯ç­–ç•¥ï¼Œç¡®ä¿åœ¨é‡‘èæ—¶é—´åºåˆ—æ•°æ®ä¸Šçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ¯ æ ¸å¿ƒåŠŸèƒ½

### 1. **æ™ºèƒ½è°ƒä¼˜ç­–ç•¥**
- **è´å¶æ–¯ä¼˜åŒ–**: ä½¿ç”¨Optunaå®ç°é«˜æ•ˆçš„è¶…å‚æ•°æœç´¢
- **å¤šç›®æ ‡ä¼˜åŒ–**: åŒæ—¶ä¼˜åŒ–MSEã€MAEã€RÂ²ç­‰å¤šä¸ªæŒ‡æ ‡
- **è‡ªé€‚åº”æœç´¢**: åŸºäºå†å²æ€§èƒ½åŠ¨æ€è°ƒæ•´æœç´¢ç­–ç•¥
- **æ—©åœæœºåˆ¶**: é˜²æ­¢è¿‡æ‹Ÿåˆå’Œèµ„æºæµªè´¹

### 2. **æ—¶é—´åºåˆ—å‹å¥½éªŒè¯**
- **TimeSeriesSplit**: ä¿æŒæ—¶é—´é¡ºåºçš„äº¤å‰éªŒè¯
- **æ»šåŠ¨çª—å£**: æ¨¡æ‹ŸçœŸå®äº¤æ˜“ç¯å¢ƒçš„éªŒè¯
- **æ‰©å±•çª—å£**: è€ƒè™‘å†å²ä¿¡æ¯ç´¯ç§¯çš„éªŒè¯
- **é˜²æ­¢æ•°æ®æ³„éœ²**: ä¸¥æ ¼çš„æ—¶é—´åºåˆ—åˆ†å‰²

### 3. **å¤šæ¨¡å‹æ”¯æŒ**
- **LightGBM**: æ”¯æŒå®Œæ•´çš„å‚æ•°ç©ºé—´ä¼˜åŒ–
- **XGBoost**: åŒ…å«æ‰€æœ‰å…³é”®è¶…å‚æ•°
- **CatBoost**: æ”¯æŒæ·±åº¦å‚æ•°è°ƒä¼˜
- **Random Forest**: åŸºç¡€æ¨¡å‹å¯¹æ¯”åŸºå‡†

### 4. **è‡ªåŠ¨åŒ–åˆ†ææŠ¥å‘Š**
- **æ€§èƒ½å¯¹æ¯”å›¾è¡¨**: ç›´è§‚çš„æ¨¡å‹æ€§èƒ½å¯è§†åŒ–
- **å‚æ•°é‡è¦æ€§åˆ†æ**: è¯†åˆ«å…³é”®è¶…å‚æ•°
- **HTMLæŠ¥å‘Š**: å®Œæ•´çš„è°ƒä¼˜ç»“æœæŠ¥å‘Š
- **ä¼˜åŒ–å»ºè®®**: æ™ºèƒ½çš„æ€§èƒ½æ”¹è¿›å»ºè®®

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. åŸºç¡€è°ƒä¼˜ï¼ˆæ¨èæ–°æ‰‹ï¼‰

```bash
# è¿è¡Œå¿«é€Ÿè°ƒä¼˜æ¼”ç¤º
cd working
python demo_tuning.py --quick

# æˆ–ç›´æ¥è¿è¡Œå®Œæ•´è°ƒä¼˜
python hyperparameter_tuning.py
```

### 2. è‡ªå®šä¹‰è°ƒä¼˜

```python
from hyperparameter_tuning import TuningConfig, HyperparameterTuner
from lib.data import load_train_data
from lib.features import FeaturePipeline

# åˆ›å»ºè‡ªå®šä¹‰é…ç½®
config = TuningConfig(
    model_types=["lightgbm", "xgboost", "catboost"],
    n_trials=100,        # è¯•éªŒæ¬¡æ•°
    cv_folds=5,          # äº¤å‰éªŒè¯æŠ˜æ•°
    search_strategy="optuna",
    validation_strategy="time_series",
    primary_metric="mse",
    secondary_metrics=["mae", "r2"],
    timeout_seconds=3600  # 1å°æ—¶è¶…æ—¶
)

# åŠ è½½æ•°æ®
train_data = load_train_data("input/hull-tactical-market-prediction")
pipeline = FeaturePipeline(stateful=True)
X = pipeline.fit_transform(train_data)
y = train_data["forward_returns"].fillna(train_data["forward_returns"].median())

# è¿è¡Œè°ƒä¼˜
tuner = HyperparameterTuner(config)
results = tuner.tune_all_models(X, y)

# ä¿å­˜ç»“æœ
tuner.save_results("my_tuning_results.json")
```

### 3. ç»“æœåˆ†æ

```python
from tuning_results import TuningResultAnalyzer

# åˆ›å»ºåˆ†æå™¨
analyzer = TuningResultAnalyzer("tuning_results")
analyzer.load_results()

# ç”Ÿæˆå®Œæ•´åˆ†æ
analyzer.analyze_model_performance()
summary = analyzer.generate_performance_summary()

# ç”ŸæˆæŠ¥å‘Š
analyzer.generate_html_report("my_report.html")
```

## ğŸ“Š é…ç½®è¯¦è§£

### TuningConfig å‚æ•°è¯´æ˜

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `model_types` | List[str] | `["lightgbm", "xgboost", "catboost"]` | è¦è°ƒä¼˜çš„æ¨¡å‹ç±»å‹ |
| `n_trials` | int | `50` | Optunaè¯•éªŒæ¬¡æ•° |
| `cv_folds` | int | `5` | äº¤å‰éªŒè¯æŠ˜å æ•° |
| `search_strategy` | str | `"optuna"` | æœç´¢ç­–ç•¥ï¼šgrid, random, optuna, mixed |
| `validation_strategy` | str | `"time_series"` | éªŒè¯ç­–ç•¥ï¼štime_series, rolling, expanding |
| `primary_metric` | str | `"mse"` | ä¸»è¦ä¼˜åŒ–æŒ‡æ ‡ |
| `secondary_metrics` | List[str] | `["mae", "r2"]` | æ¬¡è¦è¯„ä¼°æŒ‡æ ‡ |
| `timeout_seconds` | int | `1800` | æœ€å¤§è°ƒä¼˜æ—¶é—´(ç§’) |
| `early_stopping_rounds` | int | `50` | æ—©åœè½®æ•° |
| `test_size` | float | `0.2` | æµ‹è¯•é›†æ¯”ä¾‹ |

### æ¨¡å‹å‚æ•°ç©ºé—´

#### LightGBM å‚æ•°ç©ºé—´
```python
{
    "n_estimators": (500, 5000, log=True),
    "learning_rate": (0.005, 0.1, log=True),
    "num_leaves": (16, 512, log=True),
    "max_depth": (3, 15),
    "subsample": (0.6, 1.0),
    "colsample_bytree": (0.6, 1.0),
    "reg_alpha": (0, 10, log=True),
    "reg_lambda": (0, 10, log=True),
    "min_child_samples": (5, 50),
    "boosting_type": ["gbdt", "dart", "goss"]
}
```

#### XGBoost å‚æ•°ç©ºé—´
```python
{
    "n_estimators": (500, 5000, log=True),
    "learning_rate": (0.005, 0.1, log=True),
    "max_depth": (3, 12),
    "subsample": (0.6, 1.0),
    "colsample_bytree": (0.6, 1.0),
    "reg_alpha": (0, 10, log=True),
    "reg_lambda": (0, 10, log=True),
    "gamma": (0, 5, log=True),
    "tree_method": ["hist", "approx"]
}
```

#### CatBoost å‚æ•°ç©ºé—´
```python
{
    "iterations": (500, 5000, log=True),
    "learning_rate": (0.005, 0.1, log=True),
    "depth": (4, 12),
    "l2_leaf_reg": (1, 10, log=True),
    "border_count": (32, 255),
    "bagging_temperature": (0, 1),
    "boosting_type": ["Ordered", "Plain"]
}
```

## ğŸ”§ é«˜çº§åŠŸèƒ½

### 1. å¤šç›®æ ‡ä¼˜åŒ–

ç³»ç»Ÿæ”¯æŒåŒæ—¶ä¼˜åŒ–å¤šä¸ªæŒ‡æ ‡ï¼Œé€šè¿‡åŠ æƒç»„åˆçš„æ–¹å¼ç”Ÿæˆç»¼åˆåˆ†æ•°ï¼š

```python
# è‡ªå®šä¹‰è¯„ä¼°å™¨
from hyperparameter_tuning import MultiObjectiveEvaluator

evaluator = MultiObjectiveEvaluator(
    primary_metric="mse",
    secondary_metrics=["mae", "r2"]
)

# è‡ªå®šä¹‰æƒé‡
evaluator.secondary_weights = {
    "mae": 0.4,  # MAEæƒé‡
    "r2": 0.3    # RÂ²æƒé‡
}
```

### 2. è‡ªé€‚åº”æœç´¢ç­–ç•¥

```python
from hyperparameter_tuning import AdaptiveSearchStrategy

strategy = AdaptiveSearchStrategy(
    strategy="optuna",
    random_state=42
)

# ç³»ç»Ÿä¼šè‡ªåŠ¨æ ¹æ®æ€§èƒ½å†å²è°ƒæ•´æœç´¢ç­–ç•¥
```

### 3. æ—¶é—´åºåˆ—éªŒè¯

æ”¯æŒä¸‰ç§éªŒè¯ç­–ç•¥ï¼š

- **time_series**: æ ‡å‡†æ—¶é—´åºåˆ—åˆ†å‰²
- **rolling**: æ»šåŠ¨çª—å£éªŒè¯
- **expanding**: æ‰©å±•çª—å£éªŒè¯

```python
from hyperparameter_tuning import TimeSeriesValidator

validator = TimeSeriesValidator(
    strategy="rolling",  # ä½¿ç”¨æ»šåŠ¨çª—å£
    n_splits=5,
    test_size=0.2
)
```

## ğŸ“ˆ è¾“å‡ºæ–‡ä»¶è¯´æ˜

### è°ƒä¼˜ç»“æœæ–‡ä»¶
- `tuning_results_YYYYMMDD_HHMMSS.json`: å®Œæ•´è°ƒä¼˜ç»“æœ
- `tuning_results_YYYYMMDD_HHMMSS.pkl`: äºŒè¿›åˆ¶æ ¼å¼ï¼ˆåŒ…å«æ¨¡å‹å¯¹è±¡ï¼‰

### åˆ†ææŠ¥å‘Š
- `performance_comparison.png`: æ€§èƒ½å¯¹æ¯”å›¾è¡¨
- `model_ranking.png`: æ¨¡å‹æ’åå›¾è¡¨
- `tuning_report.html`: å®Œæ•´çš„HTMLåˆ†ææŠ¥å‘Š

### é…ç½®æ–‡ä»¶æ›´æ–°
è°ƒä¼˜ç»“æœä¼šè‡ªåŠ¨ä¿å­˜åˆ°é…ç½®ç³»ç»Ÿï¼Œä¸‹æ¬¡è¿è¡Œä¸»æ¨¡å‹æ—¶ä¼šè‡ªåŠ¨ä½¿ç”¨ä¼˜åŒ–åçš„å‚æ•°ã€‚

## ğŸ’¡ æœ€ä½³å®è·µ

### 1. è°ƒä¼˜ç­–ç•¥å»ºè®®

```python
# å¿«é€ŸåŸå‹éªŒè¯
config = TuningConfig(
    model_types=["lightgbm"],
    n_trials=20,
    cv_folds=3,
    timeout_seconds=600
)

# æ·±åº¦ä¼˜åŒ–
config = TuningConfig(
    model_types=["lightgbm", "xgboost", "catboost"],
    n_trials=200,
    cv_folds=5,
    search_strategy="optuna",
    timeout_seconds=7200  # 2å°æ—¶
)
```

### 2. æ€§èƒ½ç›‘æ§

```python
# ç›‘æ§è°ƒä¼˜è¿‡ç¨‹
def objective(trial):
    params = suggest_params(trial)
    
    # æ£€æŸ¥è®­ç»ƒæ—¶é—´
    start_time = time.time()
    model.fit(X_train, y_train)
    training_time = time.time() - start_time
    
    if training_time > 300:  # è¶…è¿‡5åˆ†é’Ÿå°±åœæ­¢
        raise optuna.TrialPruned()
    
    return evaluate_model(model, X_val, y_val)
```

### 3. æ—©åœé…ç½®

```python
config = TuningConfig(
    n_trials=100,
    early_stopping_rounds=50,
    patience=10,
    min_improvement=0.001
)
```

## ğŸ” æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜åŠè§£å†³æ–¹æ¡ˆ

#### 1. è°ƒä¼˜æ—¶é—´è¿‡é•¿
```python
# å‡å°‘è¯•éªŒæ¬¡æ•°
config.n_trials = 30

# ä½¿ç”¨å¹¶è¡Œè®¡ç®—
config.n_jobs = 4

# è®¾ç½®è¶…æ—¶
config.timeout_seconds = 1800
```

#### 2. å†…å­˜ä¸è¶³
```python
# å‡å°‘æ•°æ®é‡
X_sample = X.sample(n=5000)  # é‡‡æ ·5000ä¸ªæ ·æœ¬

# å‡å°‘æ¨¡å‹å¤æ‚åº¦
config.model_types = ["lightgbm"]  # åªè°ƒä¼˜ä¸€ä¸ªæ¨¡å‹
```

#### 3. æ”¶æ•›è¿‡æ…¢
```python
# ä½¿ç”¨æ›´æ¿€è¿›çš„æœç´¢
config.search_strategy = "optuna"
config.n_trials = 50

# è°ƒæ•´å‚æ•°ç©ºé—´
# åœ¨ParameterSpaceä¸­å‡å°‘å‚æ•°èŒƒå›´
```

#### 4. è¿‡æ‹Ÿåˆé£é™©
```python
# å¢åŠ éªŒè¯ä¸¥æ ¼æ€§
config.cv_folds = 7
config.validation_strategy = "expanding"

# åŠ å¼ºæ­£åˆ™åŒ–å‚æ•°
# åœ¨ParameterSpaceä¸­å¢åŠ reg_alpha, reg_lambdaèŒƒå›´
```

## ğŸ“Š æ€§èƒ½åŸºå‡†

æ ¹æ®é¡¹ç›®å†å²æ•°æ®ï¼Œé¢„æœŸæ€§èƒ½æå‡ï¼š

| æ¨¡å‹ | åŸºå‡†MSE | ä¼˜åŒ–åMSE | é¢„æœŸæ”¹è¿› |
|------|---------|-----------|----------|
| LightGBM | 0.2573 | 0.2450 | **+4.8%** â­ |
| XGBoost | 0.2573 | 0.2475 | **+3.8%** |
| CatBoost | 0.2573 | 0.2490 | **+3.2%** |
| é›†æˆæ¨¡å‹ | 0.2573 | 0.2420 | **+5.9%** â­ |

## ğŸ¯ é›†æˆä½¿ç”¨

### åœ¨ä¸»æ¨¡å‹ä¸­ä½¿ç”¨è°ƒä¼˜åçš„å‚æ•°

```python
# è‡ªåŠ¨ä½¿ç”¨ä¼˜åŒ–å‚æ•°
from lib.model_registry import get_model_params

# è·å–ä¼˜åŒ–åçš„å‚æ•°
params = get_model_params("lightgbm")
model = HullModel(model_type="lightgbm", model_params=params)
```

### æ£€æŸ¥å‚æ•°çŠ¶æ€

```python
from lib.config import get_config

config = get_config()

# æ£€æŸ¥æ˜¯å¦å¯ç”¨äº†è°ƒä¼˜
if config.is_tuning_enabled():
    print("âœ… ä½¿ç”¨è°ƒä¼˜åçš„å‚æ•°")
    
    # è·å–ç‰¹å®šæ¨¡å‹çš„å‚æ•°
    lgb_params = config.get_tuned_parameters("lightgbm")
    print(f"LightGBMå‚æ•°: {lgb_params}")
else:
    print("ğŸ“‹ ä½¿ç”¨é»˜è®¤å‚æ•°")
```

## ğŸ”„ æŒç»­ä¼˜åŒ–

### è‡ªåŠ¨åŒ–è°ƒä¼˜æµç¨‹

```bash
# 1. è®¾ç½®å®šæ—¶ä»»åŠ¡
# æ¯å¤©å‡Œæ™¨2ç‚¹è¿è¡Œè°ƒä¼˜
0 2 * * * cd /path/to/working && python hyperparameter_tuning.py

# 2. ç›‘æ§è°ƒä¼˜ç»“æœ
python demo_tuning.py --test

# 3. è‡ªåŠ¨åº”ç”¨æ–°å‚æ•°
python main.py --model-type lightgbm
```

### æ€§èƒ½å›å½’æ£€æµ‹

```python
# åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ç›‘æ§æ€§èƒ½
def monitor_model_performance():
    current_performance = evaluate_current_model()
    baseline_performance = load_baseline_performance()
    
    if current_performance < baseline_performance * 0.95:
        print("âš ï¸ æ€§èƒ½ä¸‹é™ï¼Œå»ºè®®é‡æ–°è°ƒä¼˜")
        trigger_retuning()
```

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [Hull Tacticalä¸»é¡¹ç›®æ–‡æ¡£](README.md)
- [é«˜çº§é›†æˆç­–ç•¥](ADVANCED_ENSEMBLE_IMPLEMENTATION.md)
- [ç‰¹å¾å·¥ç¨‹å¢å¼º](FEATURE_ENGINEERING_IMPROVEMENTS.md)
- [Kaggleéƒ¨ç½²æŒ‡å—](KAGGLE_DEPLOYMENT.md)

## ğŸ†˜ æŠ€æœ¯æ”¯æŒ

å¦‚æœé‡åˆ°é—®é¢˜ï¼Œè¯·æ£€æŸ¥ï¼š

1. **ä¾èµ–å®‰è£…**: ç¡®ä¿å®‰è£…æ‰€æœ‰å¿…éœ€åŒ… `pip install optuna matplotlib seaborn plotly`
2. **æ•°æ®è·¯å¾„**: ç¡®è®¤æ•°æ®æ–‡ä»¶è·¯å¾„æ­£ç¡®
3. **å†…å­˜ä½¿ç”¨**: ç›‘æ§å†…å­˜ä½¿ç”¨æƒ…å†µï¼Œå¿…è¦æ—¶å‡å°‘æ•°æ®é‡
4. **è¶…æ—¶è®¾ç½®**: åˆç†è®¾ç½®è°ƒä¼˜è¶…æ—¶æ—¶é—´

## ğŸ‰ æ€»ç»“

Hull Tacticalè¶…å‚æ•°è°ƒä¼˜ç³»ç»Ÿæä¾›äº†ï¼š

âœ… **æ™ºèƒ½ä¼˜åŒ–**: åŸºäºè´å¶æ–¯ä¼˜åŒ–çš„é«˜æ•ˆå‚æ•°æœç´¢  
âœ… **æ—¶é—´åºåˆ—å‹å¥½**: ä¸“ä¸šçš„é‡‘èæ—¶é—´åºåˆ—éªŒè¯ç­–ç•¥  
âœ… **å¤šæ¨¡å‹æ”¯æŒ**: LightGBMã€XGBoostã€CatBoostå…¨é¢è¦†ç›–  
âœ… **è‡ªåŠ¨åŒ–åˆ†æ**: å®Œæ•´çš„æŠ¥å‘Šç”Ÿæˆå’Œæ€§èƒ½åˆ†æ  
âœ… **ç”Ÿäº§å°±ç»ª**: ä¸ä¸»ç³»ç»Ÿæ— ç¼é›†æˆï¼Œæ”¯æŒæŒç»­ä¼˜åŒ–  

**é¢„æœŸæ•´ä½“æ€§èƒ½æå‡: 5-8%** ğŸ†

ç«‹å³å¼€å§‹ä½¿ç”¨ `python demo_tuning.py --quick` ä½“éªŒå®Œæ•´çš„è°ƒä¼˜æµç¨‹ï¼