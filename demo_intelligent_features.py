#!/usr/bin/env python3
"""
æ™ºèƒ½ç‰¹å¾å·¥ç¨‹åŠŸèƒ½å±•ç¤º
å±•ç¤ºæ–°çš„æ™ºèƒ½ç‰¹å¾é€‰æ‹©ã€ç»„åˆå’Œä¼˜åŒ–åŠŸèƒ½
"""

import sys
import os
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ è·¯å¾„
sys.path.append('/home/dev/github/kaggle-hull')
sys.path.append('/home/dev/github/kaggle-hull/working')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from working.lib.features import FeaturePipeline, build_feature_pipeline
from working.lib.data import load_train_data, get_feature_columns

def demonstrate_feature_selection():
    """å±•ç¤ºæ™ºèƒ½ç‰¹å¾é€‰æ‹©åŠŸèƒ½"""
    print("ğŸ¯ æ™ºèƒ½ç‰¹å¾é€‰æ‹©åŠŸèƒ½å±•ç¤º")
    print("="*50)
    
    # åŠ è½½æ•°æ®
    df = load_train_data()
    
    if df is None:
        print("âŒ æ— æ³•åŠ è½½æ•°æ®")
        return
    
    print(f"ğŸ“Š åŸå§‹æ•°æ®ä¿¡æ¯:")
    print(f"  - æ•°æ®å½¢çŠ¶: {df.shape}")
    print(f"  - ç‰¹å¾åˆ—æ•°: {len([col for col in df.columns if not col.startswith('forward_returns')])}")
    
    # åˆ›å»ºç›®æ ‡å˜é‡ç”¨äºæ¼”ç¤º
    if 'market_forward_excess_returns' in df.columns:
        target_col = 'market_forward_excess_returns'
    else:
        df['demo_target'] = np.random.normal(0, 0.02, len(df))
        target_col = 'demo_target'
    
    # æ™ºèƒ½ç‰¹å¾é€‰æ‹©æ¼”ç¤º
    pipeline = build_feature_pipeline(
        enable_intelligent_selection=True,
        feature_selection_method='mixed',
        max_features=40,
        enable_robust_scaling=True
    )
    pipeline.target_column = target_col
    
    # ä½¿ç”¨å‰300è¡Œè¿›è¡Œæ¼”ç¤º
    demo_df = df.head(300).copy()
    features = pipeline.fit_transform(demo_df)
    
    print(f"\nğŸ” æ™ºèƒ½ç‰¹å¾é€‰æ‹©ç»“æœ:")
    print(f"  - è¾“å‡ºç‰¹å¾æ•°: {features.shape[1]}")
    print(f"  - æ•°æ®æ ·æœ¬æ•°: {features.shape[0]}")
    
    if pipeline.selected_features:
        print(f"  - é€‰å®šç‰¹å¾æ•°é‡: {len(pipeline.selected_features)}")
        print(f"  - ç‰¹å¾é€‰æ‹©æ–¹æ³•: {pipeline.feature_selection_method}")
        print(f"  - å‰10ä¸ªé€‰å®šç‰¹å¾: {pipeline.selected_features[:10]}")
    
    if pipeline.selected_features_meta:
        meta = pipeline.selected_features_meta
        print(f"\nğŸ“ˆ ç‰¹å¾é€‰æ‹©ç»Ÿè®¡:")
        print(f"  - å¯é€‰ç‰¹å¾æ€»æ•°: {meta['total_available']}")
        print(f"  - å®é™…é€‰æ‹©æ•°: {meta['selected_count']}")
        print(f"  - é€‰æ‹©æ¯”ä¾‹: {meta['selected_count']/meta['total_available']*100:.1f}%")
    
    return features, pipeline

def demonstrate_feature_combinations():
    """å±•ç¤ºç‰¹å¾ç»„åˆåŠŸèƒ½"""
    print(f"\nğŸ”„ æ™ºèƒ½ç‰¹å¾ç»„åˆåŠŸèƒ½å±•ç¤º")
    print("="*50)
    
    # åŠ è½½æ•°æ®
    df = load_train_data()

    
    if df is None:
        print("âŒ æ— æ³•åŠ è½½æ•°æ®")
        return None, None
    
    # ä¸åŒå¤æ‚åº¦çº§åˆ«çš„ç‰¹å¾ç»„åˆ
    complexity_levels = [1, 3, 5]
    results = {}
    
    for complexity in complexity_levels:
        pipeline = build_feature_pipeline(
            enable_feature_combinations=True,
            combination_complexity=complexity,
            max_features=20,
            enable_robust_scaling=False  # å…³é—­æ ‡å‡†åŒ–ä»¥ä¾¿æ›´å¥½è§‚å¯Ÿç»„åˆæ•ˆæœ
        )
        
        demo_df = df.head(200).copy()
        features = pipeline.fit_transform(demo_df)
        
        # ç»Ÿè®¡ç»„åˆç‰¹å¾
        combinations = [col for col in features.columns if any(pattern in col for pattern in ['_x_', '_div_', '_squared', '_log', '_ewm_'])]
        basic_combos = [col for col in combinations if any(pattern in col for pattern in ['_x_', '_div_'])]
        advanced_combos = [col for col in combinations if any(pattern in col for pattern in ['_squared', '_log', '_ewm_'])]
        
        results[complexity] = {
            'total_features': features.shape[1],
            'combinations': len(combinations),
            'basic_combinations': len(basic_combos),
            'advanced_combinations': len(advanced_combos),
            'combination_ratio': len(combinations) / features.shape[1] * 100
        }
        
        print(f"ğŸ”¢ å¤æ‚åº¦çº§åˆ« {complexity}:")
        print(f"  - æ€»ç‰¹å¾æ•°: {features.shape[1]}")
        print(f"  - ç»„åˆç‰¹å¾æ•°: {len(combinations)}")
        print(f"  - åŸºç¡€ç»„åˆ: {len(basic_combos)}")
        print(f"  - é«˜çº§ç»„åˆ: {len(advanced_combos)}")
        print(f"  - ç»„åˆå æ¯”: {len(combinations) / features.shape[1] * 100:.1f}%")
    
    return results, pipeline

def demonstrate_tiered_features():
    """å±•ç¤ºåˆ†å±‚ç‰¹å¾å·¥ç¨‹"""
    print(f"\nğŸ—ï¸ åˆ†å±‚ç‰¹å¾å·¥ç¨‹å±•ç¤º")
    print("="*50)
    
    # åŠ è½½æ•°æ®
    df = load_train_data()
    
    if df is None:
        print("âŒ æ— æ³•åŠ è½½æ•°æ®")
        return None, None
    
    # åˆ›å»ºåˆ†å±‚ç‰¹å¾ç®¡é“
    pipeline = build_feature_pipeline(
        enable_tiered_features=True,
        tiered_levels=4,
        max_features=25,
        enable_robust_scaling=False
    )
    
    demo_df = df.head(250).copy()
    features = pipeline.fit_transform(demo_df)
    
    # ç»Ÿè®¡åˆ†å±‚ç‰¹å¾
    tiered_features = [col for col in features.columns if any(pattern in col for pattern in ['_mean_', '_std_', 'low_vol', 'high_vol', 'strong_trend', 'weak_trend', 'bull_market', 'bear_market'])]
    
    # æŒ‰ç±»å‹åˆ†ç±»
    by_market_state = [col for col in tiered_features if any(pattern in col for pattern in ['_mean_', '_std_'])]
    by_volatility = [col for col in tiered_features if 'vol' in col]
    by_trend = [col for col in tiered_features if 'trend' in col]
    by_market_regime = [col for col in tiered_features if any(pattern in col for pattern in ['bull_market', 'bear_market'])]
    
    print(f"ğŸ—ï¸ åˆ†å±‚ç‰¹å¾ç»Ÿè®¡:")
    print(f"  - æ€»ç‰¹å¾æ•°: {features.shape[1]}")
    print(f"  - åˆ†å±‚ç‰¹å¾æ•°: {len(tiered_features)}")
    print(f"  - çŠ¶æ€åˆ†å±‚: {len(by_market_state)}")
    print(f"  - æ³¢åŠ¨ç‡åˆ†å±‚: {len(by_volatility)}")
    print(f"  - è¶‹åŠ¿åˆ†å±‚: {len(by_trend)}")
    print(f"  - å¸‚åœºå½¢æ€: {len(by_market_regime)}")
    
    if tiered_features:
        print(f"\nğŸ“‹ åˆ†å±‚ç‰¹å¾ç¤ºä¾‹:")
        for i, feature in enumerate(tiered_features[:8]):
            print(f"  {i+1}. {feature}")
    
    return features, pipeline

def demonstrate_robust_scaling():
    """å±•ç¤ºRobustScaleræ”¹è¿›"""
    print(f"\nğŸ”§ RobustScaleræ”¹è¿›å±•ç¤º")
    print("="*50)
    
    # åŠ è½½æ•°æ®
    df = load_train_data()
    
    if df is None:
        print("âŒ æ— æ³•åŠ è½½æ•°æ®")
        return None, None
    
    # åˆ›å»ºå¸¦robust scalingçš„ç®¡é“
    pipeline = build_feature_pipeline(
        standardize=True,
        enable_robust_scaling=True,
        max_features=15
    )
    
    demo_df = df.head(100).copy()
    features = pipeline.fit_transform(demo_df)
    
    print(f"ğŸ”§ æ ‡å‡†åŒ–é…ç½®:")
    print(f"  - å¯ç”¨æ ‡å‡†åŒ–: {pipeline.standardize}")
    print(f"  - å¯ç”¨RobustScaler: {pipeline.enable_robust_scaling}")
    
    if pipeline.scaler:
        print(f"  - å®é™…ç¼©æ”¾å™¨ç±»å‹: {type(pipeline.scaler).__name__}")
        
        # åˆ†æç¼©æ”¾æ•ˆæœ
        numeric_features = features.select_dtypes(include=[np.number]).columns[:5]
        for col in numeric_features:
            original_std = features[col].std()
            mean_val = features[col].mean()
            print(f"  - {col}: å‡å€¼={mean_val:.3f}, æ ‡å‡†å·®={original_std:.3f}")
    else:
        print("  - ä½¿ç”¨æ ‡å‡†StandardScaler")
    
    print(f"  - è¾“å‡ºå½¢çŠ¶: {features.shape}")
    
    return features, pipeline

def demonstrate_data_quality():
    """å±•ç¤ºæ•°æ®è´¨é‡åˆ†æ"""
    print(f"\nğŸ“Š æ•°æ®è´¨é‡åˆ†æå±•ç¤º")
    print("="*50)
    
    # åŠ è½½æ•°æ®
    df = load_train_data()

    
    if df is None:
        print("âŒ æ— æ³•åŠ è½½æ•°æ®")
        return
    
    # åˆ›å»ºå¯ç”¨æ•°æ®è´¨é‡åˆ†æçš„ç®¡é“
    pipeline = build_feature_pipeline(
        enable_data_quality=True,
        enable_feature_stability=True,
        outlier_detection=True,
        max_features=20
    )
    
    demo_df = df.head(150).copy()
    features = pipeline.fit_transform(demo_df)
    
    print(f"ğŸ“Š æ•°æ®è´¨é‡æŠ¥å‘Š:")
    print(f"  - åˆ†æç‰¹å¾æ•°: {len(pipeline.numeric_columns)}")
    print(f"  - æ•°æ®è´¨é‡æŒ‡æ ‡: {len(pipeline.data_quality_metrics)} é¡¹")
    print(f"  - ç¨³å®šæ€§åˆ†æ: {'å®Œæˆ' if pipeline.feature_stability_scores else 'æœªå®Œæˆ'}")
    print(f"  - å¼‚å¸¸å€¼æ£€æµ‹: {len(pipeline.outlier_bounds)} ä¸ªç‰¹å¾å·²è®¾ç½®è¾¹ç•Œ")
    
    # å±•ç¤ºè´¨é‡æŒ‡æ ‡æ ·ä¾‹
    if pipeline.data_quality_metrics:
        sample_features = list(pipeline.data_quality_metrics.keys())[:5]
        print(f"\nğŸ“ˆ è´¨é‡æŒ‡æ ‡ç¤ºä¾‹:")
        for feature in sample_features:
            metrics = pipeline.data_quality_metrics[feature]
            print(f"  {feature}:")
            print(f"    - ç¼ºå¤±ç‡: {metrics.get('missing_rate', 0):.3f}")
            print(f"    - é›¶å€¼ç‡: {metrics.get('zero_rate', 0):.3f}")
            print(f"    - ååº¦: {metrics.get('skewness', 0):.3f}")
    
    # å±•ç¤ºç¨³å®šæ€§å¾—åˆ†
    if pipeline.feature_stability_scores:
        stable_features = [col for col, score in pipeline.feature_stability_scores.items() if score > 0.5]
        risky_features = [col for col, score in pipeline.feature_stability_scores.items() if score < 0.3]
        print(f"\nğŸ“ˆ ç¨³å®šæ€§åˆ†æ:")
        print(f"  - ç¨³å®šç‰¹å¾: {len(stable_features)} ä¸ª")
        print(f"  - ä¸ç¨³å®šç‰¹å¾: {len(risky_features)} ä¸ª")
        
        if stable_features:
            print(f"  - ç¨³å®šç‰¹å¾ç¤ºä¾‹: {stable_features[:3]}")
        if risky_features:
            print(f"  - ä¸ç¨³å®šç‰¹å¾ç¤ºä¾‹: {risky_features[:3]}")

def create_performance_report():
    """åˆ›å»ºæ€§èƒ½å¯¹æ¯”æŠ¥å‘Š"""
    print(f"\nğŸ“ˆ æ™ºèƒ½ç‰¹å¾å·¥ç¨‹æ€§èƒ½æŠ¥å‘Š")
    print("="*60)
    
    # åŠ è½½æ•°æ®
    df = load_train_data()
    
    if df is None:
        print("âŒ æ— æ³•åŠ è½½æ•°æ®")
        return
    
    demo_df = df.head(200).copy()
    
    # å®šä¹‰æµ‹è¯•æ–¹æ¡ˆ
    test_scenarios = {
        "åŸºç¡€ç‰ˆæœ¬": {
            "config": {
                "enable_intelligent_selection": False,
                "enable_feature_combinations": False,
                "enable_tiered_features": False,
                "enable_robust_scaling": False,
                "max_features": 25
            }
        },
        "æ™ºèƒ½é€‰æ‹©": {
            "config": {
                "enable_intelligent_selection": True,
                "feature_selection_method": "mixed",
                "enable_feature_combinations": False,
                "enable_tiered_features": False,
                "enable_robust_scaling": True,
                "max_features": 25
            }
        },
        "ç‰¹å¾ç»„åˆ": {
            "config": {
                "enable_intelligent_selection": True,
                "enable_feature_combinations": True,
                "combination_complexity": 3,
                "enable_tiered_features": False,
                "enable_robust_scaling": True,
                "max_features": 25
            }
        },
        "å®Œæ•´æ™ºèƒ½": {
            "config": {
                "enable_intelligent_selection": True,
                "feature_selection_method": "mixed",
                "enable_feature_combinations": True,
                "combination_complexity": 3,
                "enable_tiered_features": True,
                "tiered_levels": 4,
                "enable_robust_scaling": True,
                "max_features": 25
            }
        }
    }
    
    results = {}
    
    for scenario_name, scenario_config in test_scenarios.items():
        try:
            import time
            
            pipeline = build_feature_pipeline(**scenario_config["config"])
            start_time = time.time()
            features = pipeline.fit_transform(demo_df)
            processing_time = time.time() - start_time
            
            # è®¡ç®—ç‰¹å¾è´¨é‡æŒ‡æ ‡
            unique_features = features.shape[1]
            numeric_features = len(features.select_dtypes(include=[np.number]).columns)
            
            # æ£€æŸ¥ç‰¹å¾ç¨³å®šæ€§
            stability_score = 0
            if pipeline.feature_stability_scores:
                stability_score = np.mean(list(pipeline.feature_stability_scores.values()))
            
            results[scenario_name] = {
                'processing_time': processing_time,
                'feature_count': unique_features,
                'numeric_features': numeric_features,
                'stability_score': stability_score,
                'scalable': pipeline.scaler is not None if pipeline.standardize else True
            }
            
        except Exception as e:
            results[scenario_name] = {
                'error': str(e),
                'feature_count': 0,
                'processing_time': float('inf')
            }
    
    # å±•ç¤ºå¯¹æ¯”ç»“æœ
    print(f"{'æ–¹æ¡ˆ':<12} {'ç‰¹å¾æ•°':<8} {'æ—¶é—´(s)':<10} {'ç¨³å®šæ€§':<8} {'ç¼©æ”¾å™¨':<8}")
    print("-" * 50)
    
    for scenario, result in results.items():
        if 'error' not in result:
            features = result['feature_count']
            time_taken = result['processing_time']
            stability = result['stability_score']
            scaler = "Yes" if result['scalable'] else "No"
            
            print(f"{scenario:<12} {features:<8} {time_taken:<10.3f} {stability:<8.3f} {scaler:<8}")
        else:
            print(f"{scenario:<12} {'ERROR':<8} {'N/A':<10} {'N/A':<8} {'N/A':<8}")
    
    # æ€§èƒ½åˆ†æ
    if "å®Œæ•´æ™ºèƒ½" in results and "åŸºç¡€ç‰ˆæœ¬" in results:
        intelligent = results["å®Œæ•´æ™ºèƒ½"]
        basic = results["åŸºç¡€ç‰ˆæœ¬"]
        
        if 'error' not in intelligent and 'error' not in basic:
            feature_improvement = (intelligent['feature_count'] - basic['feature_count']) / basic['feature_count'] * 100
            time_ratio = intelligent['processing_time'] / basic['processing_time']
            
            print(f"\nğŸ“Š æ€§èƒ½æå‡åˆ†æ:")
            print(f"  - ç‰¹å¾æ•°é‡æå‡: {feature_improvement:+.1f}%")
            print(f"  - å¤„ç†æ—¶é—´æ¯”ä¾‹: {time_ratio:.2f}x")
            print(f"  - ç¨³å®šæ€§å¾—åˆ†: {intelligent['stability_score']:.3f}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ æ™ºèƒ½ç‰¹å¾å·¥ç¨‹åŠŸèƒ½å±•ç¤º")
    print("="*60)
    
    # å±•ç¤ºå„é¡¹åŠŸèƒ½
    try:
        demonstrate_feature_selection()
        demonstrate_feature_combinations()
        demonstrate_tiered_features()
        demonstrate_robust_scaling()
        demonstrate_data_quality()
        create_performance_report()
        
        print(f"\nğŸ‰ æ™ºèƒ½ç‰¹å¾å·¥ç¨‹åŠŸèƒ½å±•ç¤ºå®Œæˆï¼")
        print(f"âœ¨ ä¸»è¦æ”¹è¿›åŒ…æ‹¬:")
        print(f"  â€¢ æ™ºèƒ½ç‰¹å¾é€‰æ‹©ç®—æ³•ï¼ˆç›¸å…³æ€§ã€äº’ä¿¡æ¯ã€RFEã€èšç±»ï¼‰")
        print(f"  â€¢ å¤šç§ç‰¹å¾ç»„åˆç­–ç•¥ï¼ˆåŸºç¡€ã€å¤šé¡¹å¼ã€æ¡ä»¶ã€æ—¶é—´åºåˆ—ã€éçº¿æ€§ï¼‰")
        print(f"  â€¢ åˆ†å±‚å¸‚åœºç‰¹å¾å·¥ç¨‹ï¼ˆæ³¢åŠ¨ç‡ã€è¶‹åŠ¿ã€å¸‚åœºçŠ¶æ€ï¼‰")
        print(f"  â€¢ RobustScalerå’Œåˆ†ä½æ•°æ ‡å‡†åŒ–")
        print(f"  â€¢ å®Œæ•´çš„æ•°æ®è´¨é‡åˆ†æ")
        print(f"  â€¢ è‡ªé€‚åº”ç‰¹å¾é€‰æ‹©å’Œä¼˜åŒ–")
        
    except Exception as e:
        print(f"âŒ å±•ç¤ºè¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
