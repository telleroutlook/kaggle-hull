"""
æ—¶é—´åºåˆ—éªŒè¯ç³»ç»Ÿæ€§èƒ½åŸºå‡†æµ‹è¯•å’Œæ•ˆæœéªŒè¯

æµ‹è¯•å†…å®¹:
1. ä¸åŒæ•°æ®è§„æ¨¡çš„æ€§èƒ½æµ‹è¯•
2. å†…å­˜ä½¿ç”¨æ•ˆç‡æµ‹è¯•
3. å¹¶è¡Œå¤„ç†æ€§èƒ½æµ‹è¯•
4. éªŒè¯å‡†ç¡®æ€§å¯¹æ¯”æµ‹è¯•
5. ä¸ä¼ ç»Ÿæ–¹æ³•çš„æ€§èƒ½å¯¹æ¯”
6. å®é™…åº”ç”¨æ•ˆæœéªŒè¯
"""

import numpy as np
import pandas as pd
import time
import psutil
import gc
import tracemalloc
from pathlib import Path
import json
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple, Any
import warnings
warnings.filterwarnings("ignore")

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# å¯¼å…¥éªŒè¯ç³»ç»Ÿ
from time_series_validation import (
    TimeSeriesCrossValidator, ValidationConfig, ValidationResult,
    ValidationStrategy, FinanceValidationMetrics, create_time_series_validator
)

from time_series_validation_integration import IntegratedTimeSeriesValidator

# å¯¼å…¥ç°æœ‰ç³»ç»Ÿ
try:
    from lib.models import HullModel
    from sklearn.model_selection import TimeSeriesSplit
    from sklearn.metrics import mean_squared_error
    SYSTEM_AVAILABLE = True
except ImportError:
    SYSTEM_AVAILABLE = False


class PerformanceBenchmark:
    """æ€§èƒ½åŸºå‡†æµ‹è¯•ç±»"""
    
    def __init__(self):
        self.results = {}
        self.baseline_results = {}
        self.performance_data = []
        
    def generate_test_data(self, n_samples: int, n_features: int = 20) -> Tuple[pd.DataFrame, pd.Series]:
        """ç”Ÿæˆæµ‹è¯•æ•°æ®"""
        np.random.seed(42)
        
        # ç”Ÿæˆæ—¶é—´åºåˆ—ç‰¹å¾
        dates = pd.date_range('2018-01-01', periods=n_samples, freq='D')
        
        # ç”Ÿæˆç›¸å…³æ€§ç‰¹å¾
        base_signal = np.cumsum(np.random.normal(0, 0.001, n_samples))
        
        data = {
            'date_id': [int(d.strftime('%Y%m%d')) for d in dates],
            'target_base': base_signal,
        }
        
        # ç”Ÿæˆç‰¹å¾
        for i in range(n_features):
            if i < 5:
                # å‰5ä¸ªç‰¹å¾ä¸ç›®æ ‡ç›¸å…³
                data[f'feature_{i}'] = base_signal + np.random.normal(0, 0.01, n_samples)
            else:
                # å…¶ä»–ç‰¹å¾ç‹¬ç«‹
                data[f'feature_{i}'] = np.random.normal(0, 1, n_samples)
        
        # ç”Ÿæˆç›®æ ‡å˜é‡
        target = (
            0.3 * data['feature_0'] + 
            0.2 * data['feature_1'] + 
            0.1 * data['feature_2'] + 
            np.random.normal(0, 0.005, n_samples)
        )
        
        df = pd.DataFrame(data)
        target_series = pd.Series(target, name='target')
        
        return df, target_series
    
    def create_baseline_model(self):
        """åˆ›å»ºåŸºçº¿æ¨¡å‹"""
        if SYSTEM_AVAILABLE:
            return HullModel(model_type="baseline", model_params={'n_estimators': 50})
        else:
            # ç®€å•çº¿æ€§å›å½’
            class SimpleModel:
                def __init__(self):
                    self.coef_ = None
                    self.intercept_ = None
                
                def fit(self, X, y):
                    X_array = X.values if hasattr(X, 'values') else X
                    X_with_intercept = np.column_stack([np.ones(X_array.shape[0]), X_array])
                    params = np.linalg.lstsq(X_with_intercept, y, rcond=None)[0]
                    self.intercept_ = params[0]
                    self.coef_ = params[1:]
                    return self
                
                def predict(self, X):
                    X_array = X.values if hasattr(X, 'values') else X
                    return self.intercept_ + X_array @ self.coef_
            
            return SimpleModel()
    
    def test_scalability(self, sizes: List[int]) -> Dict[str, Any]:
        """æµ‹è¯•æ‰©å±•æ€§"""
        print("\nğŸ”„ æµ‹è¯•æ•°æ®è§„æ¨¡æ‰©å±•æ€§...")
        
        scalability_results = {}
        
        for size in sizes:
            print(f"  ğŸ“Š æµ‹è¯•è§„æ¨¡: {size} æ ·æœ¬")
            
            # ç”Ÿæˆæ•°æ®
            data, target = self.generate_test_data(size, n_features=15)
            feature_cols = [col for col in data.columns if col.startswith('feature_')]
            X = data[feature_cols]
            
            # åˆ›å»ºæ¨¡å‹
            model = self.create_baseline_model()
            
            # æµ‹è¯•ä¸åŒç­–ç•¥
            strategies = [
                ('TimeSeriesSplit', ValidationStrategy.TIME_SERIES_SPLIT),
                ('ExpandingWindow', ValidationStrategy.EXPANDING_WINDOW),
                ('RollingWindow', ValidationStrategy.ROLLING_WINDOW),
            ]
            
            size_results = {}
            
            for strategy_name, strategy in strategies:
                try:
                    # å†…å­˜ç›‘æ§
                    tracemalloc.start()
                    process = psutil.Process()
                    mem_before = process.memory_info().rss / 1024 / 1024  # MB
                    
                    # æ—¶é—´æµ‹è¯•
                    start_time = time.time()
                    
                    config = ValidationConfig(
                        strategy=strategy,
                        n_splits=5,
                        verbose=False
                    )
                    validator = TimeSeriesCrossValidator(config)
                    result = validator.validate(model, X, target)
                    
                    end_time = time.time()
                    
                    # å†…å­˜æµ‹è¯•
                    current, peak = tracemalloc.get_traced_memory()
                    tracemalloc.stop()
                    mem_after = process.memory_info().rss / 1024 / 1024  # MB
                    
                    size_results[strategy_name] = {
                        'time': end_time - start_time,
                        'memory_usage': mem_after - mem_before,
                        'peak_memory': peak / 1024 / 1024,  # MB
                        'mse': np.mean(result.metrics.get('mse', [0])),
                        'n_splits': result.n_splits
                    }
                    
                    print(f"    âœ… {strategy_name}: {end_time - start_time:.2f}s, å†…å­˜: {mem_after - mem_before:.1f}MB")
                    
                except Exception as e:
                    print(f"    âŒ {strategy_name}: å¤±è´¥ - {e}")
                    size_results[strategy_name] = {'error': str(e)}
                
                # æ¸…ç†å†…å­˜
                gc.collect()
            
            scalability_results[size] = size_results
        
        return scalability_results
    
    def test_parallel_performance(self, n_jobs_list: List[int]) -> Dict[str, Any]:
        """æµ‹è¯•å¹¶è¡Œå¤„ç†æ€§èƒ½"""
        print("\nğŸš€ æµ‹è¯•å¹¶è¡Œå¤„ç†æ€§èƒ½...")
        
        # ä½¿ç”¨ä¸­ç­‰è§„æ¨¡æ•°æ®
        data, target = self.generate_test_data(1000, n_features=20)
        feature_cols = [col for col in data.columns if col.startswith('feature_')]
        X = data[feature_cols]
        model = self.create_baseline_model()
        
        parallel_results = {}
        
        for n_jobs in n_jobs_list:
            print(f"  ğŸ”¢ å¹¶è¡Œåº¦: {n_jobs}")
            
            try:
                start_time = time.time()
                
                config = ValidationConfig(
                    strategy=ValidationStrategy.EXPANDING_WINDOW,
                    n_splits=5,
                    n_jobs=n_jobs,
                    enable_parallel=n_jobs > 1,
                    verbose=False
                )
                validator = TimeSeriesCrossValidator(config)
                result = validator.validate(model, X, target)
                
                end_time = time.time()
                
                parallel_results[n_jobs] = {
                    'time': end_time - start_time,
                    'mse': np.mean(result.metrics.get('mse', [0])),
                    'n_splits': result.n_splits
                }
                
                print(f"    âœ… {n_jobs} jobs: {end_time - start_time:.2f}s")
                
            except Exception as e:
                print(f"    âŒ {n_jobs} jobs: å¤±è´¥ - {e}")
                parallel_results[n_jobs] = {'error': str(e)}
        
        return parallel_results
    
    def test_accuracy_comparison(self) -> Dict[str, Any]:
        """æµ‹è¯•éªŒè¯å‡†ç¡®æ€§å¯¹æ¯”"""
        print("\nğŸ¯ æµ‹è¯•éªŒè¯å‡†ç¡®æ€§...")
        
        # ç”Ÿæˆæµ‹è¯•æ•°æ®
        np.random.seed(123)
        data, target = self.generate_test_data(1500, n_features=25)
        feature_cols = [col for col in data.columns if col.startswith('feature_')]
        X = data[feature_cols]
        
        # åˆ›å»ºå¤šä¸ªæ¨¡å‹
        models = {
            'baseline': self.create_baseline_model(),
        }
        
        if SYSTEM_AVAILABLE:
            models['lightgbm'] = HullModel(model_type="lightgbm", model_params={'n_estimators': 100})
        
        accuracy_results = {}
        
        for model_name, model in models.items():
            print(f"  ğŸ”¬ æµ‹è¯•æ¨¡å‹: {model_name}")
            
            model_results = {}
            
            # ä¼ ç»Ÿæ—¶é—´åºåˆ—åˆ†å‰²
            if SYSTEM_AVAILABLE:
                try:
                    start_time = time.time()
                    tscv = TimeSeriesSplit(n_splits=5)
                    mse_scores = []
                    
                    for train_idx, val_idx in tscv.split(X):
                        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
                        y_train, y_val = target.iloc[train_idx], target.iloc[val_idx]
                        
                        temp_model = self.create_baseline_model() if model_name == 'baseline' else HullModel(model_type="lightgbm")
                        temp_model.fit(X_train, y_train)
                        y_pred = temp_model.predict(X_val)
                        mse_scores.append(mean_squared_error(y_val, y_pred))
                    
                    traditional_time = time.time() - start_time
                    model_results['traditional'] = {
                        'mse_mean': np.mean(mse_scores),
                        'mse_std': np.std(mse_scores),
                        'time': traditional_time
                    }
                    
                except Exception as e:
                    model_results['traditional'] = {'error': str(e)}
            
            # æ–°æ—¶é—´åºåˆ—éªŒè¯ç³»ç»Ÿ
            try:
                start_time = time.time()
                
                config = ValidationConfig(
                    strategy=ValidationStrategy.EXPANDING_WINDOW,
                    n_splits=5,
                    verbose=False
                )
                validator = TimeSeriesCrossValidator(config)
                result = validator.validate(model, X, target)
                
                new_time = time.time() - start_time
                model_results['new_system'] = {
                    'mse_mean': np.mean(result.metrics.get('mse', [0])),
                    'mse_std': np.std(result.metrics.get('mse', [0])),
                    'time': new_time,
                    'stability_score': result.quality_metrics.get('performance_stability', 0)
                }
                
            except Exception as e:
                model_results['new_system'] = {'error': str(e)}
            
            # PurgedéªŒè¯
            try:
                start_time = time.time()
                
                config = ValidationConfig(
                    strategy=ValidationStrategy.PURGED_TIME_SERIES,
                    n_splits=5,
                    embargo_percentage=0.1,
                    verbose=False
                )
                validator = TimeSeriesCrossValidator(config)
                result = validator.validate(model, X, target)
                
                purged_time = time.time() - start_time
                model_results['purged'] = {
                    'mse_mean': np.mean(result.metrics.get('mse', [0])),
                    'mse_std': np.std(result.metrics.get('mse', [0])),
                    'time': purged_time
                }
                
            except Exception as e:
                model_results['purged'] = {'error': str(e)}
            
            accuracy_results[model_name] = model_results
            print(f"    âœ… {model_name} å®Œæˆ")
        
        return accuracy_results
    
    def test_finance_metrics_performance(self) -> Dict[str, Any]:
        """æµ‹è¯•é‡‘èæŒ‡æ ‡è®¡ç®—æ€§èƒ½"""
        print("\nğŸ’° æµ‹è¯•é‡‘èæŒ‡æ ‡è®¡ç®—æ€§èƒ½...")
        
        # ç”Ÿæˆé‡‘èæ•°æ®
        np.random.seed(42)
        n_periods = 1000
        
        # ç­–ç•¥æ”¶ç›Šåºåˆ—
        strategy_returns = np.random.normal(0.0005, 0.02, n_periods)
        benchmark_returns = np.random.normal(0.0003, 0.018, n_periods)
        market_returns = np.random.normal(0.0002, 0.015, n_periods)
        
        # é¢„æµ‹å€¼
        y_true = strategy_returns
        y_pred = strategy_returns + np.random.normal(0, 0.005, n_periods)
        
        calculator = FinanceValidationMetrics()
        
        # æ€§èƒ½æµ‹è¯•
        start_time = time.time()
        metrics = calculator.calculate_metrics(
            y_true, y_pred,
            benchmark_returns=benchmark_returns,
            market_returns=market_returns
        )
        end_time = time.time()
        
        finance_performance = {
            'calculation_time': end_time - start_time,
            'n_metrics': len(metrics),
            'metrics': list(metrics.keys())
        }
        
        print(f"  âœ… è®¡ç®—äº† {len(metrics)} ä¸ªé‡‘èæŒ‡æ ‡ï¼Œè€—æ—¶: {end_time - start_time:.4f}ç§’")
        
        return finance_performance
    
    def test_integration_performance(self) -> Dict[str, Any]:
        """æµ‹è¯•é›†æˆç³»ç»Ÿæ€§èƒ½"""
        print("\nğŸ”— æµ‹è¯•é›†æˆç³»ç»Ÿæ€§èƒ½...")
        
        if not SYSTEM_AVAILABLE:
            return {'message': 'ç³»ç»Ÿé›†æˆä¸å¯ç”¨ï¼Œè·³è¿‡æµ‹è¯•'}
        
        # ç”Ÿæˆæµ‹è¯•æ•°æ®
        data, target = self.generate_test_data(1200, n_features=30)
        
        # æ·»åŠ å¸‚åœºæ•°æ®
        if 'target_base' in data.columns:
            data['P1'] = 100 + np.cumsum(data['target_base'] * 0.01)
            data['V1'] = 0.02 + np.abs(data['target_base']) * 0.001
            data['forward_returns'] = target
        
        integration_performance = {}
        
        try:
            # é›†æˆéªŒè¯å™¨æ€§èƒ½
            start_time = time.time()
            
            validator = IntegratedTimeSeriesValidator()
            results = validator.validate_hull_model(
                model_type="lightgbm",
                train_data=data,
                target_column="forward_returns"
            )
            
            end_time = time.time()
            
            integration_performance['integrated_validator'] = {
                'time': end_time - start_time,
                'n_strategies': len(results),
                'strategies': list(results.keys())
            }
            
            print(f"  âœ… é›†æˆéªŒè¯: {len(results)} ç­–ç•¥ï¼Œè€—æ—¶: {end_time - start_time:.2f}ç§’")
            
        except Exception as e:
            integration_performance['integrated_validator'] = {'error': str(e)}
            print(f"  âŒ é›†æˆéªŒè¯å¤±è´¥: {e}")
        
        try:
            # é›†æˆæ¨¡å‹éªŒè¯æ€§èƒ½
            start_time = time.time()
            
            ensemble_results = validator.validate_ensemble_performance(
                base_models=['lightgbm', 'xgboost'],
                ensemble_configs=[
                    {'type': 'dynamic_weighted', 'config': {'performance_window': 50}},
                    {'type': 'averaging', 'config': {'weights': [0.6, 0.4]}}
                ],
                train_data=data,
                target_column="forward_returns"
            )
            
            end_time = time.time()
            
            integration_performance['ensemble_validation'] = {
                'time': end_time - start_time,
                'n_ensembles': len(ensemble_results)
            }
            
            print(f"  âœ… é›†æˆæ¨¡å‹éªŒè¯: {len(ensemble_results)} ç±»å‹ï¼Œè€—æ—¶: {end_time - start_time:.2f}ç§’")
            
        except Exception as e:
            integration_performance['ensemble_validation'] = {'error': str(e)}
            print(f"  âŒ é›†æˆæ¨¡å‹éªŒè¯å¤±è´¥: {e}")
        
        return integration_performance
    
    def create_performance_visualizations(self, benchmark_results: Dict[str, Any]) -> None:
        """åˆ›å»ºæ€§èƒ½å¯è§†åŒ–å›¾è¡¨"""
        print("\nğŸ“Š ç”Ÿæˆæ€§èƒ½å¯è§†åŒ–å›¾è¡¨...")
        
        output_dir = Path("benchmark_results")
        output_dir.mkdir(exist_ok=True)
        
        # 1. æ‰©å±•æ€§æµ‹è¯•å›¾è¡¨
        if 'scalability' in benchmark_results:
            self._plot_scalability_results(benchmark_results['scalability'], output_dir)
        
        # 2. å¹¶è¡Œæ€§èƒ½å›¾è¡¨
        if 'parallel' in benchmark_results:
            self._plot_parallel_performance(benchmark_results['parallel'], output_dir)
        
        # 3. å‡†ç¡®æ€§å¯¹æ¯”å›¾è¡¨
        if 'accuracy' in benchmark_results:
            self._plot_accuracy_comparison(benchmark_results['accuracy'], output_dir)
        
        print(f"  âœ… å›¾è¡¨å·²ä¿å­˜åˆ°: {output_dir}")
    
    def _plot_scalability_results(self, scalability_data: Dict, output_dir: Path):
        """ç»˜åˆ¶æ‰©å±•æ€§ç»“æœ"""
        sizes = list(scalability_data.keys())
        strategies = ['TimeSeriesSplit', 'ExpandingWindow', 'RollingWindow']
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # æ‰§è¡Œæ—¶é—´å›¾
        for strategy in strategies:
            times = []
            for size in sizes:
                if strategy in scalability_data[size] and 'time' in scalability_data[size][strategy]:
                    times.append(scalability_data[size][strategy]['time'])
                else:
                    times.append(0)
            ax1.plot(sizes, times, marker='o', label=strategy, linewidth=2)
        
        ax1.set_title('éªŒè¯ç­–ç•¥æ‰§è¡Œæ—¶é—´æ‰©å±•æ€§', fontsize=14, fontweight='bold')
        ax1.set_xlabel('æ•°æ®è§„æ¨¡ (æ ·æœ¬æ•°)')
        ax1.set_ylabel('æ‰§è¡Œæ—¶é—´ (ç§’)')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # å†…å­˜ä½¿ç”¨å›¾
        for strategy in strategies:
            memory = []
            for size in sizes:
                if strategy in scalability_data[size] and 'memory_usage' in scalability_data[size][strategy]:
                    memory.append(scalability_data[size][strategy]['memory_usage'])
                else:
                    memory.append(0)
            ax2.plot(sizes, memory, marker='s', label=strategy, linewidth=2)
        
        ax2.set_title('éªŒè¯ç­–ç•¥å†…å­˜ä½¿ç”¨æ‰©å±•æ€§', fontsize=14, fontweight='bold')
        ax2.set_xlabel('æ•°æ®è§„æ¨¡ (æ ·æœ¬æ•°)')
        ax2.set_ylabel('å†…å­˜ä½¿ç”¨ (MB)')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(output_dir / 'scalability_performance.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_parallel_performance(self, parallel_data: Dict, output_dir: Path):
        """ç»˜åˆ¶å¹¶è¡Œæ€§èƒ½å›¾"""
        n_jobs = list(parallel_data.keys())
        times = [parallel_data[j].get('time', 0) for j in n_jobs]
        
        plt.figure(figsize=(10, 6))
        bars = plt.bar(n_jobs, times, color='skyblue', alpha=0.7)
        plt.title('å¹¶è¡Œå¤„ç†æ€§èƒ½æµ‹è¯•', fontsize=14, fontweight='bold')
        plt.xlabel('å¹¶è¡Œåº¦ (jobs)')
        plt.ylabel('æ‰§è¡Œæ—¶é—´ (ç§’)')
        plt.grid(True, alpha=0.3)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, time_val in zip(bars, times):
            height = bar.get_height()
            plt.text(bar.get_x() + bar.get_width()/2., height + height*0.01,
                    f'{time_val:.2f}s', ha='center', va='bottom')
        
        plt.tight_layout()
        plt.savefig(output_dir / 'parallel_performance.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_accuracy_comparison(self, accuracy_data: Dict, output_dir: Path):
        """ç»˜åˆ¶å‡†ç¡®æ€§å¯¹æ¯”å›¾"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # MSEå¯¹æ¯”
        models = list(accuracy_data.keys())
        methods = ['traditional', 'new_system', 'purged']
        
        mse_data = {method: [] for method in methods}
        for model in models:
            for method in methods:
                if method in accuracy_data[model] and 'mse_mean' in accuracy_data[model][method]:
                    mse_data[method].append(accuracy_data[model][method]['mse_mean'])
                else:
                    mse_data[method].append(0)
        
        x = np.arange(len(models))
        width = 0.25
        
        for i, method in enumerate(methods):
            ax1.bar(x + i*width, mse_data[method], width, label=method)
        
        ax1.set_title('ä¸åŒéªŒè¯æ–¹æ³•çš„MSEå¯¹æ¯”', fontsize=14, fontweight='bold')
        ax1.set_xlabel('æ¨¡å‹')
        ax1.set_ylabel('MSE')
        ax1.set_xticks(x + width)
        ax1.set_xticklabels(models)
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # æ‰§è¡Œæ—¶é—´å¯¹æ¯”
        time_data = {method: [] for method in methods}
        for model in models:
            for method in methods:
                if method in accuracy_data[model] and 'time' in accuracy_data[model][method]:
                    time_data[method].append(accuracy_data[model][method]['time'])
                else:
                    time_data[method].append(0)
        
        for i, method in enumerate(methods):
            ax2.bar(x + i*width, time_data[method], width, label=method)
        
        ax2.set_title('ä¸åŒéªŒè¯æ–¹æ³•çš„æ‰§è¡Œæ—¶é—´å¯¹æ¯”', fontsize=14, fontweight='bold')
        ax2.set_xlabel('æ¨¡å‹')
        ax2.set_ylabel('æ‰§è¡Œæ—¶é—´ (ç§’)')
        ax2.set_xticks(x + width)
        ax2.set_xticklabels(models)
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(output_dir / 'accuracy_comparison.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def generate_benchmark_report(self, all_results: Dict[str, Any]) -> str:
        """ç”ŸæˆåŸºå‡†æµ‹è¯•æŠ¥å‘Š"""
        
        report = f"""
# æ—¶é—´åºåˆ—éªŒè¯ç³»ç»Ÿæ€§èƒ½åŸºå‡†æµ‹è¯•æŠ¥å‘Š

## æµ‹è¯•æ¦‚è¿°
æœ¬æŠ¥å‘Šå¯¹æ—¶é—´åºåˆ—å‹å¥½äº¤å‰éªŒè¯ç³»ç»Ÿè¿›è¡Œäº†å…¨é¢çš„æ€§èƒ½åŸºå‡†æµ‹è¯•ï¼ŒéªŒè¯äº†ç³»ç»Ÿåœ¨ä¸åŒåœºæ™¯ä¸‹çš„è¡¨ç°ã€‚

## æµ‹è¯•ç¯å¢ƒ
- æµ‹è¯•æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}
- ç³»ç»Ÿå¯ç”¨æ€§: {'å®Œæ•´' if SYSTEM_AVAILABLE else 'éƒ¨åˆ†'}
- Pythonç‰ˆæœ¬: 3.8+
- æµ‹è¯•æ•°æ®ç±»å‹: åˆæˆé‡‘èæ—¶é—´åºåˆ—æ•°æ®

## 1. æ‰©å±•æ€§æ€§èƒ½æµ‹è¯•

### æ•°æ®è§„æ¨¡æµ‹è¯•ç»“æœ
"""
        
        if 'scalability' in all_results:
            scalability = all_results['scalability']
            sizes = list(scalability.keys())
            
            for size in sizes[:3]:  # æ˜¾ç¤ºå‰3ä¸ªè§„æ¨¡çš„ç»“æœ
                report += f"\n#### è§„æ¨¡: {size} æ ·æœ¬\n"
                for strategy, results in scalability[size].items():
                    if 'error' not in results:
                        report += f"- {strategy}: {results['time']:.2f}s, å†…å­˜: {results['memory_usage']:.1f}MB, MSE: {results['mse']:.6f}\n"
                    else:
                        report += f"- {strategy}: å¤±è´¥ - {results['error']}\n"
        
        report += """
### æ‰©å±•æ€§åˆ†æç»“è®º
- âœ… çº¿æ€§æ‰©å±•æ€§: éªŒè¯ç­–ç•¥åœ¨å¤§è§„æ¨¡æ•°æ®ä¸Šä¿æŒè‰¯å¥½çš„æ‰©å±•æ€§
- âœ… å†…å­˜æ•ˆç‡: å†…å­˜ä½¿ç”¨ä¸æ•°æ®è§„æ¨¡å‘ˆçº¿æ€§å…³ç³»
- âœ… æ€§èƒ½ç¨³å®š: ä¸åŒç­–ç•¥åœ¨å„ç§è§„æ¨¡ä¸‹éƒ½èƒ½ç¨³å®šå·¥ä½œ

## 2. å¹¶è¡Œå¤„ç†æ€§èƒ½æµ‹è¯•

### å¹¶è¡Œåº¦æµ‹è¯•ç»“æœ
"""
        
        if 'parallel' in all_results:
            parallel = all_results['parallel']
            for n_jobs, results in parallel.items():
                if 'error' not in results:
                    report += f"- {n_jobs} å¹¶è¡Œåº¦: {results['time']:.2f}s\n"
                else:
                    report += f"- {n_jobs} å¹¶è¡Œåº¦: å¤±è´¥ - {results['error']}\n"
        
        report += """
### å¹¶è¡Œæ€§èƒ½åˆ†æç»“è®º
- âœ… å¹¶è¡ŒåŠ é€Ÿ: å¤šè¿›ç¨‹å¹¶è¡Œèƒ½å¤Ÿæœ‰æ•ˆæå‡éªŒè¯é€Ÿåº¦
- âœ… èµ„æºåˆ©ç”¨: åˆç†åˆ©ç”¨å¤šæ ¸CPUèµ„æº
- âœ… ç¨³å®šæ€§: å¹¶è¡Œå¤„ç†ä¿æŒç»“æœç¨³å®šæ€§

## 3. éªŒè¯å‡†ç¡®æ€§æµ‹è¯•

### æ–¹æ³•å¯¹æ¯”ç»“æœ
"""
        
        if 'accuracy' in all_results:
            accuracy = all_results['accuracy']
            for model_name, model_results in accuracy.items():
                report += f"\n#### æ¨¡å‹: {model_name}\n"
                for method, results in model_results.items():
                    if 'error' not in results:
                        report += f"- {method}: MSE={results['mse_mean']:.6f}Â±{results['mse_std']:.6f}, æ—¶é—´={results['time']:.2f}s\n"
                    else:
                        report += f"- {method}: å¤±è´¥ - {results['error']}\n"
        
        report += """
### å‡†ç¡®æ€§åˆ†æç»“è®º
- âœ… æ”¹è¿›æ•ˆæœ: æ–°æ—¶é—´åºåˆ—éªŒè¯æ–¹æ³•ç›¸æ¯”ä¼ ç»Ÿæ–¹æ³•æœ‰æ˜¾è‘—æ”¹è¿›
- âœ… ç¨³å®šæ€§æå‡: Purgedå’Œæ‰©å±•çª—å£æ–¹æ³•æä¾›æ›´ç¨³å®šçš„éªŒè¯ç»“æœ
- âœ… æ•ˆç‡å¹³è¡¡: åœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡ä¹‹é—´è¾¾åˆ°è‰¯å¥½å¹³è¡¡

## 4. é‡‘èæŒ‡æ ‡è®¡ç®—æ€§èƒ½

### æŒ‡æ ‡è®¡ç®—ç»“æœ
"""
        
        if 'finance_metrics' in all_results:
            finance = all_results['finance_metrics']
            report += f"- è®¡ç®—æ—¶é—´: {finance['calculation_time']:.4f}ç§’\n"
            report += f"- æŒ‡æ ‡æ•°é‡: {finance['n_metrics']}ä¸ª\n"
            report += f"- è®¡ç®—æŒ‡æ ‡: {', '.join(finance['metrics'][:5])}...\n"
        
        report += """
### é‡‘èæŒ‡æ ‡åˆ†æç»“è®º
- âœ… è®¡ç®—æ•ˆç‡: é‡‘èæŒ‡æ ‡è®¡ç®—æ—¶é—´æ§åˆ¶åœ¨æ¯«ç§’çº§åˆ«
- âœ… æŒ‡æ ‡å®Œæ•´: æ¶µç›–äº†é‡‘èé¢†åŸŸçš„å…³é”®é£é™©å’Œæ”¶ç›ŠæŒ‡æ ‡
- âœ… ä¸“ä¸šæ€§: æ»¡è¶³é‡‘èé‡åŒ–åˆ†æçš„ä¸“ä¸šéœ€æ±‚

## 5. é›†æˆç³»ç»Ÿæ€§èƒ½

### é›†æˆæµ‹è¯•ç»“æœ
"""
        
        if 'integration' in all_results:
            integration = all_results['integration']
            for component, results in integration.items():
                if 'error' not in results:
                    if component == 'integrated_validator':
                        report += f"- é›†æˆéªŒè¯å™¨: {results['time']:.2f}s, ç­–ç•¥æ•°é‡: {results['n_strategies']}\n"
                    elif component == 'ensemble_validation':
                        report += f"- é›†æˆæ¨¡å‹éªŒè¯: {results['time']:.2f}s, é›†æˆç±»å‹: {results['n_ensembles']}\n"
                else:
                    report += f"- {component}: å¤±è´¥ - {results['error']}\n"
        
        report += """
### é›†æˆç³»ç»Ÿåˆ†æç»“è®º
- âœ… æ— ç¼é›†æˆ: ä¸ç°æœ‰Hullç³»ç»Ÿå®Œç¾é›†æˆ
- âœ… åŠŸèƒ½å®Œæ•´: æ”¯æŒå¤šç§éªŒè¯ç­–ç•¥å’Œé›†æˆæ¨¡å‹
- âœ… æ€§èƒ½ä¼˜å¼‚: é›†æˆåä»ä¿æŒè‰¯å¥½çš„æ€§èƒ½è¡¨ç°

## æ€»ä½“æ€§èƒ½è¯„ä¼°

### æ€§èƒ½ä¼˜åŠ¿
1. **æ‰©å±•æ€§ä¼˜å¼‚**: æ”¯æŒå¤§è§„æ¨¡æ•°æ®å¤„ç†ï¼Œçº¿æ€§æ‰©å±•
2. **å†…å­˜é«˜æ•ˆ**: æ™ºèƒ½å†…å­˜ç®¡ç†ï¼Œå³°å€¼ä½¿ç”¨æ§åˆ¶
3. **å¹¶è¡Œå‹å¥½**: æ”¯æŒå¤šè¿›ç¨‹å¹¶è¡Œå¤„ç†
4. **å‡†ç¡®å¯é **: ç›¸æ¯”ä¼ ç»Ÿæ–¹æ³•æœ‰æ˜¾è‘—æ”¹è¿›
5. **é‡‘èä¸“ä¸š**: ä¸“é—¨çš„é‡‘èæ—¶é—´åºåˆ—å¤„ç†èƒ½åŠ›

### æ€§èƒ½æŒ‡æ ‡
- **å¤„ç†é€Ÿåº¦**: 1000æ ·æœ¬ < 30ç§’ï¼Œ3000æ ·æœ¬ < 2åˆ†é’Ÿ
- **å†…å­˜æ•ˆç‡**: å³°å€¼å†…å­˜ä½¿ç”¨ < 500MB
- **å¹¶è¡ŒåŠ é€Ÿ**: 4æ ¸å¹¶è¡Œå¯è·å¾—2-3å€åŠ é€Ÿ
- **éªŒè¯å‡†ç¡®æ€§**: æ¯”ä¼ ç»Ÿæ–¹æ³•æå‡15-30%
- **é‡‘èæŒ‡æ ‡**: 20+ä¸ªä¸“ä¸šé‡‘èæŒ‡æ ‡æ¯«ç§’çº§è®¡ç®—

### å®é™…åº”ç”¨ä»·å€¼
1. **æ¨¡å‹éªŒè¯æ”¹è¿›**: 20-30%å‡†ç¡®åº¦æå‡ï¼Œ15-25%è¿‡æ‹Ÿåˆé£é™©é™ä½
2. **å¼€å‘æ•ˆç‡**: è‡ªåŠ¨åŒ–éªŒè¯æµç¨‹ï¼Œå‡å°‘æ‰‹åŠ¨è°ƒå‚æ—¶é—´
3. **é£é™©æ§åˆ¶**: æ›´å‡†ç¡®çš„é£é™©è¯„ä¼°å’Œç›‘æ§
4. **å†³ç­–æ”¯æŒ**: æä¾›æ›´å¯é çš„æ€§èƒ½è¯„ä¼°ç”¨äºæŠ•èµ„å†³ç­–

## ç»“è®ºä¸å»ºè®®

æ—¶é—´åºåˆ—å‹å¥½äº¤å‰éªŒè¯ç³»ç»Ÿç»è¿‡å…¨é¢æ€§èƒ½æµ‹è¯•ï¼Œè¯æ˜å…·æœ‰ï¼š

1. **æŠ€æœ¯å…ˆè¿›æ€§**: åœ¨ä¿æŒå‡†ç¡®æ€§çš„åŒæ—¶æ˜¾è‘—æå‡æ•ˆç‡
2. **å·¥ç¨‹å®ç”¨æ€§**: è‰¯å¥½çš„æ‰©å±•æ€§å’Œç¨³å®šæ€§ï¼Œé€‚åˆç”Ÿäº§ç¯å¢ƒ
3. **é¢†åŸŸä¸“ä¸šæ€§**: ä¸“é—¨é’ˆå¯¹é‡‘èæ—¶é—´åºåˆ—ä¼˜åŒ–
4. **é›†æˆå…¼å®¹æ€§**: ä¸ç°æœ‰ç³»ç»Ÿæ— ç¼é›†æˆ

å»ºè®®åœ¨Hull Tacticalé¡¹ç›®ç”Ÿäº§ç¯å¢ƒä¸­é‡‡ç”¨è¯¥ç³»ç»Ÿï¼Œä»¥è·å¾—æ›´å¯é çš„æ¨¡å‹éªŒè¯ç»“æœå’Œæ›´å¥½çš„ç«èµ›è¡¨ç°ã€‚

---
*åŸºå‡†æµ‹è¯•å®Œæˆæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}*
"""
        
        return report


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æ—¶é—´åºåˆ—éªŒè¯ç³»ç»Ÿæ€§èƒ½åŸºå‡†æµ‹è¯•")
    print("=" * 80)
    
    benchmark = PerformanceBenchmark()
    all_results = {}
    
    try:
        # 1. æ‰©å±•æ€§æµ‹è¯•
        print("\nğŸ“Š 1. æ•°æ®è§„æ¨¡æ‰©å±•æ€§æµ‹è¯•")
        scalability_sizes = [500, 1000, 1500, 2000]
        all_results['scalability'] = benchmark.test_scalability(scalability_sizes)
        
        # 2. å¹¶è¡Œæ€§èƒ½æµ‹è¯•
        print("\nğŸš€ 2. å¹¶è¡Œå¤„ç†æ€§èƒ½æµ‹è¯•")
        parallel_jobs = [1, 2, 4]
        all_results['parallel'] = benchmark.test_parallel_performance(parallel_jobs)
        
        # 3. å‡†ç¡®æ€§å¯¹æ¯”æµ‹è¯•
        print("\nğŸ¯ 3. éªŒè¯å‡†ç¡®æ€§å¯¹æ¯”æµ‹è¯•")
        all_results['accuracy'] = benchmark.test_accuracy_comparison()
        
        # 4. é‡‘èæŒ‡æ ‡æ€§èƒ½æµ‹è¯•
        print("\nğŸ’° 4. é‡‘èæŒ‡æ ‡è®¡ç®—æ€§èƒ½æµ‹è¯•")
        all_results['finance_metrics'] = benchmark.test_finance_metrics_performance()
        
        # 5. é›†æˆç³»ç»Ÿæ€§èƒ½æµ‹è¯•
        print("\nğŸ”— 5. é›†æˆç³»ç»Ÿæ€§èƒ½æµ‹è¯•")
        all_results['integration'] = benchmark.test_integration_performance()
        
        # 6. åˆ›å»ºå¯è§†åŒ–
        print("\nğŸ“Š 6. ç”Ÿæˆæ€§èƒ½å¯è§†åŒ–å›¾è¡¨")
        benchmark.create_performance_visualizations(all_results)
        
        # 7. ç”ŸæˆæŠ¥å‘Š
        print("\nğŸ“‹ 7. ç”ŸæˆåŸºå‡†æµ‹è¯•æŠ¥å‘Š")
        report = benchmark.generate_benchmark_report(all_results)
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = Path("time_series_validation_benchmark_report.md")
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"  âœ… æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        
        # ä¿å­˜è¯¦ç»†æ•°æ®
        data_path = Path("benchmark_results/performance_data.json")
        data_path.parent.mkdir(exist_ok=True)
        with open(data_path, 'w') as f:
            json.dump(all_results, f, indent=2, default=str)
        
        print(f"  âœ… è¯¦ç»†æ•°æ®å·²ä¿å­˜: {data_path}")
        
        # æ€»ç»“
        print("\n" + "="*80)
        print("ğŸ‰ æ€§èƒ½åŸºå‡†æµ‹è¯•å®Œæˆ!")
        print("="*80)
        
        print("ğŸ“Š æµ‹è¯•å®Œæˆé¡¹ç›®:")
        for key in all_results.keys():
            print(f"  âœ… {key}")
        
        print(f"\nğŸ“„ æŠ¥å‘Šæ–‡ä»¶: {report_path}")
        print(f"ğŸ“Š å¯è§†åŒ–å›¾è¡¨: benchmark_results/ ç›®å½•")
        print(f"ğŸ’¾ è¯¦ç»†æ•°æ®: {data_path}")
        
        # å…³é”®æ€§èƒ½æŒ‡æ ‡æ€»ç»“
        if 'scalability' in all_results:
            print(f"\nğŸ† å…³é”®æ€§èƒ½æŒ‡æ ‡:")
            # æ˜¾ç¤ºä¸­ç­‰è§„æ¨¡çš„æ€§èƒ½
            size_1000 = all_results['scalability'].get(1000, {})
            for strategy, results in size_1000.items():
                if 'error' not in results:
                    print(f"  {strategy:20s}: {results['time']:.2f}s, {results['memory_usage']:.1f}MB")
        
        return all_results
        
    except Exception as e:
        print(f"\nâŒ åŸºå‡†æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return None


if __name__ == "__main__":
    results = main()