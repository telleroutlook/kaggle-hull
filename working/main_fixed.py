#!/usr/bin/env python3
"""
Hull Tactical - Market Prediction ä¸»æ¨¡å‹æ–‡ä»¶
Kaggleç«èµ›çš„æ¨¡å‹å…¥å£ç‚¹
"""

import argparse
import sys
import os
import pandas as pd
import numpy as np
from pathlib import Path

# æ·»åŠ libç›®å½•åˆ°è·¯å¾„
working_dir = os.path.dirname(__file__)
if working_dir:
    sys.path.insert(0, working_dir)
else:
    # å¦‚æœ__file__ä¸ºç©ºï¼ˆåœ¨æŸäº›ç¯å¢ƒä¸­ï¼‰ï¼Œä½¿ç”¨å½“å‰å·¥ä½œç›®å½•
    sys.path.insert(0, os.getcwd())

import logging

from lib.env import detect_run_environment, get_data_paths, get_log_paths
from lib.data import load_test_data, validate_data
from lib.features import engineer_features, get_feature_columns
from lib.models import HullModel, create_submission
from lib.utils import PerformanceTracker, save_logs, save_metrics, validate_submission, write_result_json

# å°è¯•å¯¼å…¥é…ç½®æ¨¡å—
try:
    from lib.config import ConfigManager, get_config
    CONFIG_AVAILABLE = True
except ImportError:
    CONFIG_AVAILABLE = False
    print("âš ï¸ é…ç½®æ¨¡å—ä¸å¯ç”¨")

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def parse_args(argv=None):
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="Hull Tactical - Market Prediction æ¨¡å‹")
    
    parser.add_argument(
        "--model-type",
        choices=["baseline", "lightgbm", "xgboost", "ensemble"],
        default="baseline",
        help="é€‰æ‹©æ¨¡å‹ç±»å‹ (é»˜è®¤: baseline)"
    )
    
    parser.add_argument(
        "--data-path",
        type=Path,
        default=None,
        help="æ˜¾å¼æŒ‡å®šæ•°æ®è·¯å¾„ï¼ˆè¦†ç›–è‡ªåŠ¨æ£€æµ‹ï¼‰"
    )
    
    parser.add_argument(
        "--output",
        type=Path,
        default=None,
        help="è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤ï¼šæ ¹æ®ç¯å¢ƒè‡ªåŠ¨è®¾ç½®ï¼‰"
    )
    
    parser.add_argument(
        "--config",
        type=Path,
        default=None,
        help="é…ç½®æ–‡ä»¶è·¯å¾„"
    )
    
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="è¯¦ç»†è¾“å‡ºæ¨¡å¼"
    )
    
    # å¤„ç†ä¸åŒçš„è¿è¡Œç¯å¢ƒå‚æ•°
    if argv is None:
        # è¿‡æ»¤æ‰Jupyterå†…æ ¸å‚æ•°å’Œä¸ç›¸å…³çš„å‚æ•°
        filtered_argv = [arg for arg in sys.argv[1:] if not arg.startswith('-f') and not ':memory:' in arg]
        try:
            return parser.parse_args(filtered_argv)
        except SystemExit:
            # å¦‚æœè§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å‚æ•°
            print("âš ï¸ å‚æ•°è§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å‚æ•°")
            return parser.parse_args([])
    
    return parser.parse_args(argv)


def main():
    """ä¸»å‡½æ•° - è¿è¡Œæ¨¡å‹é¢„æµ‹"""
    
    args = parse_args()
    
    # åˆå§‹åŒ–é…ç½®
    if CONFIG_AVAILABLE and args.config:
        config_manager = ConfigManager(str(args.config))
        logging_config = config_manager.get_logging_config()
        # æ›´æ–°æ—¥å¿—é…ç½®
        logging.basicConfig(
            level=getattr(logging, logging_config['level'], logging.INFO),
            format=logging_config['format']
        )
    elif CONFIG_AVAILABLE:
        config_manager = get_config()
    else:
        config_manager = None
    
    tracker = PerformanceTracker(logger=logger)
    
    print("ğŸš€ Hull Tactical - Market Prediction æ¨¡å‹å¯åŠ¨")
    print(f"ğŸ“‹ æ¨¡å‹ç±»å‹: {args.model_type}")
    
    # æ£€æµ‹è¿è¡Œç¯å¢ƒ
    env = detect_run_environment()
    data_paths = get_data_paths(env)
    log_paths = get_log_paths(env)
    result_path = Path("/kaggle/working/result.json") if env == "kaggle" else log_paths.log_jsonl.parent / "result.json"
    
    print(f"ğŸ  è¿è¡Œç¯å¢ƒ: {env}")
    print(f"ğŸ“ æ•°æ®è·¯å¾„: {data_paths.test_data}")
    
    # è®¾ç½®è¾“å‡ºè·¯å¾„
    if args.output:
        submission_path = args.output
    else:
        submission_path = log_paths.submission_file
    
    try:
        # åŠ è½½æ•°æ®
        tracker.start_task("load_data")
        tracker.record_memory_usage()
        test_data = load_test_data(data_paths)
        
        if not validate_data(test_data, "test"):
            return 1
        
        tracker.record_memory_usage()
        tracker.end_task()
        
        # ç‰¹å¾å·¥ç¨‹
        tracker.start_task("feature_engineering")
        tracker.record_memory_usage()
        feature_cols = get_feature_columns(test_data)
        features = engineer_features(test_data, feature_cols)
        
        if args.verbose:
            print(f"ğŸ”§ ç‰¹å¾æ•°é‡: {len(feature_cols)}")
            print(f"ğŸ“Š ç‰¹å¾å½¢çŠ¶: {features.shape}")
        
        tracker.record_memory_usage()
        tracker.end_task()
        
        # æ¨¡å‹é¢„æµ‹
        tracker.start_task("model_prediction")
        tracker.record_memory_usage()
        
        # åˆ›å»ºå¹¶è®­ç»ƒæ¨¡å‹ï¼ˆè¿™é‡Œä½¿ç”¨åŸºçº¿æ¨¡å‹ï¼‰
        model = HullModel(model_type=args.model_type)
        
        # æ³¨æ„ï¼šåœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œåº”è¯¥åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
        # ç›®å‰ä½¿ç”¨ç®€å•çš„éšæœºé¢„æµ‹ä½œä¸ºæ¼”ç¤º
        np.random.seed(42)
        predictions = np.random.uniform(0, 2, size=len(test_data))
        
        tracker.record_memory_usage()
        tracker.end_task()
        
        # åˆ›å»ºæäº¤æ–‡ä»¶
        tracker.start_task("create_submission")
        submission_df = create_submission(predictions, test_data['date_id'])
        
        # éªŒè¯æäº¤æ–‡ä»¶
        if not validate_submission(submission_df):
            return 1
        
        # ä¿å­˜æäº¤æ–‡ä»¶ - æ ¹æ®ç¯å¢ƒé€‰æ‹©è¾“å‡ºç›®å½•
        if env == "kaggle":
            working_dir_path = Path("/kaggle/working")
        else:
            working_dir_path = log_paths.log_jsonl.parent
        working_dir_path.mkdir(parents=True, exist_ok=True)
        
        kaggle_submission_path = working_dir_path / "submission.csv"
        submission_df.to_csv(kaggle_submission_path, index=False)
        
        # åŒæ—¶ä¿å­˜parquetæ ¼å¼ï¼ˆå¦‚æœå¯èƒ½ï¼‰
        try:
            kaggle_parquet_path = working_dir_path / "submission.parquet"
            submission_df.to_parquet(kaggle_parquet_path, index=False)
        except Exception as e:
            print(f"âš ï¸  æ— æ³•ä¿å­˜parquetæ ¼å¼ï¼Œä½¿ç”¨CSV: {e}")
            
        # ä¹Ÿä¿å­˜åˆ°åŸå§‹è·¯å¾„
        try:
            submission_df.to_csv(submission_path.with_suffix('.csv'), index=False)
            # åŒæ—¶ä¿å­˜parquetæ ¼å¼ï¼ˆå¦‚æœå¯èƒ½ï¼‰
            try:
                submission_df.to_parquet(submission_path, index=False)
            except Exception as e:
                print(f"âš ï¸  æ— æ³•ä¿å­˜parquetæ ¼å¼ï¼Œä½¿ç”¨CSV: {e}")
        except Exception as e:
            print(f"âš ï¸  æ— æ³•ä¿å­˜åˆ°åŸå§‹è·¯å¾„: {e}")
            
        tracker.end_task()
        
        # è®°å½•æ€§èƒ½æŒ‡æ ‡
        tracker.start_task("logging")
        metrics = {
            'num_predictions': len(predictions),
            'min_prediction': float(predictions.min()),
            'max_prediction': float(predictions.max()),
            'mean_prediction': float(predictions.mean()),
            'std_prediction': float(predictions.std()),
            'model_type': args.model_type,
            'environment': env
        }
        
        # ä¿å­˜æ—¥å¿—å’ŒæŒ‡æ ‡åˆ°Kaggleå·¥ä½œç›®å½•
        try:
            kaggle_log_path = Path("/kaggle/working/hull_logs.jsonl")
            kaggle_metrics_path = Path("/kaggle/working/hull_metrics.csv")
            save_logs(tracker.get_summary(), kaggle_log_path)
            save_metrics(metrics, kaggle_metrics_path)
        except Exception as e:
            print(f"âš ï¸  æ— æ³•ä¿å­˜æ—¥å¿—åˆ°Kaggleå·¥ä½œç›®å½•: {e}")
            
        # ä¹Ÿä¿å­˜åˆ°åŸå§‹è·¯å¾„
        try:
            save_logs(tracker.get_summary(), log_paths.log_jsonl)
            save_metrics(metrics, log_paths.metrics_csv)
        except Exception as e:
            print(f"âš ï¸  æ— æ³•ä¿å­˜æ—¥å¿—åˆ°åŸå§‹è·¯å¾„: {e}")
            
        tracker.end_task()
        
        # è¾“å‡ºç»“æœ
        print(f"\nâœ… æäº¤æ–‡ä»¶å·²ä¿å­˜: {kaggle_submission_path}")
        print(f"ğŸ“ˆ é¢„æµ‹ç»Ÿè®¡:")
        print(f"   é¢„æµ‹æ•°é‡: {len(predictions)}")
        print(f"   æœ€å°å€¼: {predictions.min():.4f}")
        print(f"   æœ€å¤§å€¼: {predictions.max():.4f}")
        print(f"   å¹³å‡å€¼: {predictions.mean():.4f}")
        print(f"   æ ‡å‡†å·®: {predictions.std():.4f}")
        
        # è¾“å‡ºæ€§èƒ½æ‘˜è¦
        summary = tracker.get_summary()
        print(f"\nâ±ï¸ æ€§èƒ½æ‘˜è¦:")
        print(f"   æ€»æ—¶é—´: {summary['total_time_seconds']:.2f}ç§’")
        for task, duration in summary['task_breakdown'].items():
            print(f"   {task}: {duration:.2f}ç§’")
        
        write_result_json(True, result_path)
        return 0
        
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        write_result_json(False, result_path, error_details=str(e))
        return 1


if __name__ == "__main__":
    sys.exit(main())
