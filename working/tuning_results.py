#!/usr/bin/env python3
"""
Hull Tactical - è¶…å‚æ•°è°ƒä¼˜ç»“æœåˆ†æå’ŒæŠ¥å‘Šç³»ç»Ÿ
ç”Ÿæˆè¯¦ç»†çš„æ€§èƒ½åˆ†ææŠ¥å‘Šã€å¯¹æ¯”å›¾è¡¨å’Œä¼˜åŒ–å»ºè®®
"""

import os
import json
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
import warnings
from dataclasses import asdict
import pickle

# è®¾ç½®matplotlibä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False

warnings.filterwarnings('ignore')

try:
    import plotly.graph_objects as go
    import plotly.express as px
    from plotly.subplots import make_subplots
    PLOTLY_AVAILABLE = True
except ImportError:
    PLOTLY_AVAILABLE = False
    print("âš ï¸ Plotlyæœªå®‰è£…ï¼Œå°†ä½¿ç”¨matplotlibä½œä¸ºåå¤‡")


class TuningResultAnalyzer:
    """è°ƒä¼˜ç»“æœåˆ†æå™¨"""
    
    def __init__(self, results_dir: str = "tuning_results"):
        self.results_dir = Path(results_dir)
        self.results_data = {}
        self.summary_stats = {}
        
    def load_results(self, file_pattern: str = "*.json") -> Dict[str, Any]:
        """åŠ è½½è°ƒä¼˜ç»“æœæ–‡ä»¶"""
        result_files = list(self.results_dir.glob(file_pattern))
        
        if not result_files:
            print(f"åœ¨ {self.results_dir} ä¸­æœªæ‰¾åˆ°ç»“æœæ–‡ä»¶")
            return {}
        
        all_results = {}
        for file_path in result_files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    
                # æå–åŸºæœ¬ä¿¡æ¯
                file_key = file_path.stem
                all_results[file_key] = {
                    "file_path": str(file_path),
                    "config": data.get("config", {}),
                    "results": data.get("results", {}),
                    "rankings": data.get("rankings", []),
                    "timestamp": data.get("timestamp", "")
                }
                
            except Exception as e:
                print(f"åŠ è½½æ–‡ä»¶ {file_path} å¤±è´¥: {e}")
                continue
        
        self.results_data = all_results
        return all_results
    
    def analyze_model_performance(self) -> pd.DataFrame:
        """åˆ†ææ¨¡å‹æ€§èƒ½"""
        performance_data = []
        
        for file_key, data in self.results_data.items():
            results = data.get("results", {})
            
            for model_type, result in results.items():
                # æå–CVåˆ†æ•°
                cv_scores = result.get("cv_scores", {})
                mean_scores = result.get("mean_scores", {})
                std_scores = result.get("std_scores", {})
                
                row = {
                    "file_key": file_key,
                    "model_type": model_type,
                    "best_score": result.get("best_score", np.nan),
                    "tuning_time": result.get("tuning_time", np.nan),
                    "n_trials": result.get("n_trials", 0),
                    "config": data.get("config", {})
                }
                
                # æ·»åŠ å„ä¸ªæŒ‡æ ‡çš„å‡å€¼å’Œæ ‡å‡†å·®
                for metric in ["mse", "mae", "r2"]:
                    if metric in mean_scores:
                        row[f"{metric}_mean"] = mean_scores[metric]
                        row[f"{metric}_std"] = std_scores.get(metric, np.nan)
                    else:
                        row[f"{metric}_mean"] = np.nan
                        row[f"{metric}_std"] = np.nan
                
                performance_data.append(row)
        
        self.performance_df = pd.DataFrame(performance_data)
        return self.performance_df
    
    def generate_performance_summary(self) -> Dict[str, Any]:
        """ç”Ÿæˆæ€§èƒ½æ‘˜è¦"""
        if not hasattr(self, 'performance_df'):
            self.analyze_model_performance()
        
        summary = {
            "total_experiments": len(self.performance_df),
            "unique_models": self.performance_df['model_type'].nunique(),
            "best_overall": {},
            "model_rankings": {},
            "performance_improvements": {},
            "statistical_summary": {}
        }
        
        # æœ€ä½³æ¨¡å‹
        best_overall_idx = self.performance_df['mse_mean'].idxmin()
        best_overall = self.performance_df.loc[best_overall_idx]
        summary["best_overall"] = {
            "model_type": best_overall['model_type'],
            "mse": best_overall['mse_mean'],
            "file_key": best_overall['file_key'],
            "improvement_vs_baseline": self._calculate_improvement(best_overall)
        }
        
        # æ¨¡å‹æ’å
        model_rankings = (self.performance_df.groupby('model_type')['mse_mean']
                         .mean()
                         .sort_values()
                         .to_dict())
        summary["model_rankings"] = model_rankings
        
        # æ€§èƒ½æ”¹è¿›ï¼ˆå¦‚æœæœ‰åŸºçº¿æ¨¡å‹ï¼‰
        if 'random_forest' in model_rankings:
            baseline_mse = model_rankings['random_forest']
            improvements = {}
            for model, mse in model_rankings.items():
                if model != 'random_forest':
                    improvement = (baseline_mse - mse) / baseline_mse * 100
                    improvements[model] = f"{improvement:.2f}%"
            summary["performance_improvements"] = improvements
        
        # ç»Ÿè®¡æ‘˜è¦
        summary["statistical_summary"] = self.performance_df.groupby('model_type').agg({
            'mse_mean': ['mean', 'std', 'min', 'max'],
            'mae_mean': ['mean', 'std', 'min', 'max'],
            'r2_mean': ['mean', 'std', 'min', 'max'],
            'tuning_time': ['mean', 'std', 'min', 'max']
        }).round(6).to_dict()
        
        self.summary_stats = summary
        return summary
    
    def _calculate_improvement(self, result_row) -> str:
        """è®¡ç®—æ€§èƒ½æ”¹è¿›"""
        # è¿™é‡Œå¯ä»¥ä¸åŸºçº¿æ¨¡å‹æ¯”è¾ƒ
        baseline_mse = 0.25  # å‡è®¾åŸºçº¿MSE
        improvement = (baseline_mse - result_row['mse_mean']) / baseline_mse * 100
        return f"{improvement:.2f}%"
    
    def create_performance_comparison_chart(self, output_path: str = None) -> str:
        """åˆ›å»ºæ€§èƒ½å¯¹æ¯”å›¾è¡¨"""
        if not hasattr(self, 'performance_df'):
            self.analyze_model_performance()
        
        # è®¾ç½®å›¾è¡¨æ ·å¼
        plt.style.use('seaborn-v0_8')
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('Hull Tactical - è¶…å‚æ•°è°ƒä¼˜æ€§èƒ½åˆ†æ', fontsize=16, fontweight='bold')
        
        # 1. MSEå¯¹æ¯”
        ax1 = axes[0, 0]
        mse_data = self.performance_df.dropna(subset=['mse_mean'])
        if not mse_data.empty:
            mse_data.boxplot(column='mse_mean', by='model_type', ax=ax1)
            ax1.set_title('MSEåˆ†å¸ƒå¯¹æ¯”')
            ax1.set_xlabel('æ¨¡å‹ç±»å‹')
            ax1.set_ylabel('MSE')
            ax1.tick_params(axis='x', rotation=45)
        
        # 2. MAEå¯¹æ¯”
        ax2 = axes[0, 1]
        mae_data = self.performance_df.dropna(subset=['mae_mean'])
        if not mae_data.empty:
            mae_data.boxplot(column='mae_mean', by='model_type', ax=ax2)
            ax2.set_title('MAEåˆ†å¸ƒå¯¹æ¯”')
            ax2.set_xlabel('æ¨¡å‹ç±»å‹')
            ax2.set_ylabel('MAE')
            ax2.tick_params(axis='x', rotation=45)
        
        # 3. RÂ²å¯¹æ¯”
        ax3 = axes[1, 0]
        r2_data = self.performance_df.dropna(subset=['r2_mean'])
        if not r2_data.empty:
            r2_data.boxplot(column='r2_mean', by='model_type', ax=ax3)
            ax3.set_title('RÂ²åˆ†å¸ƒå¯¹æ¯”')
            ax3.set_xlabel('æ¨¡å‹ç±»å‹')
            ax3.set_ylabel('RÂ²')
            ax3.tick_params(axis='x', rotation=45)
        
        # 4. è°ƒä¼˜æ—¶é—´å¯¹æ¯”
        ax4 = axes[1, 1]
        time_data = self.performance_df.dropna(subset=['tuning_time'])
        if not time_data.empty:
            time_data.boxplot(column='tuning_time', by='model_type', ax=ax4)
            ax4.set_title('è°ƒä¼˜æ—¶é—´å¯¹æ¯”')
            ax4.set_xlabel('æ¨¡å‹ç±»å‹')
            ax4.set_ylabel('è°ƒä¼˜æ—¶é—´(ç§’)')
            ax4.tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        
        if output_path is None:
            output_path = self.results_dir / "performance_comparison.png"
        else:
            output_path = Path(output_path)
        
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        return str(output_path)
    
    def create_model_ranking_chart(self, output_path: str = None) -> str:
        """åˆ›å»ºæ¨¡å‹æ’åå›¾è¡¨"""
        if not self.summary_stats:
            self.generate_performance_summary()
        
        model_rankings = self.summary_stats['model_rankings']
        
        # åˆ›å»ºæ’åå›¾è¡¨
        models = list(model_rankings.keys())
        scores = list(model_rankings.values())
        
        # æŒ‰åˆ†æ•°æ’åº
        sorted_data = sorted(zip(models, scores), key=lambda x: x[1])
        models_sorted, scores_sorted = zip(*sorted_data)
        
        plt.figure(figsize=(12, 6))
        
        # åˆ›å»ºæ¡å½¢å›¾
        bars = plt.bar(models_sorted, scores_sorted, 
                      color=['#2E8B57', '#4682B4', '#CD853F', '#DC143C'][:len(models_sorted)])
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, score in zip(bars, scores_sorted):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,
                    f'{score:.4f}', ha='center', va='bottom', fontweight='bold')
        
        plt.title('Hull Tactical - æ¨¡å‹æ€§èƒ½æ’å (MSE)', fontsize=14, fontweight='bold')
        plt.xlabel('æ¨¡å‹ç±»å‹', fontsize=12)
        plt.ylabel('MSE (è¶Šä½è¶Šå¥½)', fontsize=12)
        plt.xticks(rotation=45)
        plt.grid(axis='y', alpha=0.3)
        
        # æ·»åŠ æ€§èƒ½æ”¹è¿›æ ‡æ³¨
        if 'random_forest' in model_rankings:
            baseline_score = model_rankings['random_forest']
            for i, (model, score) in enumerate(zip(models_sorted, scores_sorted)):
                if model != 'random_forest':
                    improvement = (baseline_score - score) / baseline_score * 100
                    plt.text(i, score/2, f'+{improvement:.1f}%', 
                            ha='center', va='center', fontsize=10, 
                            color='white', fontweight='bold')
        
        plt.tight_layout()
        
        if output_path is None:
            output_path = self.results_dir / "model_ranking.png"
        else:
            output_path = Path(output_path)
        
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        return str(output_path)
    
    def analyze_parameter_importance(self) -> Dict[str, Any]:
        """åˆ†æå‚æ•°é‡è¦æ€§"""
        param_importance = {}
        
        for file_key, data in self.results_data.items():
            results = data.get("results", {})
            
            for model_type, result in results.items():
                best_params = result.get("best_params", {})
                cv_scores = result.get("cv_scores", {})
                
                if not best_params:
                    continue
                
                # è®¡ç®—æ¯ä¸ªå‚æ•°çš„å˜å¼‚ç³»æ•°
                param_stats = {}
                for param, value in best_params.items():
                    if isinstance(value, (int, float)):
                        # å¯¹äºæ•°å€¼å‚æ•°ï¼Œè®°å½•å…¶å€¼
                        param_stats[param] = {
                            "value": value,
                            "type": "numeric"
                        }
                    else:
                        # å¯¹äºåˆ†ç±»å‚æ•°ï¼Œè®°å½•å…¶å€¼
                        param_stats[param] = {
                            "value": str(value),
                            "type": "categorical"
                        }
                
                param_importance[f"{file_key}_{model_type}"] = {
                    "best_params": param_stats,
                    "performance": {
                        "mse_mean": np.mean(cv_scores.get("mse", [np.nan])),
                        "mse_std": np.std(cv_scores.get("mse", [np.nan])),
                        "n_trials": result.get("n_trials", 0)
                    }
                }
        
        return param_importance
    
    def generate_recommendations(self) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        if not self.summary_stats:
            self.generate_performance_summary()
        
        recommendations = []
        
        # åŸºäºæ€§èƒ½åˆ†æçš„å»ºè®®
        model_rankings = self.summary_stats['model_rankings']
        
        if model_rankings:
            best_model = min(model_rankings.keys(), key=lambda x: model_rankings[x])
            worst_model = max(model_rankings.keys(), key=lambda x: model_rankings[x])
            
            recommendations.append(f"ğŸ† æœ€ä½³æ¨¡å‹: {best_model} (MSE: {model_rankings[best_model]:.4f})")
            recommendations.append(f"ğŸ“Š å»ºè®®é‡ç‚¹ä¼˜åŒ–: {worst_model} (å½“å‰MSE: {model_rankings[worst_model]:.4f})")
            
            # æ€§èƒ½å·®å¼‚åˆ†æ
            if len(model_rankings) >= 2:
                scores = list(model_rankings.values())
                score_diff = max(scores) - min(scores)
                if score_diff > 0.01:  # å¦‚æœå·®å¼‚æ˜¾è‘—
                    recommendations.append(f"âš ï¸ æ¨¡å‹é—´æ€§èƒ½å·®å¼‚è¾ƒå¤§ ({score_diff:.4f})ï¼Œå»ºè®®è¿›è¡Œé›†æˆå­¦ä¹ ")
            
            # è°ƒä¼˜æ•ˆç‡å»ºè®®
            perf_df = self.performance_df
            if not perf_df.empty:
                avg_tuning_time = perf_df['tuning_time'].mean()
                if avg_tuning_time > 600:  # å¦‚æœè°ƒä¼˜æ—¶é—´è¶…è¿‡10åˆ†é’Ÿ
                    recommendations.append(f"â±ï¸ å¹³å‡è°ƒä¼˜æ—¶é—´è¾ƒé•¿ ({avg_tuning_time:.0f}s)ï¼Œå»ºè®®å‡å°‘è¯•éªŒæ¬¡æ•°æˆ–ä½¿ç”¨å¹¶è¡Œè®¡ç®—")
                
                # æ£€æŸ¥è¿‡æ‹Ÿåˆé£é™©
                for model in model_rankings.keys():
                    model_data = perf_df[perf_df['model_type'] == model]
                    if not model_data.empty:
                        mse_std = model_data['mse_std'].mean()
                        if not np.isnan(mse_std) and mse_std > 0.1:
                            recommendations.append(f"ğŸ” {model}å­˜åœ¨è¿‡æ‹Ÿåˆé£é™©ï¼ŒCVæ ‡å‡†å·®è¾ƒå¤§ ({mse_std:.4f})ï¼Œå»ºè®®å¢åŠ æ­£åˆ™åŒ–")
        
        # åŸºäºå‚æ•°åˆ†æçš„å»ºè®®
        param_importance = self.analyze_parameter_importance()
        if param_importance:
            recommendations.append("ğŸ“ˆ å‚æ•°åˆ†æå®Œæˆï¼Œå»ºè®®æŸ¥çœ‹è¯¦ç»†å‚æ•°é‡è¦æ€§æŠ¥å‘Š")
        
        return recommendations
    
    def generate_html_report(self, output_path: str = None) -> str:
        """ç”ŸæˆHTMLæŠ¥å‘Š"""
        if output_path is None:
            output_path = self.results_dir / "tuning_report.html"
        else:
            output_path = Path(output_path)
        
        # ç”Ÿæˆåˆ†ææ•°æ®
        performance_df = self.analyze_model_performance()
        summary = self.generate_performance_summary()
        recommendations = self.generate_recommendations()
        
        # åˆ›å»ºå›¾è¡¨
        chart1_path = self.create_performance_comparison_chart()
        chart2_path = self.create_model_ranking_chart()
        
        # HTMLæ¨¡æ¿
        html_content = f"""
        <!DOCTYPE html>
        <html lang="zh-CN">
        <head>
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>Hull Tactical - è¶…å‚æ•°è°ƒä¼˜æŠ¥å‘Š</title>
            <style>
                body {{
                    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
                    line-height: 1.6;
                    margin: 0;
                    padding: 20px;
                    background-color: #f5f5f5;
                }}
                .container {{
                    max-width: 1200px;
                    margin: 0 auto;
                    background: white;
                    padding: 30px;
                    border-radius: 10px;
                    box-shadow: 0 0 20px rgba(0,0,0,0.1);
                }}
                h1 {{
                    color: #2c3e50;
                    text-align: center;
                    margin-bottom: 30px;
                    border-bottom: 3px solid #3498db;
                    padding-bottom: 10px;
                }}
                h2 {{
                    color: #34495e;
                    margin-top: 30px;
                    border-left: 4px solid #3498db;
                    padding-left: 15px;
                }}
                .summary-box {{
                    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                    color: white;
                    padding: 20px;
                    border-radius: 10px;
                    margin: 20px 0;
                }}
                .metric-grid {{
                    display: grid;
                    grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
                    gap: 20px;
                    margin: 20px 0;
                }}
                .metric-card {{
                    background: #f8f9fa;
                    padding: 20px;
                    border-radius: 8px;
                    border-left: 4px solid #3498db;
                }}
                .metric-value {{
                    font-size: 24px;
                    font-weight: bold;
                    color: #2c3e50;
                }}
                .metric-label {{
                    font-size: 14px;
                    color: #7f8c8d;
                    margin-top: 5px;
                }}
                .chart-container {{
                    text-align: center;
                    margin: 30px 0;
                }}
                .chart-container img {{
                    max-width: 100%;
                    height: auto;
                    border-radius: 8px;
                    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
                }}
                .recommendations {{
                    background: #e8f5e8;
                    border: 1px solid #4caf50;
                    border-radius: 8px;
                    padding: 20px;
                    margin: 20px 0;
                }}
                .recommendations h3 {{
                    color: #2e7d32;
                    margin-top: 0;
                }}
                .recommendations ul {{
                    list-style-type: none;
                    padding: 0;
                }}
                .recommendations li {{
                    margin: 10px 0;
                    padding: 10px;
                    background: white;
                    border-radius: 5px;
                    border-left: 4px solid #4caf50;
                }}
                .data-table {{
                    width: 100%;
                    border-collapse: collapse;
                    margin: 20px 0;
                }}
                .data-table th, .data-table td {{
                    border: 1px solid #ddd;
                    padding: 12px;
                    text-align: left;
                }}
                .data-table th {{
                    background-color: #f2f2f2;
                    font-weight: bold;
                }}
                .data-table tr:nth-child(even) {{
                    background-color: #f9f9f9;
                }}
                .footer {{
                    text-align: center;
                    margin-top: 50px;
                    color: #7f8c8d;
                    font-size: 14px;
                }}
            </style>
        </head>
        <body>
            <div class="container">
                <h1>ğŸš€ Hull Tactical - è¶…å‚æ•°è°ƒä¼˜æŠ¥å‘Š</h1>
                
                <div class="summary-box">
                    <h2>ğŸ“Š è°ƒä¼˜æ‘˜è¦</h2>
                    <p><strong>ç”Ÿæˆæ—¶é—´:</strong> {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
                    <p><strong>å®éªŒæ•°é‡:</strong> {summary['total_experiments']}</p>
                    <p><strong>æ¨¡å‹ç±»å‹:</strong> {summary['unique_models']}</p>
                    <p><strong>æœ€ä½³æ¨¡å‹:</strong> {summary['best_overall'].get('model_type', 'N/A')}</p>
                </div>
                
                <h2>ğŸ† æ€§èƒ½æ’å</h2>
                <div class="metric-grid">
                    {self._generate_ranking_cards(summary['model_rankings'])}
                </div>
                
                <h2>ğŸ“ˆ æ€§èƒ½åˆ†æå›¾è¡¨</h2>
                <div class="chart-container">
                    <h3>æ¨¡å‹æ€§èƒ½å¯¹æ¯”</h3>
                    <img src="{Path(chart1_path).name}" alt="æ€§èƒ½å¯¹æ¯”å›¾">
                </div>
                
                <div class="chart-container">
                    <h3>æ¨¡å‹æ’å</h3>
                    <img src="{Path(chart2_path).name}" alt="æ¨¡å‹æ’åå›¾">
                </div>
                
                <h2>ğŸ’¡ ä¼˜åŒ–å»ºè®®</h2>
                <div class="recommendations">
                    <h3>æ™ºèƒ½å»ºè®®</h3>
                    <ul>
                        {self._generate_recommendations_html(recommendations)}
                    </ul>
                </div>
                
                <h2>ğŸ“‹ è¯¦ç»†æ€§èƒ½æ•°æ®</h2>
                {self._generate_data_table(performance_df)}
                
                <div class="footer">
                    <p>æŠ¥å‘Šç”± Hull Tactical è‡ªåŠ¨ç”Ÿæˆ | æ•°æ®æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
                </div>
            </div>
        </body>
        </html>
        """
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        return str(output_path)
    
    def _generate_ranking_cards(self, rankings: Dict[str, float]) -> str:
        """ç”Ÿæˆæ’åå¡ç‰‡HTML"""
        cards = ""
        for i, (model, score) in enumerate(sorted(rankings.items(), key=lambda x: x[1]), 1):
            cards += f"""
            <div class="metric-card">
                <div class="metric-value">#{i} {model.title()}</div>
                <div class="metric-label">MSE: {score:.4f}</div>
            </div>
            """
        return cards
    
    def _generate_recommendations_html(self, recommendations: List[str]) -> str:
        """ç”Ÿæˆå»ºè®®HTML"""
        items = ""
        for rec in recommendations:
            items += f"<li>{rec}</li>"
        return items
    
    def _generate_data_table(self, df: pd.DataFrame) -> str:
        """ç”Ÿæˆæ•°æ®è¡¨æ ¼HTML"""
        if df.empty:
            return "<p>æš‚æ— æ•°æ®</p>"
        
        # é€‰æ‹©å…³é”®åˆ—
        key_columns = ['model_type', 'mse_mean', 'mae_mean', 'r2_mean', 'tuning_time', 'n_trials']
        display_df = df[key_columns].round(4)
        
        table_html = "<table class='data-table'><tr>"
        for col in display_df.columns:
            table_html += f"<th>{col.title()}</th>"
        table_html += "</tr>"
        
        for _, row in display_df.iterrows():
            table_html += "<tr>"
            for col in display_df.columns:
                value = row[col]
                if pd.isna(value):
                    value = "N/A"
                table_html += f"<td>{value}</td>"
            table_html += "</tr>"
        
        table_html += "</table>"
        return table_html


def main():
    """ä¸»å‡½æ•° - ç”Ÿæˆå®Œæ•´çš„è°ƒä¼˜æŠ¥å‘Š"""
    analyzer = TuningResultAnalyzer()
    
    # åŠ è½½ç»“æœ
    print("ğŸ” æ­£åœ¨åŠ è½½è°ƒä¼˜ç»“æœ...")
    results = analyzer.load_results()
    
    if not results:
        print("âŒ æœªæ‰¾åˆ°è°ƒä¼˜ç»“æœæ–‡ä»¶")
        return
    
    print(f"âœ… æˆåŠŸåŠ è½½ {len(results)} ä¸ªå®éªŒç»“æœ")
    
    # ç”Ÿæˆåˆ†æ
    print("ğŸ“Š æ­£åœ¨ç”Ÿæˆæ€§èƒ½åˆ†æ...")
    analyzer.analyze_model_performance()
    
    print("ğŸ“ˆ æ­£åœ¨ç”Ÿæˆæ€§èƒ½æ‘˜è¦...")
    summary = analyzer.generate_performance_summary()
    
    # ç”Ÿæˆå›¾è¡¨
    print("ğŸ¨ æ­£åœ¨ç”Ÿæˆå›¾è¡¨...")
    chart1 = analyzer.create_performance_comparison_chart()
    chart2 = analyzer.create_model_ranking_chart()
    
    # ç”Ÿæˆå»ºè®®
    print("ğŸ’¡ æ­£åœ¨ç”Ÿæˆä¼˜åŒ–å»ºè®®...")
    recommendations = analyzer.generate_recommendations()
    
    # ç”ŸæˆHTMLæŠ¥å‘Š
    print("ğŸ“ æ­£åœ¨ç”ŸæˆHTMLæŠ¥å‘Š...")
    html_report = analyzer.generate_html_report()
    
    # è¾“å‡ºç»“æœ
    print("\n" + "="*60)
    print("ğŸ‰ è°ƒä¼˜ç»“æœåˆ†æå®Œæˆï¼")
    print("="*60)
    
    print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
    print(f"   ğŸ“Š æ€§èƒ½å¯¹æ¯”å›¾: {chart1}")
    print(f"   ğŸ† æ¨¡å‹æ’åå›¾: {chart2}")
    print(f"   ğŸ“„ HTMLæŠ¥å‘Š: {html_report}")
    
    print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {summary['best_overall'].get('model_type', 'N/A')}")
    print(f"   MSE: {summary['best_overall'].get('mse', 'N/A')}")
    
    print(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
    for rec in recommendations[:5]:  # æ˜¾ç¤ºå‰5ä¸ªå»ºè®®
        print(f"   {rec}")
    
    print(f"\nğŸ“‹ å®Œæ•´æŠ¥å‘Šè¯·æŸ¥çœ‹: {html_report}")


if __name__ == "__main__":
    main()
