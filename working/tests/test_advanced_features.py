"""
é«˜çº§ç‰¹å¾å·¥ç¨‹æµ‹è¯•
æµ‹è¯•æ–°æ·»åŠ çš„å¢å¼ºæŠ€æœ¯æŒ‡æ ‡ã€åˆ†å±‚ç»Ÿè®¡ã€å®è§‚äº¤äº’å’Œæ•°æ®è´¨é‡åŠŸèƒ½
"""

import numpy as np
import pandas as pd
import pytest
import sys
import os
from unittest.mock import Mock

# æ·»åŠ workingç›®å½•åˆ°è·¯å¾„
working_dir = os.path.dirname(os.path.dirname(__file__))
if working_dir not in sys.path:
    sys.path.insert(0, working_dir)

try:
    from lib.features import FeaturePipeline
    from lib.data import load_train_data, load_test_data
    from lib.env import detect_run_environment, get_data_paths
except ImportError as e:
    print(f"å¯¼å…¥é”™è¯¯: {e}")
    print(f"å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")
    print(f"Pythonè·¯å¾„: {sys.path}")
    raise


class TestAdvancedFeaturePipeline:
    """æµ‹è¯•å¢å¼ºåçš„FeaturePipelineé«˜çº§åŠŸèƒ½"""

    def setup_method(self):
        """è®¾ç½®æµ‹è¯•æ•°æ®"""
        np.random.seed(42)
        dates = pd.date_range('2020-01-01', periods=200, freq='D')
        
        # åˆ›å»ºæ›´çœŸå®çš„æµ‹è¯•æ•°æ®
        self.train_data = pd.DataFrame({
            'date_id': range(200),
            'M1': np.random.randn(200) * 0.02,
            'M2': np.random.randn(200) * 0.02,
            'M3': np.random.randn(200) * 0.02,
            'M4': np.random.randn(200) * 0.02,
            'M5': np.random.randn(200) * 0.02,
            'P1': 100 + np.cumsum(np.random.randn(200) * 0.5),  # ä»·æ ¼åºåˆ—
            'P2': 100 + np.cumsum(np.random.randn(200) * 0.5),
            'P3': 100 + np.cumsum(np.random.randn(200) * 0.5),
            'V1': np.random.exponential(0.02, 200),  # æ³¢åŠ¨ç‡
            'V2': np.random.exponential(0.02, 200),
            'E1': np.random.randn(200) * 0.01,
            'E2': np.random.randn(200) * 0.01,
            'E3': np.random.randn(200) * 0.01,
            'S1': np.random.randn(200) * 0.5,
            'S2': np.random.randn(200) * 0.5,
            'I1': np.random.uniform(0.01, 0.05, 200),  # åˆ©ç‡
            'MOM1': np.random.randn(200) * 0.01,
            'MOM2': np.random.randn(200) * 0.01,
            'lagged_forward_returns': np.random.randn(200) * 0.01,
            'lagged_risk_free_rate': np.random.uniform(0.01, 0.05, 200),
            'lagged_market_forward_excess_returns': np.random.randn(200) * 0.01,
            'forward_returns': np.random.randn(200) * 0.01,
            'risk_free_rate': np.random.uniform(0.01, 0.05, 200),
            'market_forward_excess_returns': np.random.randn(200) * 0.01,
        })
        
        # æ·»åŠ ä¸€äº›ç¼ºå¤±å€¼å’Œå¼‚å¸¸å€¼
        self.train_data.loc[10:15, 'M1'] = np.nan
        self.train_data.loc[30:35, 'P1'] = np.inf
        self.train_data.loc[50:55, 'V1'] = -1.0  # è´Ÿæ³¢åŠ¨ç‡
        
        self.test_data = self.train_data.head(20).copy()

    def test_advanced_technical_indicators(self):
        """æµ‹è¯•é«˜çº§æŠ€æœ¯æŒ‡æ ‡"""
        pipeline = FeaturePipeline(
            extra_group_stats=True,
            enable_data_quality=True,
            enable_feature_stability=True
        )
        
        features = pipeline.fit_transform(self.train_data)
        
        # æ£€æŸ¥Williams %RæŒ‡æ ‡
        williams_features = [col for col in features.columns if 'williams_r_' in col]
        assert len(williams_features) > 0, "Williams %RæŒ‡æ ‡æœªç”Ÿæˆ"
        
        # æ£€æŸ¥StochasticæŒ‡æ ‡
        stoch_features = [col for col in features.columns if 'stoch_' in col]
        assert len(stoch_features) > 0, "StochasticæŒ‡æ ‡æœªç”Ÿæˆ"
        
        # æ£€æŸ¥ADXæŒ‡æ ‡
        adx_features = [col for col in features.columns if 'adx_' in col]
        assert len(adx_features) > 0, "ADXæŒ‡æ ‡æœªç”Ÿæˆ"
        
        # æ£€æŸ¥å¤šæœŸRSI
        rsi_features = [col for col in features.columns if 'rsi_' in col]
        assert len(rsi_features) >= 2, "å¤šæœŸRSIæŒ‡æ ‡æœªç”Ÿæˆ"
        
        # éªŒè¯æŠ€æœ¯æŒ‡æ ‡å€¼åœ¨åˆç†èŒƒå›´å†…
        if 'rsi_14' in features.columns:
            rsi_values = features['rsi_14']
            assert rsi_values.min() >= 0, f"RSIæœ€å°å€¼è¶…å‡ºèŒƒå›´: {rsi_values.min()}"
            assert rsi_values.max() <= 100, f"RSIæœ€å¤§å€¼è¶…å‡ºèŒƒå›´: {rsi_values.max()}"
        
        if 'williams_r_14' in features.columns:
            williams_values = features['williams_r_14']
            assert williams_values.min() >= -100, f"Williams %Ræœ€å°å€¼è¶…å‡ºèŒƒå›´: {williams_values.min()}"
            assert williams_values.max() <= 0, f"Williams %Ræœ€å¤§å€¼è¶…å‡ºèŒƒå›´: {williams_values.max()}"

    def test_tiered_statistics(self):
        """æµ‹è¯•åˆ†å±‚ç»Ÿè®¡ç‰¹å¾"""
        pipeline = FeaturePipeline(extra_group_stats=True)
        features = pipeline.fit_transform(self.train_data)
        
        # æ£€æŸ¥åˆ†å±‚ç»Ÿè®¡ç‰¹å¾
        tiered_features = [col for col in features.columns if any(suffix in col for suffix in ['_mean_low_vol', '_std_high_vol', '_mean_strong_trend'])]
        assert len(tiered_features) > 0, "åˆ†å±‚ç»Ÿè®¡ç‰¹å¾æœªç”Ÿæˆ"
        
        # éªŒè¯åˆ†å±‚ç‰¹å¾å€¼åˆç†
        for col in tiered_features[:5]:  # æ£€æŸ¥å‰5ä¸ªåˆ†å±‚ç‰¹å¾
            values = features[col]
            assert values.notnull().sum() > 0, f"åˆ†å±‚ç‰¹å¾ {col} å…¨éƒ¨ä¸ºç©º"
            assert np.isfinite(values).all(), f"åˆ†å±‚ç‰¹å¾ {col} åŒ…å«éæœ‰é™å€¼"

    def test_macro_factor_interactions(self):
        """æµ‹è¯•å®è§‚å› å­äº¤äº’ç‰¹å¾"""
        pipeline = FeaturePipeline(extra_group_stats=True)
        features = pipeline.fit_transform(self.train_data)
        
        # æ£€æŸ¥åˆ©ç‡-å¸‚åœºäº¤äº’
        rate_features = [col for col in features.columns if 'rate_adjusted' in col]
        assert len(rate_features) > 0, "åˆ©ç‡-å¸‚åœºäº¤äº’ç‰¹å¾æœªç”Ÿæˆ"
        
        # æ£€æŸ¥æ³¢åŠ¨ç‡-åŠ¨é‡äº¤äº’
        vol_momentum_features = [col for col in features.columns if 'vol_weighted_momentum' in col]
        assert len(vol_momentum_features) > 0, "æ³¢åŠ¨ç‡-åŠ¨é‡äº¤äº’ç‰¹å¾æœªç”Ÿæˆ"
        
        # æ£€æŸ¥å®è§‚ç»æµå¼ºåº¦æŒ‡æ ‡
        econ_features = [col for col in features.columns if 'economic_' in col]
        assert len(econ_features) > 0, "å®è§‚ç»æµæŒ‡æ ‡æœªç”Ÿæˆ"

    def test_data_quality_analysis(self):
        """æµ‹è¯•æ•°æ®è´¨é‡åˆ†æåŠŸèƒ½"""
        pipeline = FeaturePipeline(
            enable_data_quality=True,
            outlier_detection=True,
            missing_value_strategy="median"
        )
        
        features = pipeline.fit_transform(self.train_data)
        
        # æ£€æŸ¥æ•°æ®è´¨é‡æŒ‡æ ‡
        assert hasattr(pipeline, 'data_quality_metrics'), "æ•°æ®è´¨é‡æŒ‡æ ‡æœªè®¡ç®—"
        assert hasattr(pipeline, 'feature_stability_scores'), "ç‰¹å¾ç¨³å®šæ€§å¾—åˆ†æœªè®¡ç®—"
        assert hasattr(pipeline, 'outlier_bounds'), "å¼‚å¸¸å€¼è¾¹ç•Œæœªè®¾ç½®"
        
        # éªŒè¯æ•°æ®è´¨é‡æŠ¥å‘Š
        quality_report = pipeline.get_data_quality_report()
        assert 'quality_metrics' in quality_report, "æ•°æ®è´¨é‡æŠ¥å‘Šç¼ºå°‘è´¨é‡æŒ‡æ ‡"
        assert 'stability_scores' in quality_report, "æ•°æ®è´¨é‡æŠ¥å‘Šç¼ºå°‘ç¨³å®šæ€§å¾—åˆ†"
        
        # æ£€æŸ¥ç¨³å®šç‰¹å¾åŠŸèƒ½
        stable_features = pipeline.get_stable_features(threshold=0.1)
        assert isinstance(stable_features, list), "ç¨³å®šç‰¹å¾åº”è¿”å›åˆ—è¡¨"
        
        risky_features = pipeline.get_risky_features(threshold=0.1)
        assert isinstance(risky_features, list), "é£é™©ç‰¹å¾åº”è¿”å›åˆ—è¡¨"

    def test_outlier_detection_and_handling(self):
        """æµ‹è¯•å¼‚å¸¸å€¼æ£€æµ‹å’Œå¤„ç†"""
        pipeline = FeaturePipeline(
            outlier_detection=True,
            clip_quantile=0.01
        )
        
        features = pipeline.fit_transform(self.train_data)
        
        # æ£€æŸ¥å¼‚å¸¸å€¼è¾¹ç•Œ
        assert len(pipeline.outlier_bounds) > 0, "å¼‚å¸¸å€¼è¾¹ç•Œæœªè®¾ç½®"
        
        # éªŒè¯è¾¹ç•Œåˆç†æ€§
        for col, (lower, upper) in pipeline.outlier_bounds.items():
            assert lower < upper, f"å¼‚å¸¸å€¼è¾¹ç•Œä¸‹ç•Œå¤§äºç­‰äºä¸Šç•Œ: {col}"
            assert np.isfinite(lower) and np.isfinite(upper), f"å¼‚å¸¸å€¼è¾¹ç•ŒåŒ…å«éæœ‰é™å€¼: {col}"
        
        # æ£€æŸ¥å¤„ç†åçš„ç‰¹å¾æ˜¯å¦ä»æœ‰å¼‚å¸¸å€¼
        for col in features.select_dtypes(include=[np.number]).columns[:10]:
            if col in pipeline.outlier_bounds:
                lower, upper = pipeline.outlier_bounds[col]
                assert features[col].min() >= lower, f"ç‰¹å¾ {col} ä»æœ‰ä½äºè¾¹ç•Œçš„å€¼"
                assert features[col].max() <= upper, f"ç‰¹å¾ {col} ä»æœ‰é«˜äºè¾¹ç•Œçš„å€¼"

    def test_smart_missing_value_handling(self):
        """æµ‹è¯•æ™ºèƒ½ç¼ºå¤±å€¼å¤„ç†"""
        # åˆ›å»ºåŒ…å«å¤§é‡ç¼ºå¤±å€¼çš„æ•°æ®
        data_with_missing = self.train_data.copy()
        data_with_missing.loc[20:40, ['M1', 'M2', 'P1']] = np.nan
        data_with_missing.loc[50:60, 'V1'] = np.nan
        
        for strategy in ['median', 'mean', 'ffill', 'bfill']:
            pipeline = FeaturePipeline(
                missing_value_strategy=strategy,
                enable_data_quality=True
            )
            
            features = pipeline.fit_transform(data_with_missing)
            
            # éªŒè¯ç¼ºå¤±å€¼å·²å¤„ç†
            for col in ['M1', 'M2', 'P1', 'V1']:
                if col in features.columns:
                    null_count = features[col].isnull().sum()
                    assert null_count == 0, f"ä½¿ç”¨ç­–ç•¥ {strategy} åç‰¹å¾ {col} ä»æœ‰ç¼ºå¤±å€¼: {null_count}"

    def test_enhanced_feature_space(self):
        """æµ‹è¯•å¢å¼ºç‰¹å¾ç©ºé—´"""
        # åŸºç¡€ç®¡é“
        basic_pipeline = FeaturePipeline(
            extra_group_stats=False,
            enable_data_quality=False
        )
        
        # å¢å¼ºç®¡é“
        enhanced_pipeline = FeaturePipeline(
            extra_group_stats=True,
            enable_data_quality=True,
            enable_feature_stability=True,
            outlier_detection=True
        )
        
        basic_features = basic_pipeline.fit_transform(self.train_data)
        enhanced_features = enhanced_pipeline.fit_transform(self.train_data)
        
        # å¢å¼ºç‰ˆæœ¬åº”è¯¥äº§ç”Ÿæ›´å¤šç‰¹å¾
        assert enhanced_features.shape[1] > basic_features.shape[1], \
            f"å¢å¼ºç‰¹å¾æœªå¢åŠ : åŸºç¡€{basic_features.shape[1]} vs å¢å¼º{enhanced_features.shape[1]}"
        
        # æ£€æŸ¥ç‰¹å¾å‘½åçš„ä¸€è‡´æ€§
        enhanced_feature_names = set(enhanced_features.columns)
        
        # æ£€æŸ¥ä¸€äº›é¢„æœŸçš„é«˜çº§ç‰¹å¾
        expected_patterns = [
            'rsi_', 'williams_r_', 'stoch_', 'adx_', 
            'macd_', 'bollinger_', 'tier_', 'economic_',
            'rate_adjusted', 'vol_weighted'
        ]
        
        found_patterns = 0
        for pattern in expected_patterns:
            matching_features = [col for col in enhanced_features.columns if pattern in col]
            if matching_features:
                found_patterns += 1
        
        assert found_patterns >= 5, f"é«˜çº§ç‰¹å¾æ¨¡å¼åŒ¹é…ä¸è¶³: {found_patterns}/{len(expected_patterns)}"

    def test_pipeline_config_serialization(self):
        """æµ‹è¯•ç®¡é“é…ç½®åºåˆ—åŒ–"""
        pipeline = FeaturePipeline(
            extra_group_stats=True,
            enable_data_quality=True,
            enable_feature_stability=True,
            outlier_detection=True,
            missing_value_strategy="median"
        )
        
        # è·å–é…ç½®
        config = pipeline.to_config()
        
        # éªŒè¯é…ç½®åŒ…å«æ‰€æœ‰å­—æ®µ
        assert 'enable_data_quality' in config, "é…ç½®ç¼ºå°‘æ•°æ®è´¨é‡å¼€å…³"
        assert 'enable_feature_stability' in config, "é…ç½®ç¼ºå°‘ç‰¹å¾ç¨³å®šæ€§å¼€å…³"
        assert 'outlier_detection' in config, "é…ç½®ç¼ºå°‘å¼‚å¸¸å€¼æ£€æµ‹å¼€å…³"
        assert 'missing_value_strategy' in config, "é…ç½®ç¼ºå°‘ç¼ºå¤±å€¼å¤„ç†ç­–ç•¥"
        
        # ä»é…ç½®é‡æ–°åˆ›å»ºç®¡é“
        new_pipeline = FeaturePipeline.from_config(config)
        
        # éªŒè¯é…ç½®ä¸€è‡´æ€§
        assert new_pipeline.enable_data_quality == pipeline.enable_data_quality
        assert new_pipeline.enable_feature_stability == pipeline.enable_feature_stability
        assert new_pipeline.outlier_detection == pipeline.outlier_detection
        assert new_pipeline.missing_value_strategy == pipeline.missing_value_strategy


if __name__ == "__main__":
    # è¿è¡Œé«˜çº§ç‰¹å¾å·¥ç¨‹æµ‹è¯•
    test_instance = TestAdvancedFeaturePipeline()
    test_instance.setup_method()
    
    print("è¿è¡Œé«˜çº§ç‰¹å¾å·¥ç¨‹æµ‹è¯•...")
    test_instance.test_advanced_technical_indicators()
    print("âœ… é«˜çº§æŠ€æœ¯æŒ‡æ ‡æµ‹è¯•é€šè¿‡")
    
    test_instance.test_tiered_statistics()
    print("âœ… åˆ†å±‚ç»Ÿè®¡ç‰¹å¾æµ‹è¯•é€šè¿‡")
    
    test_instance.test_macro_factor_interactions()
    print("âœ… å®è§‚å› å­äº¤äº’æµ‹è¯•é€šè¿‡")
    
    test_instance.test_data_quality_analysis()
    print("âœ… æ•°æ®è´¨é‡åˆ†ææµ‹è¯•é€šè¿‡")
    
    test_instance.test_outlier_detection_and_handling()
    print("âœ… å¼‚å¸¸å€¼æ£€æµ‹å’Œå¤„ç†æµ‹è¯•é€šè¿‡")
    
    test_instance.test_smart_missing_value_handling()
    print("âœ… æ™ºèƒ½ç¼ºå¤±å€¼å¤„ç†æµ‹è¯•é€šè¿‡")
    
    test_instance.test_enhanced_feature_space()
    print("âœ… å¢å¼ºç‰¹å¾ç©ºé—´æµ‹è¯•é€šè¿‡")
    
    test_instance.test_pipeline_config_serialization()
    print("âœ… ç®¡é“é…ç½®åºåˆ—åŒ–æµ‹è¯•é€šè¿‡")
    
    print("\nğŸ‰ æ‰€æœ‰é«˜çº§ç‰¹å¾å·¥ç¨‹æµ‹è¯•é€šè¿‡ï¼")
