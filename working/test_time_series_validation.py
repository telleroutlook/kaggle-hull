æ—¶é—´åºåˆ—éªŒè¯ç³»ç»Ÿå®Œæ•´æµ‹è¯•å¥—ä»¶

æµ‹è¯•è¦†ç›–:
- åŸºç¡€åŠŸèƒ½æµ‹è¯•
- éªŒè¯ç­–ç•¥æµ‹è¯•
- é‡‘èæŒ‡æ ‡è®¡ç®—æµ‹è¯•
- é›†æˆåŠŸèƒ½æµ‹è¯•
- æ€§èƒ½åŸºå‡†æµ‹è¯•
- è¾¹ç•Œæ¡ä»¶æµ‹è¯•

import pytest
import numpy as np
import pandas as pd
from pathlib import Path
import tempfile
import warnings
from typing import Dict, List, Any
import logging

# æµ‹è¯•è®¾ç½®
warnings.filterwarnings("ignore")
logging.basicConfig(level=logging.INFO)

# å¯¼å…¥æµ‹è¯•æ¨¡å—
from time_series_validation import (
    TimeSeriesCrossValidator, ValidationConfig, ValidationResult,
    ValidationStrategy, MarketRegime, FinanceValidationMetrics,
    TemporalSplitter, MarketRegimeDetector, create_time_series_validator
)

from time_series_validation_integration import (
    IntegratedTimeSeriesValidator, TimeSeriesValidationAPI,
    validate_with_time_series_cv, comprehensive_model_validation
)

# å¯¼å…¥ç°æœ‰ç³»ç»Ÿ
try:
    from lib.models import HullModel, create_baseline_model
    from lib.features import get_feature_columns
    SYSTEM_AVAILABLE = True
except ImportError:
    SYSTEM_AVAILABLE = False
    print("âš ï¸ ç°æœ‰ç³»ç»Ÿæ¨¡å—ä¸å¯ç”¨ï¼Œéƒ¨åˆ†æµ‹è¯•å°†è·³è¿‡")


class TestTimeSeriesValidator:
    """æ—¶é—´åºåˆ—éªŒè¯å™¨åŸºç¡€åŠŸèƒ½æµ‹è¯•"""
    
    @pytest.fixture
    def sample_data(self):
        """ç”Ÿæˆæµ‹è¯•æ•°æ®"""
        np.random.seed(42)
        n_samples = 1000
        
        # ç”Ÿæˆæ—¶é—´åºåˆ—æ•°æ®
        dates = pd.date_range('2020-01-01', periods=n_samples, freq='D')
        
        # ç”ŸæˆåŸºç¡€ç‰¹å¾
        data = {
            'date_id': [int(d.strftime('%Y%m%d')) for d in dates],
            'M1': np.random.normal(0, 0.02, n_samples),  # å¸‚åœºåŠ¨é‡
            'M2': np.random.normal(0, 0.01, n_samples),
            'P1': 100 + np.cumsum(np.random.normal(0, 0.01, n_samples)),  # ä»·æ ¼åºåˆ—
            'V1': np.random.gamma(2, 0.01, n_samples),  # æ³¢åŠ¨ç‡
            'E1': np.random.normal(0, 0.005, n_samples),  # å®è§‚ç»æµ
            'S1': np.random.normal(0, 0.01, n_samples),  # æƒ…ç»ª
            'forward_returns': np.random.normal(0, 0.02, n_samples)  # ç›®æ ‡å˜é‡
        }
        
        return pd.DataFrame(data)
    
    @pytest.fixture
    def simple_model(self):
        """åˆ›å»ºç®€å•æµ‹è¯•æ¨¡å‹"""
        if SYSTEM_AVAILABLE:
            return HullModel(model_type="baseline", model_params={'n_estimators': 10})
        else:
            # åˆ›å»ºç®€å•å›å½’æ¨¡å‹
            class SimpleModel:
                def __init__(self):
                    self.coef_ = None
                
                def fit(self, X, y):
                    X_with_intercept = np.column_stack([np.ones(X.shape[0]), X])
                    self.coef_ = np.linalg.lstsq(X_with_intercept, y, rcond=None)[0]
                    return self
                
                def predict(self, X):
                    X_with_intercept = np.column_stack([np.ones(X.shape[0]), X])
                    return X_with_intercept @ self.coef_
            return SimpleModel()
    
    def test_basic_validation(self, simple_model, sample_data):
        """æµ‹è¯•åŸºç¡€éªŒè¯åŠŸèƒ½"""
        # å‡†å¤‡æ•°æ®
        feature_cols = ['M1', 'M2', 'V1', 'E1', 'S1']
        X = sample_data[feature_cols]
        y = sample_data['forward_returns']
        
        # åˆ›å»ºéªŒè¯å™¨
        validator = create_time_series_validator('time_series_split', n_splits=3)
        
        # æ‰§è¡ŒéªŒè¯
        result = validator.validate(simple_model, X, y)
        
        # éªŒè¯ç»“æœ
        assert isinstance(result, ValidationResult)
        assert result.strategy == ValidationStrategy.TIME_SERIES_SPLIT
        assert result.n_splits == 3
        assert len(result.metrics) > 0
        assert 'mse' in result.metrics
        assert len(result.metrics['mse']) == 3
        
        print("âœ… åŸºç¡€éªŒè¯æµ‹è¯•é€šè¿‡")
    
    def test_all_strategies(self, simple_model, sample_data):
        """æµ‹è¯•æ‰€æœ‰éªŒè¯ç­–ç•¥"""
        feature_cols = ['M1', 'M2', 'V1', 'E1', 'S1']
        X = sample_data[feature_cols]
        y = sample_data['forward_returns']
        
        strategies = [
            ValidationStrategy.TIME_SERIES_SPLIT,
            ValidationStrategy.EXPANDING_WINDOW,
            ValidationStrategy.ROLLING_WINDOW,
            ValidationStrategy.PURGED_TIME_SERIES,
            ValidationStrategy.VOLATILITY_TIERED
        ]
        
        for strategy in strategies:
            try:
                config = ValidationConfig(strategy=strategy, n_splits=3)
                validator = TimeSeriesCrossValidator(config)
                result = validator.validate(simple_model, X, y)
                
                assert result.strategy == strategy
                assert result.n_splits == 3
                assert len(result.metrics) > 0
                
                print(f"âœ… ç­–ç•¥ {strategy.value} æµ‹è¯•é€šè¿‡")
                
            except Exception as e:
                print(f"âš ï¸ ç­–ç•¥ {strategy.value} æµ‹è¯•å¤±è´¥: {e}")
                # æŸäº›ç­–ç•¥å¯èƒ½åœ¨æ•°æ®ä¸è¶³æ—¶å¤±è´¥ï¼Œè¿™æ˜¯æ­£å¸¸çš„
    
    def test_market_regime_detection(self, sample_data):
        """æµ‹è¯•å¸‚åœºçŠ¶æ€æ£€æµ‹"""
        detector = MarketRegimeDetector()
        
        # æ¨¡æ‹Ÿä»·æ ¼åºåˆ—
        prices = sample_data['P1'].values
        returns = np.diff(prices) / prices[:-1]
        volatility = np.abs(returns)
        
        # æ£€æµ‹å¸‚åœºçŠ¶æ€
        regime = detector.detect_regime(prices, returns, volatility)
        
        assert isinstance(regime, MarketRegime)
        assert regime != MarketRegime.UNKNOWN
        
        # æµ‹è¯•ç¨³å®šæ€§
        stability = detector.get_regime_stability()
        assert 0.0 <= stability <= 1.0
        
        print("âœ… å¸‚åœºçŠ¶æ€æ£€æµ‹æµ‹è¯•é€šè¿‡")
    
    def test_finance_metrics(self):
        """æµ‹è¯•é‡‘èæŒ‡æ ‡è®¡ç®—"""
        calculator = FinanceValidationMetrics()
        
        # ç”Ÿæˆæµ‹è¯•æ•°æ®
        np.random.seed(42)
        y_true = np.random.normal(0, 0.02, 100)
        y_pred = y_true + np.random.normal(0, 0.01, 100)
        benchmark = np.random.normal(0, 0.015, 99)
        
        # è®¡ç®—æŒ‡æ ‡
        metrics = calculator.calculate_metrics(
            y_true, y_pred, 
            benchmark_returns=benchmark,
            market_returns=benchmark
        )
        
        # éªŒè¯æŒ‡æ ‡
        assert 'mse' in metrics
        assert 'mae' in metrics
        assert 'strategy_sharpe' in metrics
        assert 'directional_accuracy' in metrics
        assert isinstance(metrics['mse'], float)
        assert isinstance(metrics['strategy_sharpe'], float)
        
        print("âœ… é‡‘èæŒ‡æ ‡è®¡ç®—æµ‹è¯•é€šè¿‡")
    
    def test_purged_time_series(self, simple_model, sample_data):
        """æµ‹è¯•æ¸…ç†æ—¶é—´åºåˆ—åˆ†å‰²"""
        feature_cols = ['M1', 'M2', 'V1', 'E1', 'S1']
        X = sample_data[feature_cols]
        y = sample_data['forward_returns']
        
        config = ValidationConfig(
            strategy=ValidationStrategy.PURGED_TIME_SPLIT,
            n_splits=3,
            embargo_percentage=0.1
        )
        
        validator = TimeSeriesCrossValidator(config)
        result = validator.validate(simple_model, X, y)
        
        assert result.strategy == ValidationStrategy.PURGED_TIME_SPLIT
        assert len(result.train_indices) == 3
        assert len(result.test_indices) == 3
        
        print("âœ… æ¸…ç†æ—¶é—´åºåˆ—æµ‹è¯•é€šè¿‡")


class TestIntegratedValidator:
    """é›†æˆéªŒè¯å™¨æµ‹è¯•"""
    
    @pytest.fixture
    def sample_data_large(self):
        """ç”Ÿæˆå¤§è§„æ¨¡æµ‹è¯•æ•°æ®"""
        np.random.seed(42)
        n_samples = 2000
        
        dates = pd.date_range('2018-01-01', periods=n_samples, freq='D')
        
        # æ›´å¤æ‚çš„æ•°æ®ç”Ÿæˆ
        base_trend = np.linspace(0, 0.1, n_samples)
        seasonal = 0.02 * np.sin(2 * np.pi * np.arange(n_samples) / 252)
        noise = np.random.normal(0, 0.01, n_samples)
        
        data = {
            'date_id': [int(d.strftime('%Y%m%d')) for d in dates],
            'M1': base_trend + seasonal + noise,
            'M2': np.random.normal(0, 0.01, n_samples),
            'P1': 100 + np.cumsum(0.001 + 0.01 * np.random.randn(n_samples)),
            'V1': 0.01 + 0.02 * np.abs(np.random.randn(n_samples)),
            'E1': np.random.normal(0, 0.005, n_samples),
            'S1': np.random.normal(0, 0.01, n_samples),
            'forward_returns': 0.001 + 0.02 * np.random.randn(n_samples)
        }
        
        return pd.DataFrame(data)
    
    def test_integrated_validation(self, sample_data_large):
        """æµ‹è¯•é›†æˆéªŒè¯åŠŸèƒ½"""
        if not SYSTEM_AVAILABLE:
            pytest.skip("ç°æœ‰ç³»ç»Ÿä¸å¯ç”¨")
        
        validator = IntegratedTimeSeriesValidator()
        
        # æ‰§è¡Œé›†æˆéªŒè¯
        results = validator.validate_hull_model(
            model_type="lightgbm",
            train_data=sample_data_large,
            target_column="forward_returns"
        )
        
        assert isinstance(results, dict)
        assert len(results) > 0
        
        # éªŒè¯ç»“æœç»“æ„
        for strategy, result in results.items():
            assert isinstance(result, ValidationResult)
            assert result.strategy is not None
            assert len(result.metrics) > 0
        
        print("âœ… é›†æˆéªŒè¯æµ‹è¯•é€šè¿‡")
    
    def test_ensemble_validation(self, sample_data_large):
        """æµ‹è¯•é›†æˆæ¨¡å‹éªŒè¯"""
        if not SYSTEM_AVAILABLE:
            pytest.skip("ç°æœ‰ç³»ç»Ÿä¸å¯ç”¨")
        
        validator = IntegratedTimeSeriesValidator()
        
        # æµ‹è¯•ä¸åŒé›†æˆç­–ç•¥
        base_models = ['lightgbm', 'xgboost']
        ensemble_configs = [
            {'type': 'dynamic_weighted', 'config': {'performance_window': 50}},
            {'type': 'averaging', 'config': {'weights': [0.6, 0.4]}}
        ]
        
        results = validator.validate_ensemble_performance(
            base_models=base_models,
            ensemble_configs=ensemble_configs,
            train_data=sample_data_large,
            target_column="forward_returns"
        )
        
        assert isinstance(results, dict)
        assert len(results) > 0
        
        for ensemble_type, ensemble_results in results.items():
            assert isinstance(ensemble_results, dict)
            print(f"âœ… é›†æˆç­–ç•¥ {ensemble_type} æµ‹è¯•é€šè¿‡")
    
    def test_adaptive_validation(self, sample_data_large):
        """æµ‹è¯•è‡ªé€‚åº”éªŒè¯åºåˆ—"""
        if not SYSTEM_AVAILABLE:
            pytest.skip("ç°æœ‰ç³»ç»Ÿä¸å¯ç”¨")
        
        validator = IntegratedTimeSeriesValidator()
        
        # æ‰§è¡Œè‡ªé€‚åº”éªŒè¯
        sequence = validator.adaptive_validation_sequence(
            train_data=sample_data_large,
            target_column="forward_returns",
            max_rounds=2
        )
        
        assert isinstance(sequence, list)
        assert len(sequence) == 2
        
        for round_result in sequence:
            assert 'round_number' in round_result
            assert 'best_strategy' in round_result
            assert 'results' in round_result
        
        print("âœ… è‡ªé€‚åº”éªŒè¯åºåˆ—æµ‹è¯•é€šè¿‡")
    
    def test_comparison_with_baseline(self, sample_data_large):
        """æµ‹è¯•ä¸åŸºçº¿æ¨¡å‹æ¯”è¾ƒ"""
        if not SYSTEM_AVAILABLE:
            pytest.skip("ç°æœ‰ç³»ç»Ÿä¸å¯ç”¨")
        
        validator = IntegratedTimeSeriesValidator()
        
        # åˆ›å»ºæ¨¡å‹
        current_model = HullModel(model_type="lightgbm", 
                                 model_params={'n_estimators': 20})
        baseline_model = HullModel(model_type="baseline")
        
        # å‡†å¤‡æ•°æ®
        feature_cols = get_feature_columns(sample_data_large)
        features = sample_data_large[feature_cols]
        target = sample_data_large['forward_returns']
        
        # æ¯”è¾ƒéªŒè¯
        results = validator.compare_with_baseline(
            current_model, baseline_model, features, target
        )
        
        assert 'current_model' in results
        assert 'baseline_model' in results
        assert 'improvements' in results
        
        print("âœ… åŸºçº¿æ¨¡å‹æ¯”è¾ƒæµ‹è¯•é€šè¿‡")


class TestPerformanceBenchmark:
    """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    
    def test_validation_speed(self):
        """æµ‹è¯•éªŒè¯é€Ÿåº¦"""
        # ç”Ÿæˆå¤§é‡æ•°æ®
        np.random.seed(42)
        n_samples = 5000
        
        dates = pd.date_range('2015-01-01', periods=n_samples, freq='D')
        data = {
            'M1': np.random.normal(0, 0.02, n_samples),
            'M2': np.random.normal(0, 0.01, n_samples),
            'V1': np.random.gamma(2, 0.01, n_samples),
            'forward_returns': np.random.normal(0, 0.02, n_samples)
        }
        sample_data = pd.DataFrame(data)
        
        # åˆ›å»ºç®€å•æ¨¡å‹
        if SYSTEM_AVAILABLE:
            model = HullModel(model_type="baseline", 
                            model_params={'n_estimators': 10})
        else:
            class SimpleModel:
                def fit(self, X, y): return self
                def predict(self, X): return np.zeros(len(X))
            model = SimpleModel()
        
        # æµ‹è¯•ä¸åŒç­–ç•¥çš„é€Ÿåº¦
        strategies = [
            ValidationStrategy.TIME_SERIES_SPLIT,
            ValidationStrategy.EXPANDING_WINDOW,
            ValidationStrategy.ROLLING_WINDOW
        ]
        
        timing_results = {}
        
        for strategy in strategies:
            import time
            
            config = ValidationConfig(strategy=strategy, n_splits=5)
            validator = TimeSeriesCrossValidator(config)
            
            feature_cols = ['M1', 'M2', 'V1']
            X = sample_data[feature_cols]
            y = sample_data['forward_returns']
            
            start_time = time.time()
            result = validator.validate(model, X, y)
            end_time = time.time()
            
            timing_results[strategy.value] = end_time - start_time
        
        # éªŒè¯æ—¶é—´åˆç†æ€§ï¼ˆæ¯ä¸ªç­–ç•¥åº”è¯¥åœ¨åˆç†æ—¶é—´å†…å®Œæˆï¼‰
        for strategy_name, duration in timing_results.items():
            assert duration < 60  # åº”è¯¥åœ¨1åˆ†é’Ÿå†…å®Œæˆ
            print(f"ç­–ç•¥ {strategy_name}: {duration:.2f}ç§’")
        
        print("âœ… éªŒè¯é€Ÿåº¦æµ‹è¯•é€šè¿‡")
    
    def test_memory_usage(self):
        """æµ‹è¯•å†…å­˜ä½¿ç”¨"""
        import psutil
        import os
        
        process = psutil.Process(os.getpid())
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        # ç”Ÿæˆä¸­ç­‰è§„æ¨¡æ•°æ®
        np.random.seed(42)
        n_samples = 2000
        n_features = 50
        
        # ç”Ÿæˆç‰¹å¾
        features = {}
        for i in range(n_features):
            features[f'feature_{i}'] = np.random.normal(0, 1, n_samples)
        
        features['forward_returns'] = np.random.normal(0, 0.02, n_samples)
        sample_data = pd.DataFrame(features)
        
        # åˆ›å»ºæ¨¡å‹
        if SYSTEM_AVAILABLE:
            model = HullModel(model_type="baseline", 
                            model_params={'n_estimators': 5})
        else:
            class SimpleModel:
                def fit(self, X, y): return self
                def predict(self, X): return np.zeros(len(X))
            model = SimpleModel()
        
        # æ‰§è¡ŒéªŒè¯
        validator = create_time_series_validator('time_series_split', n_splits=3)
        feature_cols = [f'feature_{i}' for i in range(n_features)]
        X = sample_data[feature_cols]
        y = sample_data['forward_returns']
        
        result = validator.validate(model, X, y)
        
        # æ£€æŸ¥å†…å­˜ä½¿ç”¨
        final_memory = process.memory_info().rss / 1024 / 1024  # MB
        memory_increase = final_memory - initial_memory
        
        # å†…å­˜å¢é•¿åº”è¯¥åœ¨åˆç†èŒƒå›´å†…
        assert memory_increase < 500  # å¢é•¿ä¸è¶…è¿‡500MB
        
        print(f"å†…å­˜ä½¿ç”¨: åˆå§‹ {initial_memory:.1f}MB, æœ€ç»ˆ {final_memory:.1f}MB, å¢é•¿ {memory_increase:.1f}MB")
        print("âœ… å†…å­˜ä½¿ç”¨æµ‹è¯•é€šè¿‡")


class TestEdgeCases:
    """è¾¹ç•Œæ¡ä»¶æµ‹è¯•"""
    
    def test_small_data(self):
        """æµ‹è¯•å°æ•°æ®é›†"""
        # åˆ›å»ºå°æ•°æ®é›†
        np.random.seed(42)
        n_samples = 50
        
        data = {
            'M1': np.random.normal(0, 0.01, n_samples),
            'V1': np.random.gamma(2, 0.01, n_samples),
            'forward_returns': np.random.normal(0, 0.01, n_samples)
        }
        small_data = pd.DataFrame(data)
        
        class SimpleModel:
            def fit(self, X, y): return self
            def predict(self, X): return np.zeros(len(X))
        
        model = SimpleModel()
        
        # æµ‹è¯•
        config = ValidationConfig(
            strategy=ValidationStrategy.TIME_SERIES_SPLIT,
            n_splits=2,
            min_train_samples=20
        )
        
        validator = TimeSeriesCrossValidator(config)
        
        feature_cols = ['M1', 'V1']
        X = small_data[feature_cols]
        y = small_data['forward_returns']
        
        # åº”è¯¥èƒ½å¤Ÿå¤„ç†å°æ•°æ®é›†
        result = validator.validate(model, X, y)
        assert result.n_splits >= 1
        
        print("âœ… å°æ•°æ®é›†æµ‹è¯•é€šè¿‡")
    
    def test_with_nan_values(self):
        """æµ‹è¯•åŒ…å«NaNå€¼çš„æ•°æ®"""
        np.random.seed(42)
        n_samples = 200
        
        data = {
            'M1': np.random.normal(0, 0.01, n_samples),
            'M2': np.random.normal(0, 0.01, n_samples),
            'V1': np.random.gamma(2, 0.01, n_samples),
            'forward_returns': np.random.normal(0, 0.01, n_samples)
        }
        
        # æ·»åŠ NaNå€¼
        data['M1'][10:20] = np.nan
        data['V1'][30:35] = np.nan
        
        nan_data = pd.DataFrame(data)
        
        class SimpleModel:
            def fit(self, X, y): 
                # ç®€å•å¤„ç†NaNå€¼
                X_filled = X.fillna(0)
                return self
            def predict(self, X): 
                X_filled = X.fillna(0)
                return np.zeros(len(X_filled))
        
        model = SimpleModel()
        
        # æµ‹è¯•
        config = ValidationConfig(strategy=ValidationStrategy.TIME_SERIES_SPLIT, n_splits=3)
        validator = TimeSeriesCrossValidator(config)
        
        feature_cols = ['M1', 'M2', 'V1']
        X = nan_data[feature_cols]
        y = nan_data['forward_returns']
        
        result = validator.validate(model, X, y)
        assert result.n_splits == 3
        
        print("âœ… NaNå€¼å¤„ç†æµ‹è¯•é€šè¿‡")
    
    def test_constant_features(self):
        """æµ‹è¯•å¸¸å€¼ç‰¹å¾"""
        np.random.seed(42)
        n_samples = 100
        
        data = {
            'constant_feature': np.ones(n_samples),  # å¸¸å€¼ç‰¹å¾
            'varying_feature': np.random.normal(0, 0.01, n_samples),
            'forward_returns': np.random.normal(0, 0.01, n_samples)
        }
        
        constant_data = pd.DataFrame(data)
        
        class SimpleModel:
            def fit(self, X, y): return self
            def predict(self, X): return np.zeros(len(X))
        
        model = SimpleModel()
        
        # æµ‹è¯•
        config = ValidationConfig(strategy=ValidationStrategy.TIME_SERIES_SPLIT, n_splits=3)
        validator = TimeSeriesCrossValidator(config)
        
        feature_cols = ['constant_feature', 'varying_feature']
        X = constant_data[feature_cols]
        y = constant_data['forward_returns']
        
        result = validator.validate(model, X, y)
        assert result.n_splits == 3
        
        print("âœ… å¸¸å€¼ç‰¹å¾æµ‹è¯•é€šè¿‡")


class TestAPIFunctions:
    """APIåŠŸèƒ½æµ‹è¯•"""
    
    def test_quick_validate_api(self, sample_data):
        """æµ‹è¯•å¿«é€ŸéªŒè¯API"""
        if not SYSTEM_AVAILABLE:
            pytest.skip("ç°æœ‰ç³»ç»Ÿä¸å¯ç”¨")
        
        # åˆ›å»ºæ¨¡å‹
        model = HullModel(model_type="baseline", 
                         model_params={'n_estimators': 5})
        
        # ä½¿ç”¨API
        result = validate_with_time_series_cv(
            model=model,
            train_data=sample_data,
            strategy="expanding_window"
        )
        
        assert isinstance(result, ValidationResult)
        assert result.strategy == ValidationStrategy.EXPANDING_WINDOW
        
        print("âœ… å¿«é€ŸéªŒè¯APIæµ‹è¯•é€šè¿‡")
    
    def test_comprehensive_validation_api(self, sample_data):
        """æµ‹è¯•å…¨é¢éªŒè¯API"""
        if not SYSTEM_AVAILABLE:
            pytest.skip("ç°æœ‰ç³»ç»Ÿä¸å¯ç”¨")
        
        # ä½¿ç”¨å…¨é¢éªŒè¯API
        results = comprehensive_model_validation(
            train_data=sample_data,
            model_types=['baseline'],
            strategies=['time_series_split', 'expanding_window'],
            save_results=False
        )
        
        assert isinstance(results, dict)
        assert 'baseline' in results
        assert 'time_series_split' in results['baseline']
        assert 'expanding_window' in results['baseline']
        
        print("âœ… å…¨é¢éªŒè¯APIæµ‹è¯•é€šè¿‡")


def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("ğŸ§ª å¼€å§‹æ—¶é—´åºåˆ—éªŒè¯ç³»ç»Ÿæµ‹è¯•...")
    
    # æµ‹è¯•ç±»
    test_classes = [
        TestTimeSeriesValidator,
        TestIntegratedValidator,
        TestPerformanceBenchmark,
        TestEdgeCases,
        TestAPIFunctions
    ]
    
    total_tests = 0
    passed_tests = 0
    
    for test_class in test_classes:
        print(f"\nğŸ“‹ è¿è¡Œ {test_class.__name__} æµ‹è¯•...")
        
        # è·å–æ‰€æœ‰æµ‹è¯•æ–¹æ³•
        test_methods = [method for method in dir(test_class) if method.startswith('test_')]
        
        for method_name in test_methods:
            total_tests += 1
            try:
                test_instance = test_class()
                
                # è®¾ç½®fixture
                if hasattr(test_instance, 'sample_data'):
                    test_instance.sample_data = create_sample_data()
                if hasattr(test_instance, 'simple_model'):
                    test_instance.simple_model = create_simple_model()
                if hasattr(test_instance, 'sample_data_large'):
                    test_instance.sample_data_large = create_large_sample_data()
                
                # è¿è¡Œæµ‹è¯•
                getattr(test_instance, method_name)()
                passed_tests += 1
                print(f"  âœ… {method_name}")
                
            except Exception as e:
                print(f"  âŒ {method_name}: {e}")
    
    print(f"\nğŸ“Š æµ‹è¯•å®Œæˆ: {passed_tests}/{total_tests} é€šè¿‡")
    if passed_tests == total_tests:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡!")
    else:
        print(f"âš ï¸ {total_tests - passed_tests} ä¸ªæµ‹è¯•å¤±è´¥")
    
    return passed_tests, total_tests


def create_sample_data():
    """åˆ›å»ºç¤ºä¾‹æ•°æ®"""
    np.random.seed(42)
    n_samples = 500
    
    dates = pd.date_range('2020-01-01', periods=n_samples, freq='D')
    
    data = {
        'date_id': [int(d.strftime('%Y%m%d')) for d in dates],
        'M1': np.random.normal(0, 0.02, n_samples),
        'M2': np.random.normal(0, 0.01, n_samples),
        'P1': 100 + np.cumsum(np.random.normal(0, 0.01, n_samples)),
        'V1': np.random.gamma(2, 0.01, n_samples),
        'E1': np.random.normal(0, 0.005, n_samples),
        'S1': np.random.normal(0, 0.01, n_samples),
        'forward_returns': np.random.normal(0, 0.02, n_samples)
    }
    
    return pd.DataFrame(data)


def create_simple_model():
    """åˆ›å»ºç®€å•æ¨¡å‹"""
    if SYSTEM_AVAILABLE:
        return HullModel(model_type="baseline", model_params={'n_estimators': 5})
    else:
        class SimpleModel:
            def fit(self, X, y): return self
            def predict(self, X): return np.zeros(len(X))
        return SimpleModel()


def create_large_sample_data():
    """åˆ›å»ºå¤§å‹ç¤ºä¾‹æ•°æ®"""
    np.random.seed(42)
    n_samples = 1500
    
    dates = pd.date_range('2018-01-01', periods=n_samples, freq='D')
    
    data = {
        'date_id': [int(d.strftime('%Y%m%d')) for d in dates],
        'M1': np.random.normal(0, 0.02, n_samples),
        'M2': np.random.normal(0, 0.01, n_samples),
        'P1': 100 + np.cumsum(np.random.normal(0, 0.01, n_samples)),
        'V1': np.random.gamma(2, 0.01, n_samples),
        'E1': np.random.normal(0, 0.005, n_samples),
        'S1': np.random.normal(0, 0.01, n_samples),
        'forward_returns': np.random.normal(0, 0.02, n_samples)
    }
    
    return pd.DataFrame(data)


if __name__ == "__main__":
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    run_all_tests()
