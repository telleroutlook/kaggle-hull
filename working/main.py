#!/usr/bin/env python3
"""
Hull Tactical - Market Prediction ä¸»æ¨¡å‹æ–‡ä»¶
Kaggleç«èµ›çš„æ¨¡å‹å…¥å£ç‚¹
"""

import argparse
import sys
import os
import pandas as pd
from pathlib import Path
from typing import Optional

# æ·»åŠ libç›®å½•åˆ°è·¯å¾„
working_dir = os.path.dirname(__file__)
if working_dir:
    sys.path.insert(0, working_dir)
else:
    # å¦‚æœ__file__ä¸ºç©ºï¼ˆåœ¨æŸäº›ç¯å¢ƒä¸­ï¼‰ï¼Œä½¿ç”¨å½“å‰å·¥ä½œç›®å½•
    sys.path.insert(0, os.getcwd())

import logging

from lib.artifacts import load_first_available_oof, oof_artifact_candidates
from lib.env import detect_run_environment, get_data_paths, get_log_paths
from lib.data import load_test_data, load_train_data, validate_data
from lib.features import FeaturePipeline
from lib.model_registry import DEFAULT_MODEL_TYPE, MODEL_PRESETS, get_model_params, resolve_model_type
from lib.models import HullModel, create_submission
from lib.strategy import (
    VolatilityOverlay,
    optimize_scale_with_rolling_cv,
    scale_to_allocation,
    tune_allocation_scale,
)
from lib.utils import PerformanceTracker, save_logs, save_metrics, validate_submission

# å°è¯•å¯¼å…¥é…ç½®æ¨¡å—
try:
    from lib.config import ConfigManager, get_config
    CONFIG_AVAILABLE = True
except ImportError:
    CONFIG_AVAILABLE = False
    print("âš ï¸ é…ç½®æ¨¡å—ä¸å¯ç”¨")

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def parse_args(argv=None):
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="Hull Tactical - Market Prediction æ¨¡å‹")
    
    parser.add_argument(
        "--model-type",
        choices=sorted(MODEL_PRESETS.keys()),
        default=None,
        help="é€‰æ‹©æ¨¡å‹ç±»å‹ (é»˜è®¤ï¼šä½¿ç”¨HULL_MODEL_TYPEæˆ–lightgbm)"
    )
    parser.add_argument(
        "--allow-missing-oof",
        action="store_true",
        help="å…è®¸åœ¨ç¼ºå¤± OOF artefact æ—¶é€€å›åœ¨çº¿æ ¡å‡†ï¼ˆé»˜è®¤å¼ºåˆ¶éœ€è¦ artefactï¼‰",
    )
    
    parser.add_argument(
        "--data-path",
        type=Path,
        default=None,
        help="æ˜¾å¼æŒ‡å®šæ•°æ®è·¯å¾„ï¼ˆè¦†ç›–è‡ªåŠ¨æ£€æµ‹ï¼‰"
    )
    
    parser.add_argument(
        "--output",
        type=Path,
        default=None,
        help="è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤ï¼šæ ¹æ®ç¯å¢ƒè‡ªåŠ¨è®¾ç½®ï¼‰"
    )
    
    parser.add_argument(
        "--config",
        type=Path,
        default=None,
        help="é…ç½®æ–‡ä»¶è·¯å¾„"
    )
    
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="è¯¦ç»†è¾“å‡ºæ¨¡å¼"
    )
    parser.add_argument(
        "--risk-overlay-lookback",
        type=int,
        default=63,
        help="é£é™©overlayæ»šåŠ¨çª—å£é•¿åº¦",
    )
    parser.add_argument(
        "--risk-overlay-min-periods",
        type=int,
        default=21,
        help="overlayå¼€å§‹ç”Ÿæ•ˆæœ€å°å†å²é•¿åº¦",
    )
    parser.add_argument(
        "--disable-risk-overlay",
        action="store_true",
        help="ç¦ç”¨æ³¢åŠ¨ç‡é£é™©overlay",
    )
    overlay_target_default = os.getenv("HULL_OVERLAY_TARGET_QUANTILE")
    try:
        overlay_target_default = None if overlay_target_default is None else float(overlay_target_default)
    except ValueError:
        overlay_target_default = None

    parser.add_argument(
        "--risk-overlay-target-quantile",
        type=float,
        default=overlay_target_default,
        help="Overlayè‡ªé€‚åº”ç›®æ ‡æ³¢åŠ¨ç‡åˆ†ä½æ•° (0-1)",
    )
    
    # å¤„ç†ä¸åŒçš„è¿è¡Œç¯å¢ƒå‚æ•°
    if argv is None:
        # è¿‡æ»¤æ‰Jupyterå†…æ ¸å‚æ•°å’Œä¸ç›¸å…³çš„å‚æ•°
        filtered_argv = [arg for arg in sys.argv[1:] if not arg.startswith('-f') and not ':memory:' in arg]
        try:
            return parser.parse_args(filtered_argv)
        except SystemExit:
            # å¦‚æœè§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å‚æ•°
            print("âš ï¸ å‚æ•°è§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å‚æ•°")
            return parser.parse_args([])
    
    return parser.parse_args(argv)


def main():
    """ä¸»å‡½æ•° - è¿è¡Œæ¨¡å‹é¢„æµ‹"""
    
    args = parse_args()
    
    # åˆå§‹åŒ–é…ç½®
    if CONFIG_AVAILABLE and args.config:
        config_manager = ConfigManager(str(args.config))
        logging_config = config_manager.get_logging_config()
        # æ›´æ–°æ—¥å¿—é…ç½®
        logging.basicConfig(
            level=getattr(logging, logging_config['level'], logging.INFO),
            format=logging_config['format']
        )
    elif CONFIG_AVAILABLE:
        config_manager = get_config()
    else:
        config_manager = None
    
    tracker = PerformanceTracker(logger=logger)
    
    print("ğŸš€ Hull Tactical - Market Prediction æ¨¡å‹å¯åŠ¨")
    
    # æ£€æµ‹è¿è¡Œç¯å¢ƒ
    env = detect_run_environment()
    data_paths = get_data_paths(env)
    log_paths = get_log_paths(env)
    oof_entry = None
    oof_path = None
    
    print(f"ğŸ  è¿è¡Œç¯å¢ƒ: {env}")
    print(f"ğŸ“ æ•°æ®è·¯å¾„: {data_paths.test_data}")
    
    # è®¾ç½®è¾“å‡ºè·¯å¾„
    if args.output:
        submission_path = args.output
    else:
        submission_path = log_paths.submission_file
    
    try:
        # åŠ è½½æ•°æ®
        tracker.start_task("load_data")
        tracker.record_memory_usage()
        train_data = load_train_data(data_paths)
        test_data = load_test_data(data_paths)
        
        if not validate_data(test_data, "test"):
            return 1
        
        tracker.record_memory_usage()
        tracker.end_task()
        
        # ç‰¹å¾å·¥ç¨‹
        tracker.start_task("feature_engineering")
        tracker.record_memory_usage()
        pipeline = FeaturePipeline()
        train_features = pipeline.fit_transform(train_data)
        test_features = pipeline.transform(test_data)
        
        if args.verbose:
            print(f"ğŸ”§ è®­ç»ƒç‰¹å¾æ•°é‡: {train_features.shape[1]}")
            print(f"ğŸ“Š æ¨ç†ç‰¹å¾å½¢çŠ¶: {test_features.shape}")
        
        tracker.record_memory_usage()
        tracker.end_task()
        
        # æ¨¡å‹é¢„æµ‹
        tracker.start_task("model_prediction")
        tracker.record_memory_usage()
        
        model_type = resolve_model_type(args.model_type)
        if args.model_type is None:
            logger.info(
                "Model type not specified, resolved to '%s' (HULL_MODEL_TYPE=%s, default=%s)",
                model_type,
                os.getenv("HULL_MODEL_TYPE", "<unset>"),
                DEFAULT_MODEL_TYPE,
            )
        else:
            logger.info("Model type explicitly requested: %s", args.model_type)
        model_params = get_model_params(model_type)
        model = HullModel(model_type=model_type, model_params=model_params)

        oof_entry, oof_path = load_first_available_oof(
            model_type, oof_artifact_candidates(log_paths)
        )
        if oof_entry:
            print(
                "ğŸ§ª Latest OOF artefact: "
                f"Sharpe={oof_entry.get('oof_metrics', {}).get('sharpe', float('nan')):.4f}, "
                f"Scale={oof_entry.get('preferred_scale')} (timestamp={oof_entry.get('timestamp')}, "
                f"source={oof_path})"
            )
        elif not args.allow_missing_oof:
            raise RuntimeError(
                "OOF artefact not found. è¯·å…ˆè¿è¡Œ train_experiment.py ç”Ÿæˆ OOF é…ç½®ï¼Œæˆ–ä½¿ç”¨ --allow-missing-oof æ˜ç¡®å…è®¸å›é€€ã€‚"
            )
        target = train_data["forward_returns"].fillna(train_data["forward_returns"].median())
        model.fit(train_features, target)

        allocation_scale: Optional[float] = None
        scale_result: dict[str, float] = {}
        overlay_config = (oof_entry or {}).get("overlay_config") if oof_entry else None

        if oof_entry and oof_entry.get("preferred_scale") is not None:
            allocation_scale = float(oof_entry.get("preferred_scale"))
            print(
                f"â™»ï¸ Using allocation scale {allocation_scale:.2f} from OOF artefact "
                f"recorded at {oof_entry.get('timestamp')}"
            )
        elif args.allow_missing_oof:
            train_preds = model.predict(train_features, clip=False)
            scale_result = optimize_scale_with_rolling_cv(train_preds, target.values)
            allocation_scale = scale_result.get("scale", 20.0)
            if allocation_scale is None:
                tuning = tune_allocation_scale(train_preds, target.values)
                allocation_scale = tuning.get("scale", 20.0)
                scale_result = {
                    "cv_sharpe": tuning.get("strategy_sharpe", 0.0),
                    "strategy_sharpe": tuning.get("strategy_sharpe", 0.0),
                }
            print(
                f"âš ï¸ Falling back to on-the-fly scale calibration (allocation scale={allocation_scale:.2f})."
            )
        else:
            raise RuntimeError(
                "OOF artefact missing preferred_scale. è¯·é‡æ–°è¿è¡Œ train_experiment.py ä»¥è®°å½•æœ€æ–°é…ç½®ã€‚"
            )

        if args.verbose and allocation_scale is not None:
            source = "artefact" if oof_entry else "local_cv"
            print(
                f"ğŸ¯ Allocation scale={allocation_scale:.2f} (source={source}, "
                f"CVSharpe={scale_result.get('cv_sharpe', 0):.4f})"
            )

        raw_test_preds = model.predict(test_features, clip=False)
        predictions = scale_to_allocation(raw_test_preds, scale=allocation_scale)

        tracker.record_memory_usage()
        tracker.end_task()

        overlay = None
        overlay_result = None
        if not args.disable_risk_overlay:
            overlay_source = None
            if "lagged_forward_returns" in test_data.columns:
                overlay_source = test_data["lagged_forward_returns"].to_numpy()
            elif "lagged_market_forward_excess_returns" in test_data.columns:
                overlay_source = test_data["lagged_market_forward_excess_returns"].to_numpy()

            if overlay_source is not None:
                overlay_params = overlay_config or {}
                overlay = VolatilityOverlay(
                    lookback=overlay_params.get("lookback", args.risk_overlay_lookback),
                    min_periods=overlay_params.get("min_periods", args.risk_overlay_min_periods),
                    volatility_cap=overlay_params.get("volatility_cap", 1.2),
                    reference_is_lagged=True,
                    target_volatility_quantile=overlay_params.get(
                        "target_volatility_quantile", args.risk_overlay_target_quantile
                    ),
                )
                overlay_result = overlay.transform(predictions, overlay_source)
                predictions = overlay_result["allocations"]
                if args.verbose:
                    print(
                        f"ğŸ›¡ï¸ Risk overlay applied ({overlay.breaches} caps), "
                        f"mean scale={overlay_result['scaling_factors'].mean():.3f}, "
                        f"cfg={overlay_params or {'lookback': args.risk_overlay_lookback}}"
                    )

        # åˆ›å»ºæäº¤æ–‡ä»¶
        tracker.start_task("create_submission")
        submission_df = create_submission(predictions, test_data['date_id'])
        
        # éªŒè¯æäº¤æ–‡ä»¶
        if not validate_submission(submission_df):
            return 1
        
        # ä¿å­˜æäº¤æ–‡ä»¶
        submission_df.to_csv(submission_path.with_suffix('.csv'), index=False)
        # åŒæ—¶ä¿å­˜parquetæ ¼å¼ï¼ˆå¦‚æœå¯èƒ½ï¼‰
        try:
            submission_df.to_parquet(submission_path, index=False)
        except Exception as e:
            print(f"âš ï¸  æ— æ³•ä¿å­˜parquetæ ¼å¼ï¼Œä½¿ç”¨CSV: {e}")
        tracker.end_task()
        
        # è®°å½•æ€§èƒ½æŒ‡æ ‡
        tracker.start_task("logging")
        metrics = {
            'num_predictions': len(predictions),
            'min_prediction': float(predictions.min()),
            'max_prediction': float(predictions.max()),
            'mean_prediction': float(predictions.mean()),
            'std_prediction': float(predictions.std()),
            'std_raw_prediction': float(raw_test_preds.std()),
            'allocation_scale': allocation_scale,
            'train_sharpe': float(
                scale_result.get('cv_sharpe', oof_entry.get('oof_metrics', {}).get('sharpe', 0.0))
                if oof_entry
                else scale_result.get('cv_sharpe', 0.0)
            ),
            'model_type': model_type,
            'environment': env
        }
        if oof_entry:
            metrics['oof_sharpe'] = float(oof_entry.get('oof_metrics', {}).get('sharpe', 0.0))
            metrics['oof_scale_timestamp'] = oof_entry.get('timestamp')
        if overlay_result is not None:
            metrics['overlay_mean_scale'] = float(overlay_result['scaling_factors'].mean())
            metrics['overlay_breaches'] = int(getattr(overlay, 'breaches', 0))
        
        # ä¿å­˜æ—¥å¿—å’ŒæŒ‡æ ‡
        save_logs(tracker.get_summary(), log_paths.log_jsonl)
        save_metrics(metrics, log_paths.metrics_csv)
        tracker.end_task()
        
        # è¾“å‡ºç»“æœ
        print(f"\nâœ… æäº¤æ–‡ä»¶å·²ä¿å­˜: {submission_path}")
        print(f"ğŸ“ˆ é¢„æµ‹ç»Ÿè®¡:")
        print(f"   é¢„æµ‹æ•°é‡: {len(predictions)}")
        print(f"   æœ€å°å€¼: {predictions.min():.4f}")
        print(f"   æœ€å¤§å€¼: {predictions.max():.4f}")
        print(f"   å¹³å‡å€¼: {predictions.mean():.4f}")
        print(f"   æ ‡å‡†å·®: {predictions.std():.4f}")
        
        # è¾“å‡ºæ€§èƒ½æ‘˜è¦
        summary = tracker.get_summary()
        print(f"\nâ±ï¸ æ€§èƒ½æ‘˜è¦:")
        print(f"   æ€»æ—¶é—´: {summary['total_time_seconds']:.2f}ç§’")
        for task, duration in summary['task_breakdown'].items():
            print(f"   {task}: {duration:.2f}ç§’")
        
        return 0
        
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())
