#!/usr/bin/env python3
"""
Hull Tactical - Market Prediction ä¸»æ¨¡å‹æ–‡ä»¶
Kaggleç«èµ›çš„æ¨¡å‹å…¥å£ç‚¹
"""

import argparse
import sys
import os
import pandas as pd
from pathlib import Path

# æ·»åŠ libç›®å½•åˆ°è·¯å¾„
working_dir = os.path.dirname(__file__)
if working_dir:
    sys.path.insert(0, working_dir)
else:
    # å¦‚æœ__file__ä¸ºç©ºï¼ˆåœ¨æŸäº›ç¯å¢ƒä¸­ï¼‰ï¼Œä½¿ç”¨å½“å‰å·¥ä½œç›®å½•
    sys.path.insert(0, os.getcwd())

import logging

from lib.env import detect_run_environment, get_data_paths, get_log_paths
from lib.data import load_test_data, load_train_data, validate_data
from lib.features import FeaturePipeline
from lib.model_registry import get_model_params, resolve_model_type
from lib.models import HullModel, create_submission
from lib.strategy import (
    VolatilityOverlay,
    optimize_scale_with_rolling_cv,
    scale_to_allocation,
    tune_allocation_scale,
)
from lib.utils import PerformanceTracker, save_logs, save_metrics, validate_submission

# å°è¯•å¯¼å…¥é…ç½®æ¨¡å—
try:
    from lib.config import ConfigManager, get_config
    CONFIG_AVAILABLE = True
except ImportError:
    CONFIG_AVAILABLE = False
    print("âš ï¸ é…ç½®æ¨¡å—ä¸å¯ç”¨")

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def parse_args(argv=None):
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="Hull Tactical - Market Prediction æ¨¡å‹")
    
    parser.add_argument(
        "--model-type",
        choices=["baseline", "lightgbm", "xgboost", "catboost", "ensemble"],
        default="baseline",
        help="é€‰æ‹©æ¨¡å‹ç±»å‹ (é»˜è®¤: baseline)"
    )
    
    parser.add_argument(
        "--data-path",
        type=Path,
        default=None,
        help="æ˜¾å¼æŒ‡å®šæ•°æ®è·¯å¾„ï¼ˆè¦†ç›–è‡ªåŠ¨æ£€æµ‹ï¼‰"
    )
    
    parser.add_argument(
        "--output",
        type=Path,
        default=None,
        help="è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤ï¼šæ ¹æ®ç¯å¢ƒè‡ªåŠ¨è®¾ç½®ï¼‰"
    )
    
    parser.add_argument(
        "--config",
        type=Path,
        default=None,
        help="é…ç½®æ–‡ä»¶è·¯å¾„"
    )
    
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="è¯¦ç»†è¾“å‡ºæ¨¡å¼"
    )
    parser.add_argument(
        "--risk-overlay-lookback",
        type=int,
        default=63,
        help="é£é™©overlayæ»šåŠ¨çª—å£é•¿åº¦",
    )
    parser.add_argument(
        "--risk-overlay-min-periods",
        type=int,
        default=21,
        help="overlayå¼€å§‹ç”Ÿæ•ˆæœ€å°å†å²é•¿åº¦",
    )
    parser.add_argument(
        "--disable-risk-overlay",
        action="store_true",
        help="ç¦ç”¨æ³¢åŠ¨ç‡é£é™©overlay",
    )
    
    # å¤„ç†ä¸åŒçš„è¿è¡Œç¯å¢ƒå‚æ•°
    if argv is None:
        # è¿‡æ»¤æ‰Jupyterå†…æ ¸å‚æ•°å’Œä¸ç›¸å…³çš„å‚æ•°
        filtered_argv = [arg for arg in sys.argv[1:] if not arg.startswith('-f') and not ':memory:' in arg]
        try:
            return parser.parse_args(filtered_argv)
        except SystemExit:
            # å¦‚æœè§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å‚æ•°
            print("âš ï¸ å‚æ•°è§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å‚æ•°")
            return parser.parse_args([])
    
    return parser.parse_args(argv)


def main():
    """ä¸»å‡½æ•° - è¿è¡Œæ¨¡å‹é¢„æµ‹"""
    
    args = parse_args()
    
    # åˆå§‹åŒ–é…ç½®
    if CONFIG_AVAILABLE and args.config:
        config_manager = ConfigManager(str(args.config))
        logging_config = config_manager.get_logging_config()
        # æ›´æ–°æ—¥å¿—é…ç½®
        logging.basicConfig(
            level=getattr(logging, logging_config['level'], logging.INFO),
            format=logging_config['format']
        )
    elif CONFIG_AVAILABLE:
        config_manager = get_config()
    else:
        config_manager = None
    
    tracker = PerformanceTracker(logger=logger)
    
    print("ğŸš€ Hull Tactical - Market Prediction æ¨¡å‹å¯åŠ¨")
    
    # æ£€æµ‹è¿è¡Œç¯å¢ƒ
    env = detect_run_environment()
    data_paths = get_data_paths(env)
    log_paths = get_log_paths(env)
    
    print(f"ğŸ  è¿è¡Œç¯å¢ƒ: {env}")
    print(f"ğŸ“ æ•°æ®è·¯å¾„: {data_paths.test_data}")
    
    # è®¾ç½®è¾“å‡ºè·¯å¾„
    if args.output:
        submission_path = args.output
    else:
        submission_path = log_paths.submission_file
    
    try:
        # åŠ è½½æ•°æ®
        tracker.start_task("load_data")
        tracker.record_memory_usage()
        train_data = load_train_data(data_paths)
        test_data = load_test_data(data_paths)
        
        if not validate_data(test_data, "test"):
            return 1
        
        tracker.record_memory_usage()
        tracker.end_task()
        
        # ç‰¹å¾å·¥ç¨‹
        tracker.start_task("feature_engineering")
        tracker.record_memory_usage()
        pipeline = FeaturePipeline()
        train_features = pipeline.fit_transform(train_data)
        test_features = pipeline.transform(test_data)
        
        if args.verbose:
            print(f"ğŸ”§ è®­ç»ƒç‰¹å¾æ•°é‡: {train_features.shape[1]}")
            print(f"ğŸ“Š æ¨ç†ç‰¹å¾å½¢çŠ¶: {test_features.shape}")
        
        tracker.record_memory_usage()
        tracker.end_task()
        
        # æ¨¡å‹é¢„æµ‹
        tracker.start_task("model_prediction")
        tracker.record_memory_usage()
        
        model_type = resolve_model_type(args.model_type)
        model_params = get_model_params(model_type)
        model = HullModel(model_type=model_type, model_params=model_params)
        target = train_data["forward_returns"].fillna(train_data["forward_returns"].median())
        model.fit(train_features, target)

        train_preds = model.predict(train_features, clip=False)
        scale_result = optimize_scale_with_rolling_cv(train_preds, target.values)
        allocation_scale = scale_result.get("scale", 20.0)
        if allocation_scale is None:
            tuning = tune_allocation_scale(train_preds, target.values)
            allocation_scale = tuning.get("scale", 20.0)
            scale_result = {
                "cv_sharpe": tuning.get("strategy_sharpe", 0.0),
                "strategy_sharpe": tuning.get("strategy_sharpe", 0.0),
            }
        if args.verbose:
            print(
                f"ğŸ¯ Allocation scale={allocation_scale:.2f}, "
                f"CVSharpe={scale_result.get('cv_sharpe', 0):.4f}"
            )

        raw_test_preds = model.predict(test_features, clip=False)
        predictions = scale_to_allocation(raw_test_preds, scale=allocation_scale)

        tracker.record_memory_usage()
        tracker.end_task()

        if not args.disable_risk_overlay:
            overlay_source = None
            if "lagged_forward_returns" in test_data.columns:
                overlay_source = test_data["lagged_forward_returns"].to_numpy()
            elif "lagged_market_forward_excess_returns" in test_data.columns:
                overlay_source = test_data["lagged_market_forward_excess_returns"].to_numpy()

            if overlay_source is not None:
                overlay = VolatilityOverlay(
                    lookback=args.risk_overlay_lookback,
                    min_periods=args.risk_overlay_min_periods,
                    reference_is_lagged=True,
                )
                overlay_result = overlay.transform(predictions, overlay_source)
                predictions = overlay_result["allocations"]
                if args.verbose:
                    print(
                        f"ğŸ›¡ï¸ Risk overlay applied ({overlay.breaches} caps), "
                        f"mean scale={overlay_result['scaling_factors'].mean():.3f}"
                    )

        # åˆ›å»ºæäº¤æ–‡ä»¶
        tracker.start_task("create_submission")
        submission_df = create_submission(predictions, test_data['date_id'])
        
        # éªŒè¯æäº¤æ–‡ä»¶
        if not validate_submission(submission_df):
            return 1
        
        # ä¿å­˜æäº¤æ–‡ä»¶
        submission_df.to_csv(submission_path.with_suffix('.csv'), index=False)
        # åŒæ—¶ä¿å­˜parquetæ ¼å¼ï¼ˆå¦‚æœå¯èƒ½ï¼‰
        try:
            submission_df.to_parquet(submission_path, index=False)
        except Exception as e:
            print(f"âš ï¸  æ— æ³•ä¿å­˜parquetæ ¼å¼ï¼Œä½¿ç”¨CSV: {e}")
        tracker.end_task()
        
        # è®°å½•æ€§èƒ½æŒ‡æ ‡
        tracker.start_task("logging")
        metrics = {
            'num_predictions': len(predictions),
            'min_prediction': float(predictions.min()),
            'max_prediction': float(predictions.max()),
            'mean_prediction': float(predictions.mean()),
            'std_prediction': float(predictions.std()),
            'allocation_scale': allocation_scale,
            'train_sharpe': float(scale_result.get('cv_sharpe', 0.0)),
            'model_type': model_type,
            'environment': env
        }
        
        # ä¿å­˜æ—¥å¿—å’ŒæŒ‡æ ‡
        save_logs(tracker.get_summary(), log_paths.log_jsonl)
        save_metrics(metrics, log_paths.metrics_csv)
        tracker.end_task()
        
        # è¾“å‡ºç»“æœ
        print(f"\nâœ… æäº¤æ–‡ä»¶å·²ä¿å­˜: {submission_path}")
        print(f"ğŸ“ˆ é¢„æµ‹ç»Ÿè®¡:")
        print(f"   é¢„æµ‹æ•°é‡: {len(predictions)}")
        print(f"   æœ€å°å€¼: {predictions.min():.4f}")
        print(f"   æœ€å¤§å€¼: {predictions.max():.4f}")
        print(f"   å¹³å‡å€¼: {predictions.mean():.4f}")
        print(f"   æ ‡å‡†å·®: {predictions.std():.4f}")
        
        # è¾“å‡ºæ€§èƒ½æ‘˜è¦
        summary = tracker.get_summary()
        print(f"\nâ±ï¸ æ€§èƒ½æ‘˜è¦:")
        print(f"   æ€»æ—¶é—´: {summary['total_time_seconds']:.2f}ç§’")
        for task, duration in summary['task_breakdown'].items():
            print(f"   {task}: {duration:.2f}ç§’")
        
        return 0
        
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())
