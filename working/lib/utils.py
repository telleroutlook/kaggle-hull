"""
é€šç”¨å·¥å…·å‡½æ•°
"""

from __future__ import annotations

import time
import json
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional
import logging

try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False
    print("âš ï¸ psutilä¸å¯ç”¨ï¼Œå†…å­˜ç›‘æ§åŠŸèƒ½å—é™")


class PerformanceTracker:
    """æ€§èƒ½è·Ÿè¸ªå™¨"""
    
    def __init__(self, logger=None):
        self.start_time = time.time()
        self.task_times = []
        self.metrics = {}
        self.memory_usage = []
        self.logger = logger or logging.getLogger(__name__)
    
    def start_task(self, task_name: str):
        """å¼€å§‹ä»»åŠ¡è®¡æ—¶"""
        self.current_task = task_name
        self.task_start_time = time.time()
        self.logger.info(f"ğŸš€ å¼€å§‹ä»»åŠ¡: {task_name}")
    
    def end_task(self):
        """ç»“æŸä»»åŠ¡è®¡æ—¶"""
        if hasattr(self, 'current_task') and hasattr(self, 'task_start_time'):
            duration = time.time() - self.task_start_time
            self.task_times.append((self.current_task, duration))
            self.logger.info(f"âœ… ä»»åŠ¡å®Œæˆ: {self.current_task} (è€—æ—¶: {duration:.2f}ç§’)")
            
    def record_memory_usage(self):
        """è®°å½•å½“å‰å†…å­˜ä½¿ç”¨"""
        if not PSUTIL_AVAILABLE:
            return
            
        try:
            import os
            process = psutil.Process(os.getpid())
            memory_info = process.memory_info()
            self.memory_usage.append({
                'timestamp': time.time(),
                'rss_mb': memory_info.rss / 1024 / 1024,  # RSSå†…å­˜ï¼ˆMBï¼‰
                'vms_mb': memory_info.vms / 1024 / 1024   # VMSå†…å­˜ï¼ˆMBï¼‰
            })
        except Exception as e:
            print(f"âš ï¸ å†…å­˜ç›‘æ§å¤±è´¥: {e}")
    
    def log_metric(self, name: str, value: Any):
        """è®°å½•æŒ‡æ ‡"""
        self.metrics[name] = value
    
    def get_summary(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æ‘˜è¦"""
        
        total_time = time.time() - self.start_time
        
        # è®¡ç®—å†…å­˜ä½¿ç”¨ç»Ÿè®¡
        if self.memory_usage:
            memory_stats = {
                'max_rss_mb': max(m['rss_mb'] for m in self.memory_usage),
                'avg_rss_mb': np.mean([m['rss_mb'] for m in self.memory_usage]),
                'max_vms_mb': max(m['vms_mb'] for m in self.memory_usage),
                'avg_vms_mb': np.mean([m['vms_mb'] for m in self.memory_usage])
            }
        else:
            memory_stats = {}
        
        summary = {
            'total_time_seconds': total_time,
            'total_time_minutes': total_time / 60,
            'task_breakdown': dict(self.task_times),
            'metrics': self.metrics,
            'memory_stats': memory_stats
        }
        
        return summary


def save_logs(logs: Dict[str, Any], log_path: Path):
    """ä¿å­˜æ—¥å¿—åˆ°JSONLæ–‡ä»¶"""
    
    log_path.parent.mkdir(parents=True, exist_ok=True)
    
    log_entry = {
        'timestamp': datetime.now().isoformat(),
        **logs
    }
    
    with open(log_path, 'a', encoding='utf-8') as f:
        f.write(json.dumps(log_entry) + '\n')


def save_metrics(metrics: Dict[str, float], metrics_path: Path):
    """ä¿å­˜æŒ‡æ ‡åˆ°CSVæ–‡ä»¶"""
    
    metrics_path.parent.mkdir(parents=True, exist_ok=True)
    
    metrics_df = pd.DataFrame([metrics])
    metrics_df['timestamp'] = datetime.now().isoformat()
    
    if metrics_path.exists() and metrics_path.stat().st_size > 0:
        try:
            existing_df = pd.read_csv(metrics_path)
            metrics_df = pd.concat([existing_df, metrics_df], ignore_index=True)
        except pd.errors.EmptyDataError:
            # å·²å­˜åœ¨æ–‡ä»¶ä½†æ²¡æœ‰å†…å®¹ï¼Œç›´æ¥è¦†ç›–
            pass
    
    metrics_df.to_csv(metrics_path, index=False)


def write_result_json(
    succeeded: bool,
    result_path: Optional[Path] = None,
    error_type: Optional[int] = None,
    error_name: Optional[str] = None,
    error_details: Optional[str] = None,
):
    """Persist Kaggle evaluation status so the platform doesn't treat the run as a system error."""

    if result_path is None:
        result_path = Path("result.json")

    result_path.parent.mkdir(parents=True, exist_ok=True)

    payload: Dict[str, Any] = {"Succeeded": bool(succeeded)}

    if not succeeded:
        # Kaggle expects these fields when a run fails; fall back to the default gateway error codes.
        payload.update(
            {
                "ErrorType": error_type if error_type is not None else 5,
                "ErrorName": error_name or "GATEWAY_RAISED_EXCEPTION",
                "ErrorDetails": (error_details or "")[:8000],
            }
        )

    with open(result_path, "w", encoding="utf-8") as fp:
        json.dump(payload, fp)


def print_progress(current: int, total: int, prefix: str = "", suffix: str = ""):
    """æ‰“å°è¿›åº¦æ¡"""
    
    bar_length = 50
    progress = float(current) / total
    arrow = "=" * int(round(progress * bar_length) - 1) + ">"
    spaces = " " * (bar_length - len(arrow))
    
    print(f'\r{prefix}[{arrow + spaces}] {int(progress * 100)}% {suffix}', end='', flush=True)
    
    if current == total:
        print()


def validate_submission(submission_df: pd.DataFrame) -> bool:
    """éªŒè¯æäº¤æ–‡ä»¶æ ¼å¼"""
    
    required_columns = ['date_id', 'prediction']
    
    # æ£€æŸ¥å¿…è¦åˆ—
    missing_cols = [col for col in required_columns if col not in submission_df.columns]
    if missing_cols:
        print(f"é”™è¯¯: æäº¤æ–‡ä»¶ç¼ºå°‘åˆ—: {missing_cols}")
        return False
    
    # æ£€æŸ¥é¢„æµ‹å€¼èŒƒå›´
    predictions = submission_df['prediction'].values
    if np.any(predictions < 0) or np.any(predictions > 2):
        print(f"é”™è¯¯: é¢„æµ‹å€¼å¿…é¡»åœ¨0-2ä¹‹é—´, å½“å‰èŒƒå›´: [{np.min(predictions):.4f}, {np.max(predictions):.4f}]")
        return False
    
    # æ£€æŸ¥ç¼ºå¤±å€¼
    if submission_df.isnull().any().any():
        print("é”™è¯¯: æäº¤æ–‡ä»¶åŒ…å«ç¼ºå¤±å€¼")
        return False
    
    print("âœ… æäº¤æ–‡ä»¶éªŒè¯é€šè¿‡")
    return True


__all__ = [
    "PerformanceTracker",
    "save_logs", 
    "save_metrics",
    "print_progress",
    "validate_submission",
    "write_result_json",
]
